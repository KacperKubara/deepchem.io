<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.nn.copy &mdash; deepchem 1.3.1 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.3.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.3.1 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html"><span><img src="../../../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for deepchem.nn.copy</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Copies Classes from keras to remove dependency.</span>

<span class="sd">Most of this code is copied over from Keras. Hoping to use as a staging</span>
<span class="sd">area while we remove our Keras dependency.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">deepchem.nn</span> <span class="kn">import</span> <span class="n">initializations</span>
<span class="kn">from</span> <span class="nn">deepchem.nn</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">deepchem.nn</span> <span class="kn">import</span> <span class="n">activations</span>
<span class="kn">from</span> <span class="nn">deepchem.nn</span> <span class="kn">import</span> <span class="n">model_ops</span>


<div class="viewcode-block" id="to_list"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.to_list">[docs]</a><span class="k">def</span> <span class="nf">to_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;This normalizes a list/tensor into a list.</span>

<span class="sd">  If a tensor is passed, we return</span>
<span class="sd">  a list of size 1 containing the tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span></div>


<div class="viewcode-block" id="object_list_uid"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.object_list_uid">[docs]</a><span class="k">def</span> <span class="nf">object_list_uid</span><span class="p">(</span><span class="n">object_list</span><span class="p">):</span>
  <span class="n">object_list</span> <span class="o">=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">object_list</span><span class="p">)</span>
  <span class="k">return</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">object_list</span><span class="p">])</span></div>


<div class="viewcode-block" id="Layer"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Layer">[docs]</a><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Abstract base layer class.</span>

<span class="sd">  Attributes</span>
<span class="sd">  ----------</span>
<span class="sd">  name: String, must be unique within a model.</span>
<span class="sd">  trainable: Boolean, whether the layer weights</span>
<span class="sd">      will be updated during training.</span>
<span class="sd">  uses_learning_phase: Whether any operation</span>
<span class="sd">      of the layer uses model_ops.in_training_phase()</span>
<span class="sd">      or model_ops.in_test_phase().</span>
<span class="sd">  input_shape: Shape tuple. Provided for convenience,</span>
<span class="sd">    but note that there may be cases in which this</span>
<span class="sd">    attribute is ill-defined (e.g. a shared layer</span>
<span class="sd">    with multiple input shapes), in which case</span>
<span class="sd">    requesting input_shape will raise an Exception.</span>
<span class="sd">    Prefer using layer.get_input_shape_for(input_shape),</span>
<span class="sd">  output_shape: Shape tuple. See above.</span>
<span class="sd">  input, output: Input/output tensor(s). Note that if the layer is used</span>
<span class="sd">    more than once (shared layer), this is ill-defined</span>
<span class="sd">    and will raise an exception. In such cases, use</span>

<span class="sd">  Methods</span>
<span class="sd">  -------</span>
<span class="sd">  call(x): Where the layer&#39;s logic lives.</span>
<span class="sd">  __call__(x): Wrapper around the layer logic (`call`).</span>
<span class="sd">      If x is a tensor:</span>
<span class="sd">          - Connect current layer with last layer from tensor:</span>
<span class="sd">          - Add layer to tensor history</span>
<span class="sd">      If layer is not built:</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># These properties should have been set</span>
    <span class="c1"># by the child class, as appropriate.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;uses_learning_phase&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">uses_learning_phase</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;losses&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># These properties should be set by the user via keyword arguments.</span>
    <span class="c1"># note that &#39;input_dtype&#39;, &#39;input_shape&#39; and &#39;batch_input_shape&#39;</span>
    <span class="c1"># are only applicable to input layers: do not pass these keywords</span>
    <span class="c1"># to non-input layers.</span>
    <span class="n">allowed_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;input_shape&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_input_shape&#39;</span><span class="p">,</span> <span class="s1">&#39;input_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;trainable&#39;</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="n">kwarg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">kwarg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Keyword argument not understood:&#39;</span><span class="p">,</span> <span class="n">kwarg</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">model_ops</span><span class="o">.</span><span class="n">get_uid</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;trainable&#39;</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="c1"># In this case we will create an input layer</span>
      <span class="c1"># to insert before the current layer</span>
      <span class="k">if</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">])</span>
      <span class="k">elif</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_shape&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">batch_input_shape</span> <span class="o">=</span> <span class="n">batch_input_shape</span>
      <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;input_dtype&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">input_dtype</span> <span class="o">=</span> <span class="n">input_dtype</span>

<div class="viewcode-block" id="Layer.add_weight"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Layer.add_weight">[docs]</a>  <span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="p">,</span> <span class="n">regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a weight variable to the layer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    shape: The shape tuple of the weight.</span>
<span class="sd">    initializer: An Initializer instance (callable).</span>
<span class="sd">    regularizer: An optional Regularizer instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">regularizer</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">weight</span></div>

<div class="viewcode-block" id="Layer.add_loss"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Layer.add_loss">[docs]</a>  <span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds losses to model.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">losses</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="c1"># Update self.losses</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;losses&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">+=</span> <span class="n">losses</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="c1"># In case self.losses isn&#39;t settable</span>
      <span class="c1"># (i.e. it&#39;s a getter method).</span>
      <span class="c1"># In that case the `losses` property is</span>
      <span class="c1"># auto-computed and shouldn&#39;t be set.</span>
      <span class="k">pass</span>
    <span class="c1"># Update self._per_input_updates</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_per_input_losses&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_per_input_losses</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">inputs_hash</span> <span class="o">=</span> <span class="n">object_list_uid</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Updates indexed by None are unconditional</span>
      <span class="c1"># rather than input-dependent</span>
      <span class="n">inputs_hash</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">inputs_hash</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_input_losses</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_per_input_losses</span><span class="p">[</span><span class="n">inputs_hash</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_per_input_losses</span><span class="p">[</span><span class="n">inputs_hash</span><span class="p">]</span> <span class="o">+=</span> <span class="n">losses</span></div>

<div class="viewcode-block" id="Layer.call"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Layer.call">[docs]</a>  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is where the layer&#39;s logic lives.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: input tensor, or list/tuple of input tensors.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A tensor or list/tuple of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="Layer.__call__"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Layer.__call__">[docs]</a>  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper around self.call(), for handling</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: Can be a tensor or list/tuple of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div></div>
    <span class="c1">#outputs = to_list(self.call(x))</span>
    <span class="c1">#return outputs</span>


<div class="viewcode-block" id="InputLayer"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.InputLayer">[docs]</a><span class="k">class</span> <span class="nc">InputLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Layer to be used as an entry point into a graph.</span>

<span class="sd">  Create its a placeholder tensor (pass arguments `input_shape`</span>
<span class="sd">  or `batch_input_shape` as well as `input_dtype`).</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  input_shape: Shape tuple, not including the batch axis.</span>
<span class="sd">  batch_input_shape: Shape tuple, including the batch axis.</span>
<span class="sd">  input_dtype: Datatype of the input.</span>
<span class="sd">  name: Name of the layer (string).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">batch_input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">input_dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">uses_learning_phase</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">prefix</span> <span class="o">=</span> <span class="s1">&#39;input&#39;</span>
      <span class="c1"># TODO(rbharath): Keras uses a global var here to maintain</span>
      <span class="c1"># unique counts. This seems dangerous. How does tensorflow handle?</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">model_ops</span><span class="o">.</span><span class="n">get_uid</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">and</span> <span class="n">batch_input_shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only provide the input_shape OR &#39;</span>
                       <span class="s1">&#39;batch_input_shape argument to &#39;</span>
                       <span class="s1">&#39;InputLayer, not both at the same time.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_input_shape</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">input_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;An Input layer should be passed either &#39;</span>
                         <span class="s1">&#39;a `batch_input_shape` or an `input_shape`.&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">batch_input_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_dtype</span><span class="p">:</span>
      <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">batch_input_shape</span> <span class="o">=</span> <span class="n">batch_input_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_dtype</span> <span class="o">=</span> <span class="n">input_dtype</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">placeholder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_input_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">placeholder</span><span class="o">.</span><span class="n">_uses_learning_phase</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">placeholder</span><span class="p">]</span></div>


<div class="viewcode-block" id="Input"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Input">[docs]</a><span class="k">def</span> <span class="nf">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Input() is used to create a placeholder input</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  shape: A shape tuple (integer), not including the batch size.</span>
<span class="sd">      For instance, `shape=(32,)` indicates that the expected input</span>
<span class="sd">      will be batches of 32-dimensional vectors.</span>
<span class="sd">  name: An optional name string for the layer.</span>
<span class="sd">      Should be unique in a model (do not reuse the same name twice).</span>
<span class="sd">      It will be autogenerated if it isn&#39;t provided.</span>
<span class="sd">  dtype: The data type expected by the input, as a string</span>
<span class="sd">      (`float32`, `float64`, `int32`...)</span>

<span class="sd">  # TODO(rbharath): Support this type of functional API.</span>
<span class="sd">  Example:</span>

<span class="sd">  &gt;&gt;&gt; # this is a logistic regression in Keras</span>
<span class="sd">  &gt;&gt;&gt; a = dc.nn.Input(shape=(32,))</span>
<span class="sd">  &gt;&gt;&gt; b = dc.nn.Dense(16)(a)</span>
<span class="sd">  &gt;&gt;&gt; model = dc.nn.FunctionalModel(input=a, output=b)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># If batch size not specified</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">batch_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">input_layer</span> <span class="o">=</span> <span class="n">InputLayer</span><span class="p">(</span>
      <span class="n">batch_input_shape</span><span class="o">=</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">input_layer</span></div>


<div class="viewcode-block" id="Dense"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Dense">[docs]</a><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Just your regular densely-connected NN layer.</span>

<span class="sd">  TODO(rbharath): Make this functional in deepchem</span>

<span class="sd">  Example:</span>

<span class="sd">  &gt;&gt;&gt; import deepchem as dc</span>
<span class="sd">  &gt;&gt;&gt; # as first layer in a sequential model:</span>
<span class="sd">  &gt;&gt;&gt; model = dc.models.Sequential()</span>
<span class="sd">  &gt;&gt;&gt; model.add(dc.nn.Input(shape=16))</span>
<span class="sd">  &gt;&gt;&gt; model.add(dc.nn.Dense(32))</span>
<span class="sd">  &gt;&gt;&gt; # now the model will take as input arrays of shape (*, 16)</span>
<span class="sd">  &gt;&gt;&gt; # and output arrays of shape (*, 32)</span>

<span class="sd">  &gt;&gt;&gt; # this is equivalent to the above:</span>
<span class="sd">  &gt;&gt;&gt; model = dc.models.Sequential()</span>
<span class="sd">  &gt;&gt;&gt; model.add(dc.nn.Input(shape=16))</span>
<span class="sd">  &gt;&gt;&gt; model.add(dc.nn.Dense(32))</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  output_dim: int &gt; 0.</span>
<span class="sd">  init: name of initialization function for the weights of the layer</span>
<span class="sd">  activation: name of activation function to use</span>
<span class="sd">    (see [activations](../activations.md)).</span>
<span class="sd">    If you don&#39;t specify anything, no activation is applied</span>
<span class="sd">    (ie. &quot;linear&quot; activation: a(x) = x).</span>
<span class="sd">  W_regularizer: (eg. L1 or L2 regularization), applied to the main weights matrix.</span>
<span class="sd">  b_regularizer: instance of regularize applied to the bias.</span>
<span class="sd">  activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),</span>
<span class="sd">    applied to the network output.</span>
<span class="sd">  bias: whether to include a bias</span>
<span class="sd">    (i.e. make the layer affine rather than linear).</span>
<span class="sd">  input_dim: dimensionality of the input (integer). This argument</span>
<span class="sd">    (or alternatively, the keyword argument `input_shape`)</span>
<span class="sd">    is required when using this layer as the first layer in a model.</span>

<span class="sd">  # Input shape</span>
<span class="sd">    nD tensor with shape: (nb_samples, ..., input_dim).</span>
<span class="sd">    The most common situation would be</span>
<span class="sd">    a 2D input with shape (nb_samples, input_dim).</span>

<span class="sd">  # Output shape</span>
<span class="sd">    nD tensor with shape: (nb_samples, ..., output_dim).</span>
<span class="sd">    For instance, for a 2D input with shape `(nb_samples, input_dim)`,</span>
<span class="sd">    the output would have shape `(nb_samples, output_dim)`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_dim</span><span class="p">,</span>
               <span class="n">input_dim</span><span class="p">,</span>
               <span class="n">init</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
               <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
               <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Dense</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_W&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_b&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Dropout">[docs]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies Dropout to the input.</span>

<span class="sd">  Dropout consists in randomly setting</span>
<span class="sd">  a fraction `p` of input units to 0 at each update during training time,</span>
<span class="sd">  which helps prevent overfitting.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  p: float between 0 and 1. Fraction of the input units to drop.</span>
<span class="sd">  seed: A Python integer to use as random seed.</span>

<span class="sd">  # References</span>
<span class="sd">      - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
    <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">&lt;</span> <span class="mf">1.</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">uses_learning_phase</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="Dropout.call"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.Dropout.call">[docs]</a>  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">&lt;</span> <span class="mf">1.</span><span class="p">:</span>

      <span class="k">def</span> <span class="nf">dropped_inputs</span><span class="p">():</span>
        <span class="n">retain_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">retain_prob</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

      <span class="n">x</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">in_train_phase</span><span class="p">(</span><span class="n">dropped_inputs</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="BatchNormalization"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.BatchNormalization">[docs]</a><span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Batch normalization layer (Ioffe and Szegedy, 2014).</span>

<span class="sd">  Normalize the activations of the previous layer at each batch,</span>
<span class="sd">  i.e. applies a transformation that maintains the mean activation</span>
<span class="sd">  close to 0 and the activation standard deviation close to 1.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  epsilon: small float &gt; 0. Fuzz parameter.</span>
<span class="sd">  mode: integer, 0, 1 or 2.</span>
<span class="sd">    - 0: feature-wise normalization.</span>
<span class="sd">        Each feature map in the input will</span>
<span class="sd">        be normalized separately. The axis on which</span>
<span class="sd">        to normalize is specified by the `axis` argument.</span>
<span class="sd">        During training we use per-batch statistics to normalize</span>
<span class="sd">        the data, and during testing we use running averages</span>
<span class="sd">        computed during the training phase.</span>
<span class="sd">    - 1: sample-wise normalization. This mode assumes a 2D input.</span>
<span class="sd">    - 2: feature-wise normalization, like mode 0, but</span>
<span class="sd">        using per-batch statistics to normalize the data during both</span>
<span class="sd">        testing and training.</span>
<span class="sd">  axis: integer, axis along which to normalize in mode 0. For instance,</span>
<span class="sd">    if your input tensor has shape (samples, channels, rows, cols),</span>
<span class="sd">    set axis to 1 to normalize per feature map (channels axis).</span>
<span class="sd">  momentum: momentum in the computation of the</span>
<span class="sd">    exponential average of the mean and standard deviation</span>
<span class="sd">    of the data, for feature-wise normalization.</span>
<span class="sd">  beta_init: name of initialization function for shift parameter, or</span>
<span class="sd">    alternatively, TensorFlow function to use for weights initialization.</span>
<span class="sd">  gamma_init: name of initialization function for scale parameter, or</span>
<span class="sd">    alternatively, TensorFlow function to use for weights initialization.</span>
<span class="sd">  gamma_regularizer: instance of WeightRegularizer</span>
<span class="sd">    (eg. L1 or L2 regularization), applied to the gamma vector.</span>
<span class="sd">  beta_regularizer: instance of WeightRegularizer,</span>
<span class="sd">    applied to the beta vector.</span>

<span class="sd">  Input shape:</span>
<span class="sd">  Arbitrary. Use the keyword argument input_shape</span>
<span class="sd">  (tuple of integers, does not include the samples axis)</span>
<span class="sd">  when using this layer as the first layer in a model.</span>

<span class="sd">  Output shape:</span>
<span class="sd">  Same shape as input.</span>

<span class="sd">  References:</span>
<span class="sd">    - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
               <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
               <span class="n">beta_init</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span>
               <span class="n">gamma_init</span><span class="o">=</span><span class="s1">&#39;one&#39;</span><span class="p">,</span>
               <span class="n">gamma_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">beta_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta_init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">beta_init</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma_init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">gamma_init</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">gamma_regularizer</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">beta_regularizer</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">uses_learning_phase</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="BatchNormalization.build"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.BatchNormalization.build">[docs]</a>  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">],)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma_init</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_gamma&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_init</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_beta&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="c1"># Not Trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_running_mean&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="c1"># Not Trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_running_std&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span></div>

<div class="viewcode-block" id="BatchNormalization.call"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.copy.BatchNormalization.call">[docs]</a>  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">input_shape</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

      <span class="n">reduction_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>
      <span class="k">del</span> <span class="n">reduction_axes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>
      <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
      <span class="n">broadcast_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>

      <span class="n">x_normed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">normalize_batch_in_training</span><span class="p">(</span>
          <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction_axes</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_update</span><span class="p">([</span>
            <span class="n">model_ops</span><span class="o">.</span><span class="n">moving_average_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">),</span>
            <span class="n">model_ops</span><span class="o">.</span><span class="n">moving_average_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span>
        <span class="p">],</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">reduction_axes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_ops</span><span class="o">.</span><span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
          <span class="n">x_normed_running</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
              <span class="n">x</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
              <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># need broadcasting</span>
          <span class="n">broadcast_running_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span>
                                              <span class="n">broadcast_shape</span><span class="p">)</span>
          <span class="n">broadcast_running_std</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
          <span class="n">broadcast_beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
          <span class="n">broadcast_gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
          <span class="n">x_normed_running</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
              <span class="n">x</span><span class="p">,</span>
              <span class="n">broadcast_running_mean</span><span class="p">,</span>
              <span class="n">broadcast_running_std</span><span class="p">,</span>
              <span class="n">broadcast_beta</span><span class="p">,</span>
              <span class="n">broadcast_gamma</span><span class="p">,</span>
              <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>

        <span class="c1"># pick the normalized form of x corresponding to the training phase</span>
        <span class="n">x_normed</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">in_train_phase</span><span class="p">(</span><span class="n">x_normed</span><span class="p">,</span> <span class="n">x_normed_running</span><span class="p">)</span>

    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># sample-wise normalization</span>
      <span class="n">m</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">std</span> <span class="o">=</span> <span class="n">model_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
          <span class="n">model_ops</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
      <span class="n">x_normed</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
      <span class="n">x_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_normed</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
    <span class="k">return</span> <span class="n">x_normed</span></div></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>