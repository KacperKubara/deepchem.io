<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.rl.mcts &mdash; deepchem 1.3.1 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.3.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.3.1 documentation" href="../../../index.html" />
    <link rel="up" title="deepchem.rl" href="../rl.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html"><span><img src="../../../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for deepchem.rl.mcts</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Monte Carlo tree search algorithm for reinforcement learning.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">deepchem.models</span> <span class="kn">import</span> <span class="n">TensorGraph</span>
<span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph.layers</span> <span class="kn">import</span> <span class="n">Feature</span><span class="p">,</span> <span class="n">Weights</span><span class="p">,</span> <span class="n">Label</span><span class="p">,</span> <span class="n">Layer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">time</span>


<div class="viewcode-block" id="MCTSLoss"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTSLoss">[docs]</a><span class="k">class</span> <span class="nc">MCTSLoss</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;This layer computes the loss function for MCTS.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value_weight</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MCTSLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value_weight</span> <span class="o">=</span> <span class="n">value_weight</span>

<div class="viewcode-block" id="MCTSLoss.create_tensor"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTSLoss.create_tensor">[docs]</a>  <span class="k">def</span> <span class="nf">create_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">pred_prob</span><span class="p">,</span> <span class="n">pred_value</span><span class="p">,</span> <span class="n">search_prob</span><span class="p">,</span> <span class="n">search_value</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">out_tensor</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layers</span>
    <span class="p">]</span>
    <span class="n">log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_prob</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">probability_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">search_prob</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">)</span>
    <span class="n">value_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred_value</span> <span class="o">-</span> <span class="n">search_value</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_tensor</span> <span class="o">=</span> <span class="n">probability_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_weight</span> <span class="o">*</span> <span class="n">value_loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">probability_loss</span> <span class="o">=</span> <span class="n">probability_loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value_loss</span> <span class="o">=</span> <span class="n">value_loss</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_tensor</span></div></div>


<div class="viewcode-block" id="MCTS"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTS">[docs]</a><span class="k">class</span> <span class="nc">MCTS</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Implements a Monte Carlo tree search algorithm for reinforcement learning.</span>

<span class="sd">  This is adapted from Silver et al, &quot;Mastering the game of Go without human</span>
<span class="sd">  knowledge&quot; (https://www.nature.com/articles/nature24270).  The methods</span>
<span class="sd">  described in that paper rely on features of Go that are not generally true of</span>
<span class="sd">  all reinforcement learning problems.  To transform it into a more generally</span>
<span class="sd">  useful RL algorithm, it has been necessary to change some aspects of the</span>
<span class="sd">  method.  The overall approach used in this implementation is still the same,</span>
<span class="sd">  although some of the details differ.</span>

<span class="sd">  This class requires the policy to output two quantities: a vector giving the</span>
<span class="sd">  probability of taking each action, and an estimate of the value function for</span>
<span class="sd">  the current state.  At every step of simulating an episode, it performs an</span>
<span class="sd">  expensive tree search to explore the consequences of many possible actions.</span>
<span class="sd">  Based on that search, it computes much better estimates for the value function</span>
<span class="sd">  of the current state and the desired action probabilities.  In then tries to</span>
<span class="sd">  optimize the policy to make its outputs match the result of the tree search.</span>

<span class="sd">  Optimization proceeds through a series of iterations.  Each iteration consists</span>
<span class="sd">  of two stages:</span>

<span class="sd">  1. Simulate many episodes.  At every step perform a tree search to determine</span>
<span class="sd">     targets for the probabilities and value function, and store them into a</span>
<span class="sd">     buffer.</span>
<span class="sd">  2. Optimize the policy using batches drawn from the buffer generated in step 1.</span>

<span class="sd">  The tree search involves repeatedly selecting actions starting from the</span>
<span class="sd">  current state.  This is done by using deepcopy() to clone the environment.  It</span>
<span class="sd">  is essential that this produce a deterministic sequence of states: performing</span>
<span class="sd">  an action on the cloned environment must always lead to the same state as</span>
<span class="sd">  performing that action on the original environment.  For environments whose</span>
<span class="sd">  state transitions are deterministic, this is not a problem.  For ones whose</span>
<span class="sd">  state transitions are stochastic, it is essential that the random number</span>
<span class="sd">  generator used to select new states be stored as part of the environment and</span>
<span class="sd">  be properly cloned by deepcopy().</span>

<span class="sd">  This class does not support policies that include recurrent layers.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">env</span><span class="p">,</span>
               <span class="n">policy</span><span class="p">,</span>
               <span class="n">max_search_depth</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
               <span class="n">n_search_episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
               <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
               <span class="n">value_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(),</span>
               <span class="n">model_dir</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create an object for optimizing a policy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    env: Environment</span>
<span class="sd">      the Environment to interact with</span>
<span class="sd">    policy: Policy</span>
<span class="sd">      the Policy to optimize.  Its create_layers() method must return a dict containing the</span>
<span class="sd">      keys &#39;action_prob&#39; and &#39;value&#39;, corresponding to the action probabilities and value estimate</span>
<span class="sd">    max_search_depth: int</span>
<span class="sd">      the maximum depth of the tree search, measured in steps</span>
<span class="sd">    n_search_episodes: int</span>
<span class="sd">      the number of episodes to simulate (up to max_search_depth, if they do not</span>
<span class="sd">      terminate first) for each tree search</span>
<span class="sd">    discount_factor: float</span>
<span class="sd">      the discount factor to use when computing rewards</span>
<span class="sd">    value_weight: float</span>
<span class="sd">      a scale factor for the value loss term in the loss function</span>
<span class="sd">    optimizer: Optimizer</span>
<span class="sd">      the optimizer to use</span>
<span class="sd">    model_dir: str</span>
<span class="sd">      the directory in which the model will be saved.  If None, a temporary directory will be created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span> <span class="o">=</span> <span class="n">policy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_search_depth</span> <span class="o">=</span> <span class="n">max_search_depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_search_episodes</span> <span class="o">=</span> <span class="n">n_search_episodes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">value_weight</span> <span class="o">=</span> <span class="n">value_weight</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_list</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">state_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred_value</span><span class="p">,</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">_search_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_search_value</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_graph</span><span class="p">(</span>
         <span class="bp">None</span><span class="p">,</span> <span class="s1">&#39;global&#39;</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_graph</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a TensorGraph containing the policy and loss calculations.&quot;&quot;&quot;</span>
    <span class="n">state_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">state_shape</span>
    <span class="n">state_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">state_dtype</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_list</span><span class="p">:</span>
      <span class="n">state_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_shape</span><span class="p">]</span>
      <span class="n">state_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_dtype</span><span class="p">]</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">state_shape</span><span class="p">,</span> <span class="n">state_dtype</span><span class="p">):</span>
      <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">)))</span>
    <span class="n">policy_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span><span class="o">.</span><span class="n">create_layers</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">action_prob</span> <span class="o">=</span> <span class="n">policy_layers</span><span class="p">[</span><span class="s1">&#39;action_prob&#39;</span><span class="p">]</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">policy_layers</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
    <span class="n">search_prob</span> <span class="o">=</span> <span class="n">Label</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
    <span class="n">search_value</span> <span class="o">=</span> <span class="n">Label</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">MCTSLoss</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_weight</span><span class="p">,</span>
        <span class="n">in_layers</span><span class="o">=</span><span class="p">[</span><span class="n">action_prob</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">search_prob</span><span class="p">,</span> <span class="n">search_value</span><span class="p">])</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">TensorGraph</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_search_depth</span><span class="p">,</span>
        <span class="n">use_queue</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">graph</span><span class="o">=</span><span class="n">tf_graph</span><span class="p">,</span>
        <span class="n">model_dir</span><span class="o">=</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">_add_layer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">add_output</span><span class="p">(</span><span class="n">action_prob</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">add_output</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">set_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">set_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">rnn_initial_states</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;MCTS does not support policies with recurrent layers&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">action_prob</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">search_prob</span><span class="p">,</span> <span class="n">search_value</span>

<div class="viewcode-block" id="MCTS.fit"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTS.fit">[docs]</a>  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
          <span class="n">iterations</span><span class="p">,</span>
          <span class="n">steps_per_iteration</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
          <span class="n">epochs_per_iteration</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
          <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
          <span class="n">puct_scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
          <span class="n">max_checkpoints_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
          <span class="n">checkpoint_interval</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
          <span class="n">restore</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train the policy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    iterations: int</span>
<span class="sd">      the total number of iterations (simulation followed by optimization) to perform</span>
<span class="sd">    steps_per_iteration: int</span>
<span class="sd">      the total number of steps to simulate in each iteration.  Every step consists</span>
<span class="sd">      of a tree search, followed by selecting an action based on the results of</span>
<span class="sd">      the search.</span>
<span class="sd">    epochs_per_iteration: int</span>
<span class="sd">      the number of epochs of optimization to perform for each iteration.  Each</span>
<span class="sd">      epoch involves randomly ordering all the steps that were just simulated in</span>
<span class="sd">      the current iteration, splitting them into batches, and looping over the</span>
<span class="sd">      batches.</span>
<span class="sd">    temperature: float</span>
<span class="sd">      the temperature factor to use when selecting a move for each step of</span>
<span class="sd">      simulation.  Larger values produce a broader probability distribution and</span>
<span class="sd">      hence more exploration.  Smaller values produce a stronger preference for</span>
<span class="sd">      whatever action did best in the tree search.</span>
<span class="sd">    puct_scale: float</span>
<span class="sd">      the scale of the PUCT term in the expression for selecting actions during</span>
<span class="sd">      tree search.  This should be roughly similar in magnitude to the rewards</span>
<span class="sd">      given by the environment, since the PUCT term is added to the mean</span>
<span class="sd">      discounted reward.  This may be None, in which case a value is adaptively</span>
<span class="sd">      selected that tries to match the mean absolute value of the discounted</span>
<span class="sd">      reward.</span>
<span class="sd">    max_checkpoints_to_keep: int</span>
<span class="sd">      the maximum number of checkpoint files to keep.  When this number is reached, older</span>
<span class="sd">      files are deleted.</span>
<span class="sd">    checkpoint_interval: float</span>
<span class="sd">      the time interval at which to save checkpoints, measured in seconds</span>
<span class="sd">    restore: bool</span>
<span class="sd">      if True, restore the model from the most recent checkpoint and continue training</span>
<span class="sd">      from there.  If False, retrain the model from scratch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">puct_scale</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_puct_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">adapt_puct</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_puct_scale</span> <span class="o">=</span> <span class="n">puct_scale</span>
      <span class="n">adapt_puct</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
      <span class="k">if</span> <span class="n">restore</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>
      <span class="n">variables</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;global&#39;</span><span class="p">)</span>
      <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="n">max_checkpoints_to_keep</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_index</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">+</span> <span class="n">checkpoint_interval</span>

      <span class="c1"># Run the algorithm.</span>

      <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="nb">buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_episodes</span><span class="p">(</span><span class="n">steps_per_iteration</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">saver</span><span class="p">,</span>
                                    <span class="n">adapt_puct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimize_policy</span><span class="p">(</span><span class="nb">buffer</span><span class="p">,</span> <span class="n">epochs_per_iteration</span><span class="p">)</span>

      <span class="c1"># Save a file checkpoint.</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_index</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">save_file</span><span class="p">,</span>
          <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="MCTS.predict"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTS.predict">[docs]</a>  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the policy&#39;s output predictions for a state.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    state: array</span>
<span class="sd">      the state of the environment for which to generate predictions</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    the array of action probabilities, and the estimated value function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_list</span><span class="p">:</span>
      <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">feed_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_feed_dict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pred_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pred_value</span><span class="p">]</span>
      <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">results</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span></div>

<div class="viewcode-block" id="MCTS.select_action"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTS.select_action">[docs]</a>  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Select an action to perform based on the environment&#39;s state.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    state: array</span>
<span class="sd">      the state of the environment for which to select an action</span>
<span class="sd">    deterministic: bool</span>
<span class="sd">      if True, always return the best action (that is, the one with highest probability).</span>
<span class="sd">      If False, randomly select an action based on the computed probabilities.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    the index of the selected action</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_list</span><span class="p">:</span>
      <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">feed_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_feed_dict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_pred_prob</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">n_actions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></div>

<div class="viewcode-block" id="MCTS.restore"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.MCTS.restore">[docs]</a>  <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reload the model parameters from the most recent checkpoint file.&quot;&quot;&quot;</span>
    <span class="n">last_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">last_checkpoint</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;No checkpoint found&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">variables</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;global&#39;</span><span class="p">)</span>
      <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">variables</span><span class="p">)</span>
      <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="n">last_checkpoint</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_create_feed_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create a feed dict for use by predict() or select_action().&quot;&quot;&quot;</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">f</span><span class="o">.</span><span class="n">out_tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
                     <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_features</span><span class="p">,</span> <span class="n">state</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">feed_dict</span>

  <span class="k">def</span> <span class="nf">_run_episodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">saver</span><span class="p">,</span> <span class="n">adapt_puct</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simulate the episodes for one iteration.&quot;&quot;&quot;</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">root</span> <span class="o">=</span> <span class="n">TreeSearchNode</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
      <span class="n">prob</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_tree_search</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">adapt_puct</span><span class="p">)</span>
      <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">state</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_list</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
      <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">n_actions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">prob</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">terminated</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">root</span> <span class="o">=</span> <span class="n">TreeSearchNode</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">root</span> <span class="o">=</span> <span class="n">root</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_time</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">session</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">save_file</span><span class="p">,</span>
            <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">buffer</span>

  <span class="k">def</span> <span class="nf">_optimize_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">buffer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optimize the policy based on the replay buffer from the current iteration.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">generate_batches</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
          <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">))</span>
          <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{}</span>
          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_features</span><span class="p">):</span>
            <span class="n">feed_dict</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="nb">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">)</span>
          <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_search_prob</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="nb">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">)</span>
          <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_search_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
              <span class="p">[</span><span class="nb">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
          <span class="k">yield</span> <span class="n">feed_dict</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span>
          <span class="n">generate_batches</span><span class="p">(),</span> <span class="n">checkpoint_interval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_do_tree_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">adapt_puct</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Perform the tree search for a state.&quot;&quot;&quot;</span>
    <span class="c1"># Build the tree.</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_search_episodes</span><span class="p">):</span>
      <span class="n">env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_create_trace</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">root</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute the final probabilities and expected reward.</span>

    <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">c</span><span class="o">.</span><span class="n">count</span><span class="o">**</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">root</span><span class="o">.</span><span class="n">children</span><span class="p">])</span>
    <span class="n">prob</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">mean_reward</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">root</span><span class="o">.</span><span class="n">children</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">adapt_puct</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
          <span class="p">[</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">mean_reward</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">root</span><span class="o">.</span><span class="n">children</span><span class="p">)])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_puct_scale</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_puct_scale</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">prob</span><span class="p">,</span> <span class="n">reward</span>

  <span class="k">def</span> <span class="nf">_create_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create one trace as part of the tree search.&quot;&quot;&quot;</span>
    <span class="n">node</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">terminated</span><span class="p">:</span>
      <span class="c1"># Mark this node as terminal</span>
      <span class="n">node</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="bp">None</span>
      <span class="n">node</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="k">return</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">children</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">children</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># Expand this node.</span>
      <span class="n">prob_pred</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
      <span class="n">node</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">node</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="p">[</span><span class="n">TreeSearchNode</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prob_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_search_depth</span><span class="p">:</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="n">future_rewards</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Select the next action to perform.</span>

      <span class="n">total_counts</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">count</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">children</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">total_counts</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">prior_prob</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">children</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_puct_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_counts</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">c</span><span class="o">.</span><span class="n">mean_reward</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">prior_prob</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="o">.</span><span class="n">count</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">children</span>
        <span class="p">]</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
      <span class="n">next_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="c1"># Recursively build the tree.</span>

      <span class="n">future_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_trace</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">next_node</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Update statistics for this node.</span>

    <span class="n">future_rewards</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">future_rewards</span>
    <span class="n">node</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">+=</span> <span class="n">future_rewards</span>
    <span class="n">node</span><span class="o">.</span><span class="n">mean_reward</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">/</span> <span class="n">node</span><span class="o">.</span><span class="n">count</span>
    <span class="k">return</span> <span class="n">future_rewards</span></div>


<div class="viewcode-block" id="TreeSearchNode"><a class="viewcode-back" href="../../../deepchem.rl.html#deepchem.rl.mcts.TreeSearchNode">[docs]</a><span class="k">class</span> <span class="nc">TreeSearchNode</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents a node in the Monte Carlo tree search.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prior_prob</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prior_prob</span> <span class="o">=</span> <span class="n">prior_prob</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="p">[]</span></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>