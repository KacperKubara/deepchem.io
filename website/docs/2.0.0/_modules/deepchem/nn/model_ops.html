<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.nn.model_ops &mdash; deepchem 1.2 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.2 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html"><span><img src="../../../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.2</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for deepchem.nn.model_ops</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Ops for graph construction.</span>

<span class="sd">Large amounts of code borrowed from Keras. Will try to incorporate into</span>
<span class="sd">DeepChem properly.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training</span> <span class="kn">import</span> <span class="n">moving_averages</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="c1"># TODO(rbharath): What does this line do?</span>
<span class="n">py_all</span> <span class="o">=</span> <span class="nb">all</span>

<span class="c1"># TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!</span>
<span class="n">_UID_PREFIXES</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="c1"># This dictionary holds a mapping {graph: learning_phase}.</span>
<span class="c1"># A learning phase is a bool tensor used to run Keras models in</span>
<span class="c1"># either train mode (learning_phase == 1) or test mode (learning_phase == 0).</span>
<span class="n">_GRAPH_LEARNING_PHASES</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="learning_phase"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.learning_phase">[docs]</a><span class="k">def</span> <span class="nf">learning_phase</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the learning phase flag.</span>

<span class="sd">  The learning phase flag is a bool tensor (0 = test, 1 = train)</span>
<span class="sd">  to be passed as input to any Keras function</span>
<span class="sd">  that uses a different behavior at train time and test time.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">graph</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_GRAPH_LEARNING_PHASES</span><span class="p">:</span>
    <span class="n">phase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;bool&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keras_learning_phase&#39;</span><span class="p">)</span>
    <span class="n">_GRAPH_LEARNING_PHASES</span><span class="p">[</span><span class="n">graph</span><span class="p">]</span> <span class="o">=</span> <span class="n">phase</span>
  <span class="k">return</span> <span class="n">_GRAPH_LEARNING_PHASES</span><span class="p">[</span><span class="n">graph</span><span class="p">]</span></div>


<div class="viewcode-block" id="in_train_phase"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.in_train_phase">[docs]</a><span class="k">def</span> <span class="nf">in_train_phase</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alt</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Selects `x` in train phase, and `alt` otherwise.</span>
<span class="sd">  Note that `alt` should have the *same shape* as `x`.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  Either `x` or `alt` based on `K.learning_phase`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">learning_phase</span><span class="p">()</span> <span class="ow">is</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">elif</span> <span class="n">learning_phase</span><span class="p">()</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">alt</span>
  <span class="c1"># else: assume learning phase is a placeholder tensor.</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">learning_phase</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">alt</span><span class="p">)</span>
  <span class="n">x</span><span class="o">.</span><span class="n">_uses_learning_phase</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="switch"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.switch">[docs]</a><span class="k">def</span> <span class="nf">switch</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">then_expression</span><span class="p">,</span> <span class="n">else_expression</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Switches between two operations</span>
<span class="sd">  depending on a scalar value (`int` or `bool`).</span>
<span class="sd">  Note that both `then_expression` and `else_expression`</span>
<span class="sd">  should be symbolic tensors of the *same shape*.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  condition: scalar tensor.</span>
<span class="sd">  then_expression: either a tensor, or a callable that returns a tensor.</span>
<span class="sd">  else_expression: either a tensor, or a callable that returns a tensor.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  The selected tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">condition</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
    <span class="n">condition</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="s1">&#39;bool&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">then_expression</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">then_expression_fn</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">then_expression</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">then_expression_fn</span> <span class="o">=</span> <span class="n">then_expression</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">else_expression</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">else_expression_fn</span><span class="p">():</span>
      <span class="k">return</span> <span class="n">else_expression</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">else_expression_fn</span> <span class="o">=</span> <span class="n">else_expression</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">then_expression_fn</span><span class="p">,</span> <span class="n">else_expression_fn</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="normalize_batch_in_training"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.normalize_batch_in_training">[docs]</a><span class="k">def</span> <span class="nf">normalize_batch_in_training</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">reduction_axes</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes mean and std for batch then apply batch_normalization on batch.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tuple length of 3, (normalized_tensor, mean, variance).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">reduction_axes</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">reduction_axes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">normed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># need broadcasting</span>
    <span class="n">target_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
      <span class="k">if</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">reduction_axes</span><span class="p">:</span>
        <span class="n">target_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">target_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="n">axis</span><span class="p">])</span>
    <span class="n">target_shape</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span>

    <span class="n">broadcast_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)</span>
    <span class="n">broadcast_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)</span>
    <span class="n">broadcast_gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)</span>
    <span class="n">broadcast_beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">)</span>
    <span class="n">normed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">broadcast_mean</span><span class="p">,</span> <span class="n">broadcast_var</span><span class="p">,</span>
                                       <span class="n">broadcast_beta</span><span class="p">,</span> <span class="n">broadcast_gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">normed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span></div>


<div class="viewcode-block" id="ones"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.ones">[docs]</a><span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Instantiates an all-ones tensor variable and returns it.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  shape: Tuple of integers, shape of returned Keras variable.</span>
<span class="sd">  dtype: Tensorflow dtype</span>
<span class="sd">  name: String, name of returned Keras variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A Keras variable, filled with `1.0`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="cast_to_floatx"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.cast_to_floatx">[docs]</a><span class="k">def</span> <span class="nf">cast_to_floatx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Cast a Numpy array to the default Keras float type.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: Numpy array.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  The same Numpy array, cast to its new type.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="moving_average_update"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.moving_average_update">[docs]</a><span class="k">def</span> <span class="nf">moving_average_update</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">momentum</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">moving_averages</span><span class="o">.</span><span class="n">assign_moving_average</span><span class="p">(</span>
        <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">zero_debias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">moving_averages</span><span class="o">.</span><span class="n">assign_moving_average</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span></div>


<div class="viewcode-block" id="int_shape"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.int_shape">[docs]</a><span class="k">def</span> <span class="nf">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the shape of a Keras tensor or a Keras variable as a tuple of</span>
<span class="sd">  integers or None entries.</span>

<span class="sd">  Arguments</span>
<span class="sd">  ---------</span>
<span class="sd">  x: Tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tuple of integers (or None entries).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="fm">__int__</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">])</span></div>


<div class="viewcode-block" id="get_uid"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.get_uid">[docs]</a><span class="k">def</span> <span class="nf">get_uid</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Provides a unique UID given a string prefix.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  prefix: string.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  An integer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_UID_PREFIXES</span><span class="p">[</span><span class="n">prefix</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">_UID_PREFIXES</span><span class="p">[</span><span class="n">prefix</span><span class="p">]</span></div>


<div class="viewcode-block" id="concatenate"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.concatenate">[docs]</a><span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates a list of tensors alongside the specified axis.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">:</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">dims</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat_v2</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">])</span></div>


<span class="k">def</span> <span class="nf">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">%</span> <span class="n">ndim</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>
  <span class="k">return</span> <span class="n">axis</span>


<div class="viewcode-block" id="mean"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.mean">[docs]</a><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Mean of a tensor, alongside the specified axis.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>
<span class="sd">  axis: A list of integer. Axes to compute the mean.</span>
<span class="sd">  keepdims: A boolean, whether to keep the dimensions or not.</span>
<span class="sd">    If keepdims is False, the rank of the tensor is reduced</span>
<span class="sd">    by 1 for each entry in axis. If keep_dims is True,</span>
<span class="sd">    the reduced dimensions are retained with length 1.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor with the mean of elements of x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="dot"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.dot">[docs]</a><span class="k">def</span> <span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Multiplies 2 tensors (and/or variables) and returns a *tensor*.</span>
<span class="sd">  When attempting to multiply a ND tensor</span>
<span class="sd">  with a ND tensor, it reproduces the Theano behavior.</span>
<span class="sd">  (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: Tensor or variable.</span>
<span class="sd">  y: Tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor, dot product of x and y.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">int_shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))):</span>
      <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">x_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">x_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="n">y_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">int_shape</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">))):</span>
      <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">y_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">y_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">y_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y_shape</span><span class="p">)</span>
    <span class="n">y_permute_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">get_ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
    <span class="n">y_permute_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_permute_dim</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)]</span> <span class="o">+</span> <span class="n">y_permute_dim</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">yt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">y_permute_dim</span><span class="p">),</span> <span class="p">[</span><span class="n">y_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">),</span> <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="get_ndim"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.get_ndim">[docs]</a><span class="k">def</span> <span class="nf">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the number of axes in a tensor, as an integer.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: Tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  Integer (scalar), number of axes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dims</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">_dims</span>
  <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="bp">None</span></div>


<div class="viewcode-block" id="get_dtype"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.get_dtype">[docs]</a><span class="k">def</span> <span class="nf">get_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the dtype of a Keras tensor or variable, as a string.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: Tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  String, dtype of `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span></div>


<div class="viewcode-block" id="clip"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.clip">[docs]</a><span class="k">def</span> <span class="nf">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Element-wise value clipping.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">max_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">max_value</span> <span class="o">&lt;</span> <span class="n">min_value</span><span class="p">:</span>
    <span class="n">max_value</span> <span class="o">=</span> <span class="n">min_value</span>
  <span class="n">min_value</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">min_value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="n">max_value</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">max_value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="epsilon"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.epsilon">[docs]</a><span class="k">def</span> <span class="nf">epsilon</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns the value of the fuzz</span>
<span class="sd">  factor used in numeric expressions.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A float.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="mf">1e-7</span></div>


<div class="viewcode-block" id="random_uniform_variable"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.random_uniform_variable">[docs]</a><span class="k">def</span> <span class="nf">random_uniform_variable</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                            <span class="n">low</span><span class="p">,</span>
                            <span class="n">high</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                            <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Instantiates an variable filled with</span>
<span class="sd">  samples drawn from a uniform distribution and returns it.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  shape: Tuple of integers, shape of returned variable.</span>
<span class="sd">  low: Float, lower boundary of the output inteval.</span>
<span class="sd">  high: Float, upper boundary of the output interval.</span>
<span class="sd">  dtype: Tensorflow dtype</span>
<span class="sd">  name: String, name of returned variable.</span>
<span class="sd">  seed: Integer, random seed.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tf.Variable, filled with drawn samples.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># ensure that randomness is conditioned by the Numpy RNG</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mf">10e8</span><span class="p">)</span>
  <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
      <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="random_normal_variable"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.random_normal_variable">[docs]</a><span class="k">def</span> <span class="nf">random_normal_variable</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                           <span class="n">mean</span><span class="p">,</span>
                           <span class="n">scale</span><span class="p">,</span>
                           <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                           <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Instantiates an Keras variable filled with</span>
<span class="sd">  samples drawn from a normal distribution and returns it.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  shape: Tuple of integers, shape of returned Keras variable.</span>
<span class="sd">  mean: Float, mean of the normal distribution.</span>
<span class="sd">  scale: Float, standard deviation of the normal distribution.</span>
<span class="sd">  dtype: Tensorflow dtype</span>
<span class="sd">  name: String, name of returned Keras variable.</span>
<span class="sd">  seed: Integer, random seed.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tf.Variable, filled with drawn samples.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># ensure that randomness is conditioned by the Numpy RNG</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mf">10e8</span><span class="p">)</span>
  <span class="n">value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span>
      <span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="max"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.max">[docs]</a><span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Maximum value in a tensor.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>
<span class="sd">  axis: An integer, the axis to find maximum values.</span>
<span class="sd">  keepdims: A boolean, whether to keep the dimensions or not.</span>
<span class="sd">      If `keepdims` is `False`, the rank of the tensor is reduced</span>
<span class="sd">      by 1. If `keepdims` is `True`,</span>
<span class="sd">      the reduced dimension is retained with length 1.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor with maximum values of `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="l2_normalize"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.l2_normalize">[docs]</a><span class="k">def</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Normalizes a tensor wrt the L2 norm alongside the specified axis.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: input tensor.</span>
<span class="sd">  axis: axis along which to perform normalization.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="categorical_crossentropy"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.categorical_crossentropy">[docs]</a><span class="k">def</span> <span class="nf">categorical_crossentropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Categorical crossentropy between an output tensor</span>
<span class="sd">  and a target tensor, where the target is a tensor of the same</span>
<span class="sd">  shape as the output.</span>

<span class="sd">  # TODO(rbharath): Should probably swap this over to tf mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Note: tf.nn.softmax_cross_entropy_with_logits</span>
  <span class="c1"># expects logits, Keras expects probabilities.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">from_logits</span><span class="p">:</span>
    <span class="c1"># scale preds so that the class probas of each sample sum to 1</span>
    <span class="n">output</span> <span class="o">/=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># manual computation of crossentropy</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_EPSILON</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
        <span class="n">target</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">labels</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
          <span class="n">logits</span><span class="o">=</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="sparse_categorical_crossentropy"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.sparse_categorical_crossentropy">[docs]</a><span class="k">def</span> <span class="nf">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Categorical crossentropy between an output tensor</span>
<span class="sd">  and a target tensor, where the target is an integer tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Note: tf.nn.softmax_cross_entropy_with_logits</span>
  <span class="c1"># expects logits, Keras expects probabilities.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">from_logits</span><span class="p">:</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_EPSILON</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="n">targets</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="s1">&#39;int64&#39;</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
    <span class="c1"># if our output includes timesteps we need to reshape</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="binary_crossentropy"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.binary_crossentropy">[docs]</a><span class="k">def</span> <span class="nf">binary_crossentropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor.</span>

<span class="sd">  # Arguments</span>
<span class="sd">      output: A tensor.</span>
<span class="sd">      target: A tensor with the same shape as `output`.</span>
<span class="sd">      from_logits: Whether `output` is expected to be a logits tensor.</span>
<span class="sd">          By default, we consider that `output`</span>
<span class="sd">          encodes a probability distribution.</span>

<span class="sd">  # Returns</span>
<span class="sd">      A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Note: tf.nn.softmax_cross_entropy_with_logits</span>
  <span class="c1"># expects logits, Keras expects probabilities.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">from_logits</span><span class="p">:</span>
    <span class="c1"># transform back to logits</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">_EPSILON</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">))</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="sum"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.sum">[docs]</a><span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sum of the values in a tensor, alongside the specified axis.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>
<span class="sd">  axis: An integer, the axis to sum over.</span>
<span class="sd">  keepdims: A boolean, whether to keep the dimensions or not.</span>
<span class="sd">    If keepdims is False, the rank of the tensor is reduced</span>
<span class="sd">    by 1. If keepdims is True,</span>
<span class="sd">    the reduced dimension is retained with length 1.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor with sum of x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>


<span class="c1"># TODO(rbharath): Need to rename this. This makes a variable, not just creates</span>
<span class="c1"># a tensor. Confusing with tf.zeros...</span>
<div class="viewcode-block" id="zeros"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.zeros">[docs]</a><span class="k">def</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Instantiates an all-zeros variable and returns it.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  shape: Tuple of integers, shape of returned Keras variable</span>
<span class="sd">  dtype: Tensorflow dtype</span>
<span class="sd">  name: String, name of returned Keras variable</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A variable (including Keras metadata), filled with `0.0`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_distances"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.cosine_distances">[docs]</a><span class="k">def</span> <span class="nf">cosine_distances</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">support</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes pairwise cosine distances between provided tensors</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  test: tf.Tensor</span>
<span class="sd">    Of shape (n_test, n_feat)</span>
<span class="sd">  support: tf.Tensor</span>
<span class="sd">    Of shape (n_support, n_feat)</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  tf.Tensor:</span>
<span class="sd">    Of shape (n_test, n_support)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">rnorm_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">test</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="o">+</span> <span class="mf">1e-7</span>
  <span class="n">rnorm_support</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">support</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="o">+</span> <span class="mf">1e-7</span>
  <span class="n">test_normalized</span> <span class="o">=</span> <span class="n">test</span> <span class="o">*</span> <span class="n">rnorm_test</span>
  <span class="n">support_normalized</span> <span class="o">=</span> <span class="n">support</span> <span class="o">*</span> <span class="n">rnorm_support</span>

  <span class="c1"># Transpose for mul</span>
  <span class="n">support_normalized_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">support_normalized</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">test_normalized</span><span class="p">,</span> <span class="n">support_normalized_t</span><span class="p">)</span>  <span class="c1"># Gram matrix</span>
  <span class="k">return</span> <span class="n">g</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Exponential linear unit.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable to compute the activation function for.</span>
<span class="sd">  alpha: A scalar, slope of positive section.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">alpha</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">res</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">res</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Rectified linear unit.</span>
<span class="sd">  With default values, it returns element-wise `max(x, 0)`.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>
<span class="sd">  alpha: A scalar, slope of negative section (default=`0.`).</span>
<span class="sd">  max_value: Saturation threshold.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">alpha</span> <span class="o">!=</span> <span class="mf">0.</span><span class="p">:</span>
    <span class="n">negative_part</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">max_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">max_value</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">max_value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">alpha</span> <span class="o">!=</span> <span class="mf">0.</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">negative_part</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="lrelu"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.lrelu">[docs]</a><span class="k">def</span> <span class="nf">lrelu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a leaky rectified linear unit function.</span>

<span class="sd">  This function returns a new function that implements the LReLU with a</span>
<span class="sd">  specified alpha.  The returned value can be used as an activation function in</span>
<span class="sd">  network layers.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  alpha: float</span>
<span class="sd">    the slope of the function when x&lt;0</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  a function f(x) that returns alpha*x when x&lt;0, and x when x&gt;0.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

  <span class="k">return</span> <span class="nb">eval</span></div>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scaled Exponential Linear unit.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>

<span class="sd">  References</span>
<span class="sd">  ----------</span>
<span class="sd">  - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.6732632423543772848170429916717</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0507009873554804934193349852946</span>
  <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span></div>


<div class="viewcode-block" id="hard_sigmoid"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.hard_sigmoid">[docs]</a><span class="k">def</span> <span class="nf">hard_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Segment-wise linear approximation of sigmoid.</span>
<span class="sd">  Faster than sigmoid.</span>
<span class="sd">  Returns 0. if x &lt; -2.5, 1. if x &gt; 2.5.</span>
<span class="sd">  In -2.5 &lt;= x &lt;= 2.5, returns 0.2 * x + 0.5.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span>
  <span class="n">zero</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="n">one</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">one</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.sqrt">[docs]</a><span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Element-wise square root.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: input tensor.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">zero</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="n">inf</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">inf</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="var"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.var">[docs]</a><span class="k">def</span> <span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variance of a tensor, alongside the specified axis.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  x: A tensor or variable.</span>
<span class="sd">  axis: An integer, the axis to compute the variance.</span>
<span class="sd">  keepdims: A boolean, whether to keep the dimensions or not.</span>
<span class="sd">      If keepdims is False, the rank of the tensor is reduced</span>
<span class="sd">      by 1. If keepdims is True,</span>
<span class="sd">      the reduced dimension is retained with length 1.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  A tensor with the variance of elements of `x`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">get_ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">devs_squared</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">devs_squared</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>


<div class="viewcode-block" id="euclidean_distance"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.euclidean_distance">[docs]</a><span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">support</span><span class="p">,</span> <span class="n">max_dist_sq</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes pairwise euclidean distances between provided tensors</span>

<span class="sd">  TODO(rbharath): BROKEN! THIS DOESN&#39;T WORK!</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  test: tf.Tensor</span>
<span class="sd">    Of shape (n_test, n_feat)</span>
<span class="sd">  support: tf.Tensor</span>
<span class="sd">    Of shape (n_support, n_feat)</span>
<span class="sd">  max_dist_sq: float, optional</span>
<span class="sd">    Maximum pairwise distance allowed.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  tf.Tensor:</span>
<span class="sd">    Of shape (n_test, n_support)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">support</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">g</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">test</span> <span class="o">-</span> <span class="n">support</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="n">max_dist_sq</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">g</span></div>


<div class="viewcode-block" id="add_bias"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.add_bias">[docs]</a><span class="k">def</span> <span class="nf">add_bias</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a bias term to a tensor.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  tensor: tf.Tensor</span>
<span class="sd">    Variable tensor.</span>
<span class="sd">  init: float</span>
<span class="sd">    Bias initializer. Defaults to zero.</span>
<span class="sd">  name: str</span>
<span class="sd">    Name for this op. Defaults to tensor.op.name.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  tf.Tensor</span>
<span class="sd">    A biased tensor with the same shape as the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">training_only</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Random dropout.</span>

<span class="sd">  This implementation supports &quot;always-on&quot; dropout (training_only=False), which</span>
<span class="sd">  can be used to calculate model uncertainty. See Gal and Ghahramani,</span>
<span class="sd">  http://arxiv.org/abs/1506.02142.</span>

<span class="sd">  NOTE(user): To simplify the implementation, I have chosen not to reverse</span>
<span class="sd">    the scaling that occurs in tf.nn.dropout when using dropout during</span>
<span class="sd">    inference. This shouldn&#39;t be an issue since the activations will be scaled</span>
<span class="sd">    by the same constant in both training and inference. This means that there</span>
<span class="sd">    are no training-time differences between networks that use dropout during</span>
<span class="sd">    inference and those that do not.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  tensor: tf.Tensor</span>
<span class="sd">    Input tensor.</span>
<span class="sd">  dropout_prob: float</span>
<span class="sd">    Float giving dropout probability for weights (NOT keep probability).</span>
<span class="sd">  training_only: bool</span>
<span class="sd">    Boolean. If True (standard dropout), apply dropout only</span>
<span class="sd">    during training. If False, apply dropout during inference as well.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  tf.Tensor:</span>
<span class="sd">    A tensor with the same shape as the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">dropout_prob</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>  <span class="c1"># do nothing</span>
  <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_prob</span>
  <span class="k">if</span> <span class="n">training</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">training_only</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="fully_connected_layer"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.fully_connected_layer">[docs]</a><span class="k">def</span> <span class="nf">fully_connected_layer</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span>
                          <span class="n">size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                          <span class="n">weight_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                          <span class="n">bias_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                          <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fully connected layer.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  tensor: tf.Tensor</span>
<span class="sd">    Input tensor.</span>
<span class="sd">  size: int</span>
<span class="sd">    Number of output nodes for this layer.</span>
<span class="sd">  weight_init: float</span>
<span class="sd">    Weight initializer.</span>
<span class="sd">  bias_init: float</span>
<span class="sd">    Bias initializer.</span>
<span class="sd">  name: str</span>
<span class="sd">    Name for this op. Defaults to &#39;fully_connected&#39;.</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  tf.Tensor:</span>
<span class="sd">    A new tensor representing the output of the fully connected layer.</span>

<span class="sd">  Raises</span>
<span class="sd">  ------</span>
<span class="sd">  ValueError</span>
<span class="sd">    If input tensor is not 2D.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">weight_init</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
    <span class="n">weight_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_features</span><span class="p">,</span> <span class="n">size</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">bias_init</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">bias_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">])</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;fully_connected&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="weight_decay"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.weight_decay">[docs]</a><span class="k">def</span> <span class="nf">weight_decay</span><span class="p">(</span><span class="n">penalty_type</span><span class="p">,</span> <span class="n">penalty</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add weight decay.</span>

<span class="sd">  Args:</span>
<span class="sd">    model: TensorflowGraph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A scalar tensor containing the weight decay cost.</span>

<span class="sd">  Raises:</span>
<span class="sd">    NotImplementedError: If an unsupported penalty type is requested.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">variables</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="c1"># exclude bias variables</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
      <span class="n">variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">penalty_type</span> <span class="o">==</span> <span class="s1">&#39;l1&#39;</span><span class="p">:</span>
      <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">penalty_type</span> <span class="o">==</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span>
      <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Unsupported penalty_type </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">penalty_type</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">*=</span> <span class="n">penalty</span>
    <span class="c1">#tf.scalar_summary(&#39;Weight Decay Cost&#39;, cost)</span>
  <span class="k">return</span> <span class="n">cost</span></div>


<div class="viewcode-block" id="multitask_logits"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.multitask_logits">[docs]</a><span class="k">def</span> <span class="nf">multitask_logits</span><span class="p">(</span><span class="n">features</span><span class="p">,</span>
                     <span class="n">num_tasks</span><span class="p">,</span>
                     <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                     <span class="n">weight_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">bias_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">dropout_prob</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a logit tensor for each classification task.</span>

<span class="sd">  Args:</span>
<span class="sd">    features: A 2D tensor with dimensions batch_size x num_features.</span>
<span class="sd">    num_tasks: Number of classification tasks.</span>
<span class="sd">    num_classes: Number of classes for each task.</span>
<span class="sd">    weight_init: Weight initializer.</span>
<span class="sd">    bias_init: Bias initializer.</span>
<span class="sd">    dropout_prob: Float giving dropout probability for weights (NOT keep</span>
<span class="sd">      probability).</span>
<span class="sd">    name: Name for this op. Defaults to &#39;multitask_logits&#39;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of logit tensors; one for each classification task.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">logits_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;multitask_logits&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">task_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                         <span class="p">(</span><span class="s1">&#39;task&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_idx</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">)))),</span>
                         <span class="p">[</span><span class="n">features</span><span class="p">]):</span>
        <span class="n">logits_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">(</span>
                <span class="n">features</span><span class="p">,</span>
                <span class="n">num_classes</span><span class="p">,</span>
                <span class="n">weight_init</span><span class="o">=</span><span class="n">weight_init</span><span class="p">,</span>
                <span class="n">bias_init</span><span class="o">=</span><span class="n">bias_init</span><span class="p">,</span>
                <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">logits_list</span></div>


<div class="viewcode-block" id="logits"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.logits">[docs]</a><span class="k">def</span> <span class="nf">logits</span><span class="p">(</span><span class="n">features</span><span class="p">,</span>
           <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
           <span class="n">weight_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
           <span class="n">bias_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
           <span class="n">dropout_prob</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a logits tensor for a single classification task.</span>

<span class="sd">  You almost certainly don&#39;t want dropout on there -- it&#39;s like randomly setting</span>
<span class="sd">  the (unscaled) probability of a target class to 0.5.</span>

<span class="sd">  Args:</span>
<span class="sd">    features: A 2D tensor with dimensions batch_size x num_features.</span>
<span class="sd">    num_classes: Number of classes for each task.</span>
<span class="sd">    weight_init: Weight initializer.</span>
<span class="sd">    bias_init: Bias initializer.</span>
<span class="sd">    dropout_prob: Float giving dropout probability for weights (NOT keep</span>
<span class="sd">      probability).</span>
<span class="sd">    name: Name for this op.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A logits tensor with shape batch_size x num_classes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">features</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">dropout</span><span class="p">(</span>
        <span class="n">fully_connected_layer</span><span class="p">(</span>
            <span class="n">features</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="o">=</span><span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="o">=</span><span class="n">bias_init</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span> <span class="n">dropout_prob</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax_N"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.softmax_N">[docs]</a><span class="k">def</span> <span class="nf">softmax_N</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply softmax across last dimension of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: Input tensor.</span>
<span class="sd">    name: Name for this op. If None, defaults to &#39;softmax_N&#39;.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with softmax-normalized values on the last dimension.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;softmax_N&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]):</span>
    <span class="n">exp_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="n">reduction_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">exp_tensor</span><span class="p">,</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                      <span class="n">exp_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">reduction_indices</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span></div>


<div class="viewcode-block" id="optimizer"><a class="viewcode-back" href="../../../deepchem.nn.html#deepchem.nn.model_ops.optimizer">[docs]</a><span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=.</span><span class="mo">001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=.</span><span class="mi">9</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create model optimizer.</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  optimizer: str, optional</span>
<span class="sd">    Name of optimizer</span>
<span class="sd">  learning_rate: float, optional</span>
<span class="sd">    Learning rate for algorithm</span>
<span class="sd">  momentum: float, optional</span>
<span class="sd">    Momentum rate</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">    A training Optimizer.</span>

<span class="sd">  Raises:</span>
<span class="sd">    NotImplementedError: If an unsupported optimizer is requested.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(user): gradient clipping (see Minimize)</span>
  <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adagrad&#39;</span><span class="p">:</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;rmsprop&#39;</span><span class="p">:</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Unsupported optimizer </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">optimizer</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_op</span></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>