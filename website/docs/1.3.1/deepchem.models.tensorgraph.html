<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.models.tensorgraph package &mdash; deepchem 1.2 documentation</title>
    
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.2 documentation" href="index.html" />
    <link rel="up" title="deepchem.models package" href="deepchem.models.html" />
    <link rel="next" title="deepchem.models.tensorgraph.models package" href="deepchem.models.tensorgraph.models.html" />
    <link rel="prev" title="deepchem.models.tensorflow_models package" href="deepchem.models.tensorflow_models.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.2</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">deepchem.models.tensorgraph package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.graph_layers">deepchem.models.tensorgraph.graph_layers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.layers">deepchem.models.tensorgraph.layers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.optimizers">deepchem.models.tensorgraph.optimizers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.symmetry_functions">deepchem.models.tensorgraph.symmetry_functions module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.tensor_graph">deepchem.models.tensorgraph.tensor_graph module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph">Module contents</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="deepchem-models-tensorgraph-package">
<h1>deepchem.models.tensorgraph package<a class="headerlink" href="#deepchem-models-tensorgraph-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="deepchem.models.tensorgraph.models.html">deepchem.models.tensorgraph.models package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.atomic_conv">deepchem.models.tensorgraph.models.atomic_conv module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.gan">deepchem.models.tensorgraph.models.gan module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.graph_models">deepchem.models.tensorgraph.models.graph_models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.robust_multitask">deepchem.models.tensorgraph.models.robust_multitask module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.seqtoseq">deepchem.models.tensorgraph.models.seqtoseq module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.symmetry_function_regression">deepchem.models.tensorgraph.models.symmetry_function_regression module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.test_graph_models">deepchem.models.tensorgraph.models.test_graph_models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.test_symmetry_functions">deepchem.models.tensorgraph.models.test_symmetry_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.text_cnn">deepchem.models.tensorgraph.models.text_cnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-deepchem.models.tensorgraph.graph_layers">
<span id="deepchem-models-tensorgraph-graph-layers-module"></span><h2>deepchem.models.tensorgraph.graph_layers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.graph_layers" title="Permalink to this headline">¶</a></h2>
<p>Created on Thu Mar 30 14:02:04 2017</p>
<p>&#64;author: michael</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DAGGather</code><span class="sig-paren">(</span><em>n_graph_feat=30, n_outputs=30, max_atoms=50, layer_sizes=[100], init='glorot_uniform', activation='relu', dropout=None, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.DAGGather</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.DAGgraph_step">
<code class="descname">DAGgraph_step</code><span class="sig-paren">(</span><em>batch_inputs</em>, <em>W_list</em>, <em>b_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.DAGgraph_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.DAGgraph_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.build" title="Permalink to this definition">¶</a></dt>
<dd><p>&#8220;Construct internal trainable weights.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.DAGGather
parent layers: atom_features, membership</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DAGLayer</code><span class="sig-paren">(</span><em>n_graph_feat=30, n_atom_feat=75, max_atoms=50, layer_sizes=[100], init='glorot_uniform', activation='relu', dropout=None, batch_size=64, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.DAGLayer</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.DAGgraph_step">
<code class="descname">DAGgraph_step</code><span class="sig-paren">(</span><em>batch_inputs</em>, <em>W_list</em>, <em>b_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.DAGgraph_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.DAGgraph_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>&#8220;Construct internal trainable weights.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.DAGLayer
parent layers: atom_features, parents, calculation_orders, calculation_masks, n_atoms</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNEmbedding</code><span class="sig-paren">(</span><em>n_embedding=30</em>, <em>periodic_table_length=30</em>, <em>init='glorot_uniform'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.DTNNEmbedding</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.DTNNEmbedding
parent layers: atom_number</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNExtract</code><span class="sig-paren">(</span><em>task_id</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNExtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNExtract.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNGather</code><span class="sig-paren">(</span><em>n_embedding=30, n_outputs=100, layer_sizes=[100], output_activation=True, init='glorot_uniform', activation='tanh', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.DTNNGather</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.DTNNGather
parent layers: atom_features, atom_membership</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNStep</code><span class="sig-paren">(</span><em>n_embedding=30</em>, <em>n_distance=100</em>, <em>n_hidden=60</em>, <em>init='glorot_uniform'</em>, <em>activation='tanh'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.DTNNStep</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.DTNNStep
parent layers: atom_features, distance, distance_membership_i, distance_membership_j</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">EdgeNetwork</code><span class="sig-paren">(</span><em>pair_features</em>, <em>n_pair_features=8</em>, <em>n_hidden=100</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>Submodule for Message Passing</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>atom_features</em>, <em>atom_to_pair</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">GatedRecurrentUnit</code><span class="sig-paren">(</span><em>n_hidden=100</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>Submodule for Message Passing</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>inputs</em>, <em>messages</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">MessagePassing</code><span class="sig-paren">(</span><em>T</em>, <em>message_fn='enn'</em>, <em>update_fn='gru'</em>, <em>n_hidden=100</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>General class for MPNN
default structures built according to <a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>pair_features</em>, <em>n_pair_features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform T steps of message passing</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">SetGather</code><span class="sig-paren">(</span><em>M</em>, <em>batch_size</em>, <em>n_hidden=100</em>, <em>init='orthogonal'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>set2set gather layer for graph-based model
model using this layer must set pad_batches=True</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.LSTMStep">
<code class="descname">LSTMStep</code><span class="sig-paren">(</span><em>h</em>, <em>c</em>, <em>x=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.LSTMStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.LSTMStep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform M steps of set2set gather,
detailed descriptions in: <a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveGather</code><span class="sig-paren">(</span><em>batch_size</em>, <em>n_input=128</em>, <em>gaussian_expand=False</em>, <em>init='glorot_uniform'</em>, <em>activation='tanh'</em>, <em>epsilon=0.001</em>, <em>momentum=0.99</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.WeaveGather</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.WeaveGather
parent layers: atom_features, atom_split</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.gaussian_histogram">
<code class="descname">gaussian_histogram</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.gaussian_histogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.gaussian_histogram" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveLayer</code><span class="sig-paren">(</span><em>n_atom_input_feat=75</em>, <em>n_pair_input_feat=14</em>, <em>n_atom_output_feat=50</em>, <em>n_pair_output_feat=50</em>, <em>n_hidden_AA=50</em>, <em>n_hidden_PA=50</em>, <em>n_hidden_AP=50</em>, <em>n_hidden_PP=50</em>, <em>update_pair=True</em>, <em>init='glorot_uniform'</em>, <em>activation='relu'</em>, <em>dropout=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
The same as deepchem.nn.WeaveLayer
Note: Use WeaveLayerFactory to construct this layer</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct internal trainable weights.</p>
<p>TODO(rbharath): Need to make this not set instance variables to
follow style in other layers.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>description and explanation refer to deepchem.nn.WeaveLayer
parent layers: [atom_features, pair_features], pair_split, atom_to_pair</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayerFactory">
<code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveLayerFactory</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayerFactory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayerFactory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.layers">
<span id="deepchem-models-tensorgraph-layers-module"></span><h2>deepchem.models.tensorgraph.layers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ANIFeat</code><span class="sig-paren">(</span><em>in_layers, max_atoms=23, radial_cutoff=4.6, angular_cutoff=3.1, radial_length=32, angular_length=8, atom_cases=[1, 6, 7, 8, 16], atomic_number_differentiated=True, coordinates_in_bohr=True, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Performs transform from 3D coordinates to ANI symmetry functions</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.angular_symmetry">
<code class="descname">angular_symmetry</code><span class="sig-paren">(</span><em>d_cutoff</em>, <em>d</em>, <em>atom_numbers</em>, <em>coordinates</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.angular_symmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.angular_symmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Angular Symmetry Function</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>In layers should be of shape dtype tf.float32, (None, self.max_atoms, 4)</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.distance_cutoff">
<code class="descname">distance_cutoff</code><span class="sig-paren">(</span><em>d</em>, <em>cutoff</em>, <em>flags</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.distance_cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.distance_cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix with trainable cutoff</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.distance_matrix">
<code class="descname">distance_matrix</code><span class="sig-paren">(</span><em>coordinates</em>, <em>flags</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.get_num_feats">
<code class="descname">get_num_feats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.get_num_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.get_num_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.radial_symmetry">
<code class="descname">radial_symmetry</code><span class="sig-paren">(</span><em>d_cutoff</em>, <em>d</em>, <em>atom_numbers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.radial_symmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.radial_symmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Radial Symmetry Function</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Add">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Add</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>weights=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the (optionally weighted) sum of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Add.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Add.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Add.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.AlphaShare">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AlphaShare</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShare" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should be used when constructing AlphaShare layers from Sluice Networks</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>output_layers</strong> (<em>list of Layers or tensors with same size as in_layers</em>) &#8211;
Distance matrix.</li>
<li><em>References</em></li>
<li><strong>Sluice networks</strong> (<em>Learning what to share between loosely related tasks</em>)</li>
<li><strong>https</strong> (<em>//arxiv.org/abs/1705.08142</em>)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AlphaShareLayer</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Part of a sluice network. Adds alpha parameters to control
sharing between the main and auxillary tasks</p>
<p>Factory method AlphaShare should be used for construction</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>out_tensor</strong> (<em>a tensor with shape [len(in_layers), x, y] where x, y were the original layer dimensions</em>)</li>
<li><em>Distance matrix.</em></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AtomicConvolution</code><span class="sig-paren">(</span><em>atom_types=None</em>, <em>radial_params=[]</em>, <em>boxsize=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features.</li>
<li><strong>Nbrs</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Neighbor list.</li>
<li><strong>Nbrs_Z</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Atomic numbers of neighbor atoms.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>layer</strong> &#8211;
A new tensor representing the output of the atomic conv layer</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, l)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.distance_matrix">
<code class="descname">distance_matrix</code><span class="sig-paren">(</span><em>D</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calcuates the distance matrix from the distance tensor</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>D</strong> (<em>tf.Tensor of shape (B, N, M, d)</em>) &#8211; Distance tensor.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>R</strong> &#8211;
Distance matrix.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor of shape (B, N, M)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.distance_tensor">
<code class="descname">distance_tensor</code><span class="sig-paren">(</span><em>X</em>, <em>Nbrs</em>, <em>boxsize</em>, <em>B</em>, <em>N</em>, <em>M</em>, <em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.distance_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.distance_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates distance tensor for batch of molecules.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features tensor.</li>
<li><strong>Nbrs</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Neighbor list tensor.</li>
<li><strong>boxsize</strong> (<em>float or None</em>) &#8211; Simulation box length [Angstrom].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>D</strong> &#8211;
Coordinates/features distance tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, M, d)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.gather_neighbors">
<code class="descname">gather_neighbors</code><span class="sig-paren">(</span><em>X</em>, <em>nbr_indices</em>, <em>B</em>, <em>N</em>, <em>M</em>, <em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.gather_neighbors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.gather_neighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers the neighbor subsets of the atoms in X.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features tensor.</li>
<li><strong>atom_indices</strong> (<em>tf.Tensor of shape (B, M)</em>) &#8211; Neighbor list for single atom.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>neighbors</strong> &#8211;
Neighbor coordinates/features tensor for single atom.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, M, d)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.gaussian_distance_matrix">
<code class="descname">gaussian_distance_matrix</code><span class="sig-paren">(</span><em>R</em>, <em>rs</em>, <em>e</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.gaussian_distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.gaussian_distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates gaussian distance matrix.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>[B, N, M]</strong> (<em>R</em>) &#8211; Distance matrix.</li>
<li><strong>rs</strong> (<em>tf.Variable</em>) &#8211; Gaussian distance matrix mean.</li>
<li><strong>e</strong> (<em>tf.Variable</em>) &#8211; Gaussian distance matrix width (e = .5/std**2).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>retval [B, N, M]</strong> &#8211;
Gaussian distance matrix.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.radial_cutoff">
<code class="descname">radial_cutoff</code><span class="sig-paren">(</span><em>R</em>, <em>rc</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.radial_cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.radial_cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates radial cutoff matrix.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>[B, N, M]</strong> (<em>R</em>) &#8211; Distance matrix.</li>
<li><strong>rc</strong> (<em>tf.Variable</em>) &#8211; Interaction cutoff [Angstrom].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>FC [B, N, M]</strong> &#8211;
Radial cutoff matrix.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.radial_symmetry_function">
<code class="descname">radial_symmetry_function</code><span class="sig-paren">(</span><em>R</em>, <em>rc</em>, <em>rs</em>, <em>e</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.radial_symmetry_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.radial_symmetry_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates radial symmetry function.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_filters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>R</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Distance matrix.</li>
<li><strong>rc</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) &#8211; Interaction cutoff [Angstrom].</li>
<li><strong>rs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) &#8211; Gaussian distance matrix mean.</li>
<li><strong>e</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) &#8211; Gaussian distance matrix width.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>retval</strong> &#8211;
Radial symmetry function (before summation)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, M)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AttnLSTMEmbedding</code><span class="sig-paren">(</span><em>n_test</em>, <em>n_support</em>, <em>n_feat</em>, <em>max_depth</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Implements AttnLSTM as in matching networks paper.</p>
<p>The AttnLSTM embedding adjusts two sets of vectors, the &#8220;test&#8221; and
&#8220;support&#8221; sets. The &#8220;support&#8221; consists of a set of evidence vectors.
Think of these as the small training set for low-data machine
learning.  The &#8220;test&#8221; consists of the queries we wish to answer with
the small amounts ofavailable data. The AttnLSTMEmbdding allows us to
modify the embedding of the &#8220;test&#8221; set depending on the contents of
the &#8220;support&#8221;.  The AttnLSTMEmbedding is thus a type of learnable
metric that allows a network to modify its internal notion of
distance.</p>
<p>References:
Matching Networks for One Shot Learning
<a class="reference external" href="https://arxiv.org/pdf/1606.04080v1.pdf">https://arxiv.org/pdf/1606.04080v1.pdf</a></p>
<p>Order Matters: Sequence to sequence for sets
<a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of two tensors (X, Xp). X should be of shape (n_test,
n_feat) and Xp should be of shape (n_support, n_feat) where
n_test is the size of the test set, n_support that of the support
set, and n_feat is the number of per-atom features.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns two tensors of same shape as input. Namely the output
shape will be [(n_test, n_feat), (n_support, n_feat)]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BatchNorm</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNorm.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BatchNormalization</code><span class="sig-paren">(</span><em>epsilon=1e-05</em>, <em>axis=-1</em>, <em>momentum=0.99</em>, <em>beta_init='zero'</em>, <em>gamma_init='one'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.add_weight">
<code class="descname">add_weight</code><span class="sig-paren">(</span><em>shape</em>, <em>initializer</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.add_weight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.add_weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BetaShare">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BetaShare</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Part of a sluice network. Adds beta params to control which layer
outputs are used for prediction</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>output_layers</strong> &#8211;
Distance matrix.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of Layers or tensors with same size as in_layers</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Size of input layers must all be the same</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">CombineMeanStd</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>training_only=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#CombineMeanStd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Generate Gaussian nose.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#CombineMeanStd.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Concat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Concat</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Concat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Concat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Concat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Constant">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Constant</code><span class="sig-paren">(</span><em>value</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Output a constant value.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Constant.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Constant.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Constant.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv1D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv1D</code><span class="sig-paren">(</span><em>width</em>, <em>out_channels</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.RandomNormal'&gt;</em>, <em>weights_initializer=&lt;class 'tensorflow.python.ops.init_ops.RandomNormal'&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 1D convolution on the input.</p>
<p>This layer expects its input to be a three dimensional tensor of shape (batch size, width, # channels).
If there is only one channel, the third dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv1D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv2D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv2D</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A 2D convolution on the input.</p>
<p>This layer expects its input to be a four dimensional tensor of shape (batch size, height, width, # channels).
If there is only one channel, the fourth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv2DTranspose</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2DTranspose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A transposed 2D convolution on the input.</p>
<p>This layer is typically used for upsampling in a deconvolutional network.  It
expects its input to be a four dimensional tensor of shape (batch size, height, width, # channels).
If there is only one channel, the fourth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2DTranspose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv3D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv3D</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A 3D convolution on the input.</p>
<p>This layer expects its input to be a five dimensional tensor of shape
(batch size, height, width, depth, # channels).
If there is only one channel, the fifth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv3DTranspose</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3DTranspose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A transposed 3D convolution on the input.</p>
<p>This layer is typically used for upsampling in a deconvolutional network.  It
expects its input to be a five dimensional tensor of shape (batch size, height, width, depth, # channels).
If there is only one channel, the fifth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3DTranspose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Dense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Dense</code><span class="sig-paren">(</span><em>out_channels</em>, <em>activation_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function variance_scaling_initializer&gt;</em>, <em>time_series=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Divide">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Divide</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Divide"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the ratio of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Divide.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Divide.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Divide.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Dropout">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>dropout_prob</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dropout.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dropout.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dropout.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Exp">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Exp</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Exp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the exponential of the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Exp.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Exp.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Exp.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Feature">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Feature</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Feature"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Feature.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Feature.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Flatten">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Flatten</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Flatten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Flatten every dimension except the first</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Flatten.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Flatten.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Flatten.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GRU">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>n_hidden</em>, <em>batch_size</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A Gated Recurrent Unit.</p>
<p>This layer expects its input to be of shape (batch_size, sequence_length, ...).
It consists of a set of independent sequence (one for each element in the batch),
that are each propagated independently through the GRU.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GRU.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GRU.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Gather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Gather</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>indices=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Gather elements or slices from the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Gather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Gather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Gather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphCNN</code><span class="sig-paren">(</span><em>num_filters</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>GraphCNN Layer from Robust Spatial Filtering with Graph Convolutional Neural Networks
<a class="reference external" href="https://arxiv.org/abs/1703.00792">https://arxiv.org/abs/1703.00792</a></p>
<p>Spatial-domain convolutions can be defined as
H = h_0I + h_1A + h_2A^2 + ... + hkAk, H ∈ R**(N×N)</p>
<p>We approximate it by
H ≈ h_0I + h_1A</p>
<p>We can define a convolution as applying multiple these linear filters
over edges of different types (think up, down, left, right, diagonal in images)
Where each edge type has its own adjacency matrix
H ≈ h_0I + h_1A_1 + h_2A_2 + . . . h_(L−1)A_(L−1)</p>
<p>V_out = sum_{c=1}^{C} H^{c} V^{c} + b</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.batch_mat_mult">
<code class="descname">batch_mat_mult</code><span class="sig-paren">(</span><em>A</em>, <em>B</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.batch_mat_mult"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.batch_mat_mult" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.graphConvolution">
<code class="descname">graphConvolution</code><span class="sig-paren">(</span><em>V</em>, <em>A</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.graphConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.graphConvolution" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.GraphCNNPool">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphCNNPool</code><span class="sig-paren">(</span><em>num_vertices</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNNPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNNPool" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphConv">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphConv</code><span class="sig-paren">(</span><em>out_channel</em>, <em>min_deg=0</em>, <em>max_deg=10</em>, <em>activation_fn=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.sum_neigh">
<code class="descname">sum_neigh</code><span class="sig-paren">(</span><em>atoms</em>, <em>deg_adj_lists</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.sum_neigh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.sum_neigh" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the summed atoms by degree</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphEmbedPoolLayer</code><span class="sig-paren">(</span><em>num_vertices</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>GraphCNNPool Layer from Robust Spatial Filtering with Graph Convolutional Neural Networks
<a class="reference external" href="https://arxiv.org/abs/1703.00792">https://arxiv.org/abs/1703.00792</a></p>
<p>This is a learnable pool operation
It constructs a new adjacency matrix for a graph of specified number of nodes.</p>
<p>This differs from our other pool opertions which set vertices to a function value
without altering the adjacency matrix.</p>
<p>$V_{emb} = SpatialGraphCNN({V_{in}})$$V_{out} = sigma(V_{emb})^{T} * V_{in}$
$A_{out} = V_{emb}^{T} * A_{in} * V_{emb}$</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_filters</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; Number of filters to have in the output</li>
<li><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; <p>[V, A, mask]
V are the vertex features must be of shape (batch, vertex, channel)</p>
<dl class="docutils">
<dt>A are the adjacency matrixes for each graph</dt>
<dd>Shape (batch, from_vertex, adj_matrix, to_vertex)</dd>
</dl>
<p>mask is optional, to be used when not every graph has the
same number of vertices</p>
</li>
<li><strong>Returns</strong> (<em>tf.tensor</em>) &#8211; </li>
<li><strong>a tf.tensor with a graph convolution applied</strong> (<em>Returns</em>) &#8211; </li>
<li><strong>shape will be (batch, vertex, self.num_filters)</strong> (<em>The</em>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.embedding_factors">
<code class="descname">embedding_factors</code><span class="sig-paren">(</span><em>V</em>, <em>no_filters</em>, <em>name='default'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.embedding_factors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.embedding_factors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.softmax_factors">
<code class="descname">softmax_factors</code><span class="sig-paren">(</span><em>V</em>, <em>axis=1</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.softmax_factors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.softmax_factors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphGather</code><span class="sig-paren">(</span><em>batch_size</em>, <em>activation_fn=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphPool">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphPool</code><span class="sig-paren">(</span><em>min_degree=0</em>, <em>max_degree=10</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphPool.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Highway">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Highway</code><span class="sig-paren">(</span><em>activation_fn=&lt;function relu&gt;</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function variance_scaling_initializer&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Highway"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Create a highway layer. y = H(x) * T(x) + x * (1 - T(x))
H(x) = activation_fn(matmul(W_H, x) + b_H) is the non-linear transformed output
T(x) = sigmoid(matmul(W_T, x) + b_T) is the transform gate</p>
<p>reference: <a class="reference external" href="https://arxiv.org/pdf/1505.00387.pdf">https://arxiv.org/pdf/1505.00387.pdf</a></p>
<p>This layer expects its input to be a two dimensional tensor of shape (batch size, # input features).
Outputs will be in the same shape.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Highway.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Highway.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Highway.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Input">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Input</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.create_pre_q"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.get_pre_q_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Input.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Input.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">InputFifoQueue</code><span class="sig-paren">(</span><em>shapes</em>, <em>names</em>, <em>capacity=5</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>This Queue Is used to allow asynchronous batching of inputs
During the fitting process</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">InteratomicL2Distances</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InteratomicL2Distances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute (squared) L2 Distances between atoms given neighbors.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InteratomicL2Distances.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">IterRefLSTMEmbedding</code><span class="sig-paren">(</span><em>n_test</em>, <em>n_support</em>, <em>n_feat</em>, <em>max_depth</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Implements the Iterative Refinement LSTM.</p>
<p>Much like AttnLSTMEmbedding, the IterRefLSTMEmbedding is another type
of learnable metric which adjusts &#8220;test&#8221; and &#8220;support.&#8221; Recall that
&#8220;support&#8221; is the small amount of data available in a low data machine
learning problem, and that &#8220;test&#8221; is the query. The AttnLSTMEmbedding
only modifies the &#8220;test&#8221; based on the contents of the support.
However, the IterRefLSTM modifies both the &#8220;support&#8221; and &#8220;test&#8221; based
on each other. This allows the learnable metric to be more malleable
than that from AttnLSTMEmbeding.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of two tensors (X, Xp). X should be of shape (n_test, n_feat) and
Xp should be of shape (n_support, n_feat) where n_test is the size of
the test set, n_support that of the support set, and n_feat is the number
of per-atom features.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns two tensors of same shape as input. Namely the output shape will
be [(n_test, n_feat), (n_support, n_feat)]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.L1Loss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L1Loss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.L2Loss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">L2Loss</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L2Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L2Loss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">LSTMStep</code><span class="sig-paren">(</span><em>output_dim</em>, <em>input_dim</em>, <em>init_fn=&lt;function glorot_uniform&gt;</em>, <em>inner_init_fn=&lt;function orthogonal&gt;</em>, <em>activation_fn=&lt;function tanh&gt;</em>, <em>inner_activation_fn=&lt;function hard_sigmoid&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Layer that performs a single step LSTM update.</p>
<p>This layer performs a single step LSTM update. Note that it is <em>not</em>
a full LSTM recurrent network. The LSTMStep layer is useful as a
primitive for designing layers such as the AttnLSTMEmbedding or the
IterRefLSTMEmbedding below.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs learnable weights for this layer.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of three tensors (x, h_tm1, c_tm1). h_tm1 means &#8220;h, t-1&#8221;.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns h, [h + c]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)">list</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.get_initial_states">
<code class="descname">get_initial_states</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.get_initial_states"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.get_initial_states" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Zeros out stored tensors for pickling.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets all stored tensors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Label">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Label</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Label"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Label.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Label.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Layer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Layer</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.add_summary_to_tg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.clone"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.copy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Layer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_variable_initial_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Layer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">LayerSplitter</code><span class="sig-paren">(</span><em>output_num</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LayerSplitter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Layer which takes a tensor from in_tensor[0].out_tensors at an index
Only layers which need to output multiple layers set and use the variable
self.out_tensors.
This is a utility for those special layers which set self.out_tensors
to return a layer wrapping a specific tensor in in_layers[0].out_tensors</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LayerSplitter.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Log">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Log</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the natural log of the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Log.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Log.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Log.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool1D</code><span class="sig-paren">(</span><em>window_shape=2</em>, <em>strides=1</em>, <em>padding='SAME'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 1D max pooling on the input.</p>
<p>This layer expects its input to be a three dimensional tensor of shape
(batch size, width, # channels).</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool1D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool2D</code><span class="sig-paren">(</span><em>ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool2D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool3D</code><span class="sig-paren">(</span><em>ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 3D max pooling on the input.</p>
<p>This layer expects its input to be a five dimensional tensor of shape
(batch size, height, width, depth, # channels).</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool3D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Multiply">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Multiply</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Multiply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the product of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Multiply.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Multiply.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Multiply.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.NeighborList">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">NeighborList</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>nbr_cutoff</em>, <em>start</em>, <em>stop</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes a neighbor-list in Tensorflow.</p>
<p>Neighbor-lists (also called Verlet Lists) are a tool for grouping atoms which
are close to each other spatially</p>
<p>TODO(rbharath): Make this layer support batching.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.compute_nbr_list">
<code class="descname">compute_nbr_list</code><span class="sig-paren">(</span><em>coords</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.compute_nbr_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.compute_nbr_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get closest neighbors for atoms.</p>
<p>Needs to handle padding for atoms with no neighbors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; Shape (N_atoms, ndim)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>nbr_list</strong> &#8211;
Shape (N_atoms, M_nbrs) of atom indices</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates tensors associated with neighbor-listing.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_atoms_in_nbrs">
<code class="descname">get_atoms_in_nbrs</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_atoms_in_nbrs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_atoms_in_nbrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the atoms in neighboring cells for each cells.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">atoms_in_nbrs = (N_atoms, n_nbr_cells, M_nbrs)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_cells">
<code class="descname">get_cells</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_cells"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_cells" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the locations of all grid points in box.</p>
<p>Suppose start is -10 Angstrom, stop is 10 Angstrom, nbr_cutoff is 1.
Then would return a list of length 20^3 whose entries would be
[(-10, -10, -10), (-10, -10, -9), ..., (9, 9, 9)]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>cells</strong> &#8211;
(n_cells, ndim) shape.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_cells_for_atoms">
<code class="descname">get_cells_for_atoms</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_cells_for_atoms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_cells_for_atoms" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cells each atom belongs to.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; Shape (N_atoms, ndim)</li>
<li><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>cells_for_atoms</strong> &#8211;
Shape (N_atoms, 1)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_closest_atoms">
<code class="descname">get_closest_atoms</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_closest_atoms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_closest_atoms" title="Permalink to this definition">¶</a></dt>
<dd><p>For each cell, find M_nbrs closest atoms.</p>
<p>Let N_atoms be the number of atoms.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; (N_atoms, ndim) shape.</li>
<li><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>closest_inds</strong> &#8211;
Of shape (n_cells, M_nbrs)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_neighbor_cells">
<code class="descname">get_neighbor_cells</code><span class="sig-paren">(</span><em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_neighbor_cells"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_neighbor_cells" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute neighbors of cells in grid.</p>
<p># TODO(rbharath): Do we need to handle periodic boundary conditions
properly here?
# TODO(rbharath): This doesn&#8217;t handle boundaries well. We hard-code
# looking for n_nbr_cells neighbors, which isn&#8217;t right for boundary cells in
# the cube.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>nbr_cells</strong> &#8211;
(n_cells, n_nbr_cells)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceMean</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMean.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceSquareDifference</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSquareDifference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSquareDifference.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceSum</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSum.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Repeat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Repeat</code><span class="sig-paren">(</span><em>n_times</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Repeat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Repeat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Repeat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Reshape">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Reshape</code><span class="sig-paren">(</span><em>shape</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Reshape.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Reshape.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Reshape.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SharedVariableScope</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SharedVariableScope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A Layer that can share variables with another layer via name scope.</p>
<p>This abstract class can be used as a parent for any layer that implements
shared() by means of the variable name scope.  It exists to avoid duplicated
code.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SharedVariableScope.shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SluiceLoss</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SluiceLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Calculates the loss in a Sluice Network
Every input into an AlphaShare should be used in SluiceLoss</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SluiceLoss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SoftMax">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SoftMax</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMax.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SoftMaxCrossEntropy</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMaxCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMaxCrossEntropy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SparseSoftMaxCrossEntropy</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SparseSoftMaxCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SparseSoftMaxCrossEntropy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Squeeze">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Squeeze</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>squeeze_dims=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Squeeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Squeeze.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Stack">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Stack</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Stack.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Stack.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Stack.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.StopGradient">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">StopGradient</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#StopGradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Block the flow of gradients.</p>
<p>This layer copies its input directly to its output, but reports that all
gradients of its output are zero.  This means, for example, that optimizers
will not try to optimize anything &#8220;upstream&#8221; of this layer.</p>
<p>For example, suppose you have pre-trained a stack of layers to perform a
calculation.  You want to use the result of that calculation as the input to
another layer, but because they are already pre-trained, you do not want the
optimizer to modify them.  You can wrap the output in a StopGradient layer,
then use that as the input to the next layer.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#StopGradient.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">TensorWrapper</code><span class="sig-paren">(</span><em>out_tensor</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TensorWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Used to wrap a tensorflow tensor.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TensorWrapper.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Take no actions.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">TimeSeriesDense</code><span class="sig-paren">(</span><em>out_channels</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TimeSeriesDense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TimeSeriesDense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ToFloat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ToFloat</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ToFloat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ToFloat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Transpose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Transpose</code><span class="sig-paren">(</span><em>perm</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Transpose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Transpose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Transpose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Variable">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Variable</code><span class="sig-paren">(</span><em>initial_value</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Output a trainable value.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Variable.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Variable.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Variable.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">VinaFreeEnergy</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>nbr_cutoff</em>, <em>start</em>, <em>stop</em>, <em>stddev=0.3</em>, <em>Nrot=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes free-energy as defined by Autodock Vina.</p>
<p>TODO(rbharath): Make this layer support batching.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (N, d)</em>) &#8211; Coordinates/features.</li>
<li><strong>Z</strong> (<em>tf.Tensor of shape (N)</em>) &#8211; Atomic numbers of neighbor atoms.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>layer</strong> &#8211;
The free energy of each complex in batch</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.cutoff">
<code class="descname">cutoff</code><span class="sig-paren">(</span><em>d</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.cutoff" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_first">
<code class="descname">gaussian_first</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.gaussian_first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s first Gaussian interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_second">
<code class="descname">gaussian_second</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.gaussian_second"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_second" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s second Gaussian interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrogen_bond">
<code class="descname">hydrogen_bond</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.hydrogen_bond"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrogen_bond" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s hydrogen bond interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrophobic">
<code class="descname">hydrophobic</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.hydrophobic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrophobic" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s hydrophobic interaction term.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.nonlinearity">
<code class="descname">nonlinearity</code><span class="sig-paren">(</span><em>c</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.nonlinearity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.nonlinearity" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes non-linearity used in Vina.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.repulsion">
<code class="descname">repulsion</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.repulsion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.repulsion" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s repulsion interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightDecay</code><span class="sig-paren">(</span><em>penalty</em>, <em>penalty_type</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Apply a weight decay penalty.</p>
<p>The input should be the loss value.  This layer adds a weight decay penalty to it
and outputs the sum.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightDecay.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightedError">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightedError</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedError"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedError.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightedLinearCombo</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>std=0.3</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedLinearCombo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes a weighted linear combination of input layers, with the weights defined by trainable variables.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedLinearCombo.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Weights">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Weights</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Weights.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Weights.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.convert_to_layers">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">convert_to_layers</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#convert_to_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.convert_to_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrap all inputs into tensors if necessary.</p>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.optimizers">
<span id="deepchem-models-tensorgraph-optimizers-module"></span><h2>deepchem.models.tensorgraph.optimizers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.optimizers" title="Permalink to this headline">¶</a></h2>
<p>Optimizers and related classes for use with TensorGraph.</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.Adam">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">Adam</code><span class="sig-paren">(</span><em>learning_rate=0.001</em>, <em>beta1=0.9</em>, <em>beta2=0.999</em>, <em>epsilon=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#Adam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.Adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.Optimizer</span></code></a></p>
<p>The Adam optimization algorithm.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.ExponentialDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">ExponentialDecay</code><span class="sig-paren">(</span><em>initial_rate</em>, <em>decay_rate</em>, <em>decay_steps</em>, <em>staircase=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#ExponentialDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.ExponentialDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="deepchem.models.tensorgraph.optimizers.LearningRateSchedule"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.LearningRateSchedule</span></code></a></p>
<p>A learning rate that decreases exponentially with the number of training steps.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.GradientDescent">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">GradientDescent</code><span class="sig-paren">(</span><em>learning_rate=0.001</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#GradientDescent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.GradientDescent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.Optimizer</span></code></a></p>
<p>The gradient descent optimization algorithm.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.LearningRateSchedule">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">LearningRateSchedule</code><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#LearningRateSchedule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>A schedule for changing the learning rate over the course of optimization.</p>
<p>This is an abstract class.  Subclasses represent specific schedules.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.Optimizer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">Optimizer</code><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#Optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>An algorithm for optimizing a TensorGraph based model.</p>
<p>This is an abstract class.  Subclasses represent specific optimization algorithms.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.PolynomialDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">PolynomialDecay</code><span class="sig-paren">(</span><em>initial_rate</em>, <em>final_rate</em>, <em>decay_steps</em>, <em>power=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#PolynomialDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.PolynomialDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="deepchem.models.tensorgraph.optimizers.LearningRateSchedule"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.LearningRateSchedule</span></code></a></p>
<p>A learning rate that decreases from an initial value to a final value over a fixed number of training steps.</p>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.symmetry_functions">
<span id="deepchem-models-tensorgraph-symmetry-functions-module"></span><h2>deepchem.models.tensorgraph.symmetry_functions module<a class="headerlink" href="#module-deepchem.models.tensorgraph.symmetry_functions" title="Permalink to this headline">¶</a></h2>
<p>Created on Thu Jul  6 20:43:23 2017</p>
<p>&#64;author: zqwu</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AngularSymmetry</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>lambd_init=None</em>, <em>ita_init=None</em>, <em>zeta_init=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Angular Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Angular Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AngularSymmetryMod</code><span class="sig-paren">(</span><em>max_atoms, lambd_init=None, ita_init=None, zeta_init=None, Rs_init=None, thetas_init=None, atomic_number_differentiated=False, atom_numbers=[1, 6, 7, 8], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Angular Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Angular Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AtomicDifferentiatedDense</code><span class="sig-paren">(</span><em>max_atoms, out_channels, atom_number_cases=[1, 6, 7, 8], init='glorot_uniform', activation='relu', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Separate Dense module for different atoms</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Radial Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">BPFeatureMerge</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPFeatureMerge"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPFeatureMerge.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge features together</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">BPGather</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge features together</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">DistanceCutoff</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>cutoff=11.338356747390371</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix for BPSymmetryFunction with trainable cutoff</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">DistanceMatrix</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceMatrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceMatrix.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix for BPSymmetryFunction with trainable cutoff</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">RadialSymmetry</code><span class="sig-paren">(</span><em>max_atoms, Rs_init=None, ita_init=None, atomic_number_differentiated=False, atom_numbers=[1, 6, 7, 8], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Radial Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameters for the Gaussian</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v3.6)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Radial Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.tensor_graph">
<span id="deepchem-models-tensorgraph-tensor-graph-module"></span><h2>deepchem.models.tensorgraph.tensor_graph module<a class="headerlink" href="#module-deepchem.models.tensorgraph.tensor_graph" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.Submodel">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">Submodel</code><span class="sig-paren">(</span><em>graph</em>, <em>layers</em>, <em>loss</em>, <em>optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#Submodel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>An alternate objective for training one piece of a TensorGraph.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.Submodel.get_train_op">
<code class="descname">get_train_op</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#Submodel.get_train_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.Submodel.get_train_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Tensorflow operator to use for training.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.TFWrapper">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">TFWrapper</code><span class="sig-paren">(</span><em>tf_class</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TFWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TFWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>This class exists as a workaround for Tensorflow objects not being picklable.</p>
<p>The job of a TFWrapper is to create Tensorflow objects by passing defined arguments
to a constructor.  There are cases where we really want to store Tensorflow objects
of various sorts (optimizers, initializers, etc.), but we can&#8217;t because they cannot
be pickled.  So instead we store a TFWrapper that creates the object when needed.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">TensorGraph</code><span class="sig-paren">(</span><em>tensorboard=False</em>, <em>tensorboard_log_frequency=100</em>, <em>batch_size=100</em>, <em>random_seed=None</em>, <em>use_queue=True</em>, <em>graph=None</em>, <em>learning_rate=0.001</em>, <em>configproto=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="deepchem.models.html#deepchem.models.models.Model" title="deepchem.models.models.Model"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.models.Model</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.add_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.create_submodel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="deepchem.nn.html#deepchem.nn.copy.Layer" title="deepchem.nn.copy.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.6)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.evaluate_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_global_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_layer_variables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_num_tasks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_pickling_errors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_pre_q_input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.load_from_dir">
<em class="property">static </em><code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.load_from_dir"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_on_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v3.6)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba_on_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.restore"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from the most recent checkpoint file.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.save_checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.set_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.set_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.topsort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-deepchem.models.tensorgraph" title="Permalink to this headline">¶</a></h2>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="_sources/deepchem.models.tensorgraph.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>