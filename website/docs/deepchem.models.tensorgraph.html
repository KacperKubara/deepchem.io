<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.models.tensorgraph package &mdash; deepchem 1.3.1 documentation</title>
    
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.3.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.3.1 documentation" href="index.html" />
    <link rel="up" title="deepchem.models package" href="deepchem.models.html" />
    <link rel="next" title="deepchem.models.tensorgraph.models package" href="deepchem.models.tensorgraph.models.html" />
    <link rel="prev" title="deepchem.models.sklearn_models package" href="deepchem.models.sklearn_models.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">deepchem.models.tensorgraph package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.IRV">deepchem.models.tensorgraph.IRV module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.activations">deepchem.models.tensorgraph.activations module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.fcnet">deepchem.models.tensorgraph.fcnet module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.graph_layers">deepchem.models.tensorgraph.graph_layers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.initializations">deepchem.models.tensorgraph.initializations module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.layers">deepchem.models.tensorgraph.layers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.model_ops">deepchem.models.tensorgraph.model_ops module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.optimizers">deepchem.models.tensorgraph.optimizers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.progressive_multitask">deepchem.models.tensorgraph.progressive_multitask module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.regularizers">deepchem.models.tensorgraph.regularizers module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.robust_multitask">deepchem.models.tensorgraph.robust_multitask module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.sequential">deepchem.models.tensorgraph.sequential module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.symmetry_functions">deepchem.models.tensorgraph.symmetry_functions module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph.tensor_graph">deepchem.models.tensorgraph.tensor_graph module</a></li>
<li><a class="reference internal" href="#module-deepchem.models.tensorgraph">Module contents</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="deepchem-models-tensorgraph-package">
<h1>deepchem.models.tensorgraph package<a class="headerlink" href="#deepchem-models-tensorgraph-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="deepchem.models.tensorgraph.models.html">deepchem.models.tensorgraph.models package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.atomic_conv">deepchem.models.tensorgraph.models.atomic_conv module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.gan">deepchem.models.tensorgraph.models.gan module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.graph_models">deepchem.models.tensorgraph.models.graph_models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.robust_multitask">deepchem.models.tensorgraph.models.robust_multitask module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.seqtoseq">deepchem.models.tensorgraph.models.seqtoseq module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.sequence_dnn">deepchem.models.tensorgraph.models.sequence_dnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.symmetry_function_regression">deepchem.models.tensorgraph.models.symmetry_function_regression module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.test_graph_models">deepchem.models.tensorgraph.models.test_graph_models module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.test_symmetry_functions">deepchem.models.tensorgraph.models.test_symmetry_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models.text_cnn">deepchem.models.tensorgraph.models.text_cnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.models.html#module-deepchem.models.tensorgraph.models">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html">deepchem.models.tensorgraph.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_gan">deepchem.models.tensorgraph.tests.test_gan module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_layers">deepchem.models.tensorgraph.tests.test_layers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_layers_pickle">deepchem.models.tensorgraph.tests.test_layers_pickle module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_model_ops">deepchem.models.tensorgraph.tests.test_model_ops module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_nbr_list">deepchem.models.tensorgraph.tests.test_nbr_list module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_optimizers">deepchem.models.tensorgraph.tests.test_optimizers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_seqtoseq">deepchem.models.tensorgraph.tests.test_seqtoseq module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_sequencednn">deepchem.models.tensorgraph.tests.test_sequencednn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_sequential">deepchem.models.tensorgraph.tests.test_sequential module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests.test_tensor_graph">deepchem.models.tensorgraph.tests.test_tensor_graph module</a></li>
<li class="toctree-l2"><a class="reference internal" href="deepchem.models.tensorgraph.tests.html#module-deepchem.models.tensorgraph.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-deepchem.models.tensorgraph.IRV">
<span id="deepchem-models-tensorgraph-irv-module"></span><h2>deepchem.models.tensorgraph.IRV module<a class="headerlink" href="#module-deepchem.models.tensorgraph.IRV" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.IRV.</code><code class="descname">IRVLayer</code><span class="sig-paren">(</span><em>n_tasks</em>, <em>K</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Core layer of IRV classifier, architecture described in:
<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2750043/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2750043/</a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVLayer.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.IRV.</code><code class="descname">IRVRegularize</code><span class="sig-paren">(</span><em>IRVLayer</em>, <em>penalty=0.0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVRegularize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Extracts the trainable weights in IRVLayer
and return their L2-norm
No in_layers is required, but should be built after target IRVLayer</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#IRVRegularize.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.IRVRegularize.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.IRVRegularize.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.IRV.Slice">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.IRV.</code><code class="descname">Slice</code><span class="sig-paren">(</span><em>slice_num</em>, <em>axis=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#Slice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Choose a slice of input on the last axis given order,
Suppose input x has two dimensions,
output f(x) = x[:, slice_num:slice_num+1]</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#Slice.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.Slice.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.IRV.Slice.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.Slice.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.Slice.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.IRV.</code><code class="descname">TensorflowMultiTaskIRVClassifier</code><span class="sig-paren">(</span><em>n_tasks</em>, <em>K=10</em>, <em>penalty=0.0</em>, <em>mode='classification'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#TensorflowMultiTaskIRVClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.build_graph">
<code class="descname">build_graph</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#TensorflowMultiTaskIRVClassifier.build_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.build_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs the graph architecture of IRV as described in:</p>
<p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2750043/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2750043/</a></p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#TensorflowMultiTaskIRVClassifier.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.default_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>TensorGraph style implementation</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#TensorflowMultiTaskIRVClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/IRV.html#TensorflowMultiTaskIRVClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.IRV.TensorflowMultiTaskIRVClassifier.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.activations">
<span id="deepchem-models-tensorgraph-activations-module"></span><h2>deepchem.models.tensorgraph.activations module<a class="headerlink" href="#module-deepchem.models.tensorgraph.activations" title="Permalink to this headline">¶</a></h2>
<p>Activations for models.</p>
<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.elu">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>x</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#elu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.elu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.get">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">get</code><span class="sig-paren">(</span><em>identifier</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#get"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.get_from_module">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">get_from_module</code><span class="sig-paren">(</span><em>identifier</em>, <em>module_params</em>, <em>module_name</em>, <em>instantiate=False</em>, <em>kwargs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#get_from_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.get_from_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves a class of function member of a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>identifier</strong> (<em>the object to retrieve. It could be specified</em>) &#8211; by name (as a string), or by dict. In any other case,
identifier itself will be returned without any changes.</li>
<li><strong>module_params</strong> (<em>the members of a module</em>) &#8211; (e.g. the output of globals()).</li>
<li><strong>module_name</strong> (<em>string; the name of the target module. Only used</em>) &#8211; to format error messages.</li>
<li><strong>instantiate</strong> (<em>whether to instantiate the returned object</em>) &#8211; (if it&#8217;s a class).</li>
<li><strong>kwargs</strong> (<em>a dictionary of keyword arguments to pass to the</em>) &#8211; class constructor if <cite>instantiate</cite> is <cite>True</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">The target object.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">ValueError: if the identifier cannot be found.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.hard_sigmoid">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">hard_sigmoid</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#hard_sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.hard_sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>The hard sigmoidal activation function</p>
<p>Piecewise-linear approximation to sigmoid.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) &#8211; Input tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.linear">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>A linear activation function.</p>
<p>Note that a linear activation function is simply the identity.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) &#8211; Input tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.relu">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>x</em>, <em>alpha=0.0</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>The rectified linear activation function</p>
<p>Wrapper around model_ops.relu.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) &#8211; Input tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.selu">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.selu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.sigmoid">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>The sigmoidal activation function</p>
<p>Wrapper around tf.nn.sigmoid.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) &#8211; Input tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.softmax">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.softmax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.softplus">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.softsign">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.softsign" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.activations.tanh">
<code class="descclassname">deepchem.models.tensorgraph.activations.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/activations.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.activations.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>The hyperbolic tanget activation function</p>
<p>Wrapper around tf.nn.tanh.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) &#8211; Input tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.fcnet">
<span id="deepchem-models-tensorgraph-fcnet-module"></span><h2>deepchem.models.tensorgraph.fcnet module<a class="headerlink" href="#module-deepchem.models.tensorgraph.fcnet" title="Permalink to this headline">¶</a></h2>
<p>TensorFlow implementation of fully connected networks.</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.fcnet.</code><code class="descname">MultiTaskClassifier</code><span class="sig-paren">(</span><em>n_tasks, n_features, layer_sizes=[1000], weight_init_stddevs=0.02, bias_init_consts=1.0, weight_decay_penalty=0.0, weight_decay_penalty_type='l2', dropouts=0.5, activation_fns=&lt;function relu&gt;, n_classes=2, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskClassifier.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskClassifier.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.fcnet.</code><code class="descname">MultiTaskFitTransformRegressor</code><span class="sig-paren">(</span><em>n_tasks</em>, <em>n_features</em>, <em>fit_transformers=[]</em>, <em>n_evals=1</em>, <em>batch_size=50</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskFitTransformRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor" title="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.fcnet.MultiTaskRegressor</span></code></a></p>
<p>Implements a MultiTaskRegressor that performs on-the-fly transformation during fit/predict.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_features</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_tasks</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_tasks</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_tasks</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fit_transformers</span> <span class="o">=</span> <span class="p">[</span><span class="n">dc</span><span class="o">.</span><span class="n">trans</span><span class="o">.</span><span class="n">CoulombFitTransformer</span><span class="p">(</span><span class="n">dataset</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MultiTaskFitTransformRegressor</span><span class="p">(</span><span class="n">n_tasks</span><span class="p">,</span> <span class="p">[</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">dropouts</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">weight_init_stddevs</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1000</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">fit_transformers</span><span class="o">=</span><span class="n">fit_transformers</span><span class="p">,</span> <span class="n">n_evals</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">n_features after fit_transform: 12</span>
</pre></div>
</div>
<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskFitTransformRegressor.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskFitTransformRegressor.predict_on_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskFitTransformRegressor.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.fcnet.</code><code class="descname">MultiTaskRegressor</code><span class="sig-paren">(</span><em>n_tasks, n_features, layer_sizes=[1000], weight_init_stddevs=0.02, bias_init_consts=1.0, weight_decay_penalty=0.0, weight_decay_penalty_type='l2', dropouts=0.5, activation_fns=&lt;function relu&gt;, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/fcnet.html#MultiTaskRegressor.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.fcnet.MultiTaskRegressor.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.graph_layers">
<span id="deepchem-models-tensorgraph-graph-layers-module"></span><h2>deepchem.models.tensorgraph.graph_layers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.graph_layers" title="Permalink to this headline">¶</a></h2>
<p>Created on Thu Mar 30 14:02:04 2017</p>
<p>&#64;author: michael</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DAGGather</code><span class="sig-paren">(</span><em>n_graph_feat=30, n_outputs=30, max_atoms=50, layer_sizes=[100], init='glorot_uniform', activation='relu', dropout=None, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.DAGgraph_step">
<code class="descname">DAGgraph_step</code><span class="sig-paren">(</span><em>batch_inputs</em>, <em>W_list</em>, <em>b_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.DAGgraph_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.DAGgraph_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.build" title="Permalink to this definition">¶</a></dt>
<dd><p>&#8220;Construct internal trainable weights.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_features, membership</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DAGLayer</code><span class="sig-paren">(</span><em>n_graph_feat=30, n_atom_feat=75, max_atoms=50, layer_sizes=[100], init='glorot_uniform', activation='relu', dropout=None, batch_size=64, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.DAGgraph_step">
<code class="descname">DAGgraph_step</code><span class="sig-paren">(</span><em>batch_inputs</em>, <em>W_list</em>, <em>b_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.DAGgraph_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.DAGgraph_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>&#8220;Construct internal trainable weights.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_features, parents, calculation_orders, calculation_masks, n_atoms</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DAGLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DAGLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DAGLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNEmbedding</code><span class="sig-paren">(</span><em>n_embedding=30</em>, <em>periodic_table_length=30</em>, <em>init='glorot_uniform'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_number</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNExtract</code><span class="sig-paren">(</span><em>task_id</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNExtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNExtract.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNExtract.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNExtract.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNGather</code><span class="sig-paren">(</span><em>n_embedding=30, n_outputs=100, layer_sizes=[100], output_activation=True, init='glorot_uniform', activation='tanh', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_features, atom_membership</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">DTNNStep</code><span class="sig-paren">(</span><em>n_embedding=30</em>, <em>n_distance=100</em>, <em>n_hidden=60</em>, <em>init='glorot_uniform'</em>, <em>activation='tanh'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_features, distance, distance_membership_i, distance_membership_j</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#DTNNStep.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.DTNNStep.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.DTNNStep.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">EdgeNetwork</code><span class="sig-paren">(</span><em>pair_features</em>, <em>n_pair_features=8</em>, <em>n_hidden=100</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>Submodule for Message Passing</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>atom_features</em>, <em>atom_to_pair</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.EdgeNetwork.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#EdgeNetwork.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.EdgeNetwork.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">GatedRecurrentUnit</code><span class="sig-paren">(</span><em>n_hidden=100</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>Submodule for Message Passing</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>inputs</em>, <em>messages</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#GatedRecurrentUnit.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.GatedRecurrentUnit.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">MessagePassing</code><span class="sig-paren">(</span><em>T</em>, <em>message_fn='enn'</em>, <em>update_fn='gru'</em>, <em>n_hidden=100</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>General class for MPNN
default structures built according to <a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>pair_features</em>, <em>n_pair_features</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform T steps of message passing</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#MessagePassing.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.MessagePassing.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.MessagePassing.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">SetGather</code><span class="sig-paren">(</span><em>M</em>, <em>batch_size</em>, <em>n_hidden=100</em>, <em>init='orthogonal'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>set2set gather layer for graph-based model
model using this layer must set pad_batches=True</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.LSTMStep">
<code class="descname">LSTMStep</code><span class="sig-paren">(</span><em>h</em>, <em>c</em>, <em>x=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.LSTMStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.LSTMStep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform M steps of set2set gather,
detailed descriptions in: <a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#SetGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.SetGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.SetGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveGather</code><span class="sig-paren">(</span><em>batch_size</em>, <em>n_input=128</em>, <em>gaussian_expand=False</em>, <em>init='glorot_uniform'</em>, <em>activation='tanh'</em>, <em>epsilon=0.001</em>, <em>momentum=0.99</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>parent layers: atom_features, atom_split</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.gaussian_histogram">
<code class="descname">gaussian_histogram</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.gaussian_histogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.gaussian_histogram" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveGather.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveLayer</code><span class="sig-paren">(</span><em>n_atom_input_feat=75</em>, <em>n_pair_input_feat=14</em>, <em>n_atom_output_feat=50</em>, <em>n_pair_output_feat=50</em>, <em>n_hidden_AA=50</em>, <em>n_hidden_PA=50</em>, <em>n_hidden_AP=50</em>, <em>n_hidden_PP=50</em>, <em>update_pair=True</em>, <em>init='glorot_uniform'</em>, <em>activation='relu'</em>, <em>dropout=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>TensorGraph style implementation
Note: Use WeaveLayerFactory to construct this layer</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct internal trainable weights.</p>
<p>TODO(rbharath): Need to make this not set instance variables to
follow style in other layers.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates weave tensors.</p>
<p>parent layers: [atom_features, pair_features], pair_split, atom_to_pair</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.graph_layers.WeaveLayerFactory">
<code class="descclassname">deepchem.models.tensorgraph.graph_layers.</code><code class="descname">WeaveLayerFactory</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/graph_layers.html#WeaveLayerFactory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.graph_layers.WeaveLayerFactory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.initializations">
<span id="deepchem-models-tensorgraph-initializations-module"></span><h2>deepchem.models.tensorgraph.initializations module<a class="headerlink" href="#module-deepchem.models.tensorgraph.initializations" title="Permalink to this headline">¶</a></h2>
<p>Ops for tensor initialization</p>
<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.get">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">get</code><span class="sig-paren">(</span><em>identifier</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#get"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.get_fans">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">get_fans</code><span class="sig-paren">(</span><em>shape</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#get_fans"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.get_fans" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.glorot_normal">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">glorot_normal</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#glorot_normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.glorot_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Glorot normal variance scaling initializer.</p>
<dl class="docutils">
<dt># References</dt>
<dd>Glorot &amp; Bengio, AISTATS 2010</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.glorot_uniform">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">glorot_uniform</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#glorot_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.glorot_uniform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.he_normal">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">he_normal</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#he_normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.he_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>He normal variance scaling initializer.</p>
<dl class="docutils">
<dt># References</dt>
<dd>He et al., <a class="reference external" href="http://arxiv.org/abs/1502.01852">http://arxiv.org/abs/1502.01852</a></dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.he_uniform">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">he_uniform</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#he_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.he_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>He uniform variance scaling initializer.</p>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.identity">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">identity</code><span class="sig-paren">(</span><em>shape</em>, <em>scale=1</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.identity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.lecun_uniform">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">lecun_uniform</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#lecun_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.lecun_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>LeCun uniform variance scaling initializer.</p>
<dl class="docutils">
<dt># References</dt>
<dd>LeCun 98, Efficient Backprop,
<a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a></dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.normal">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>shape</em>, <em>scale=0.05</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.normal" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.one">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">one</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#one"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.one" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.orthogonal">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">orthogonal</code><span class="sig-paren">(</span><em>shape</em>, <em>scale=1.1</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#orthogonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.orthogonal" title="Permalink to this definition">¶</a></dt>
<dd><p>Orthogonal initializer.</p>
<dl class="docutils">
<dt># References</dt>
<dd>Saxe et al., <a class="reference external" href="http://arxiv.org/abs/1312.6120">http://arxiv.org/abs/1312.6120</a></dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.uniform">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">uniform</code><span class="sig-paren">(</span><em>shape</em>, <em>scale=0.05</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.uniform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.initializations.zero">
<code class="descclassname">deepchem.models.tensorgraph.initializations.</code><code class="descname">zero</code><span class="sig-paren">(</span><em>shape</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/initializations.html#zero"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.initializations.zero" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.layers">
<span id="deepchem-models-tensorgraph-layers-module"></span><h2>deepchem.models.tensorgraph.layers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ANIFeat</code><span class="sig-paren">(</span><em>in_layers, max_atoms=23, radial_cutoff=4.6, angular_cutoff=3.1, radial_length=32, angular_length=8, atom_cases=[1, 6, 7, 8, 16], atomic_number_differentiated=True, coordinates_in_bohr=True, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Performs transform from 3D coordinates to ANI symmetry functions</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.angular_symmetry">
<code class="descname">angular_symmetry</code><span class="sig-paren">(</span><em>d_cutoff</em>, <em>d</em>, <em>atom_numbers</em>, <em>coordinates</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.angular_symmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.angular_symmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Angular Symmetry Function</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>In layers should be of shape dtype tf.float32, (None, self.max_atoms, 4)</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.distance_cutoff">
<code class="descname">distance_cutoff</code><span class="sig-paren">(</span><em>d</em>, <em>cutoff</em>, <em>flags</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.distance_cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.distance_cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix with trainable cutoff</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.distance_matrix">
<code class="descname">distance_matrix</code><span class="sig-paren">(</span><em>coordinates</em>, <em>flags</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.get_num_feats">
<code class="descname">get_num_feats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.get_num_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.get_num_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.radial_symmetry">
<code class="descname">radial_symmetry</code><span class="sig-paren">(</span><em>d_cutoff</em>, <em>d</em>, <em>atom_numbers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ANIFeat.radial_symmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.radial_symmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Radial Symmetry Function</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ANIFeat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ANIFeat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Add">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Add</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>weights=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the (optionally weighted) sum of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Add.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Add.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Add.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Add.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Add.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.AlphaShare">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AlphaShare</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShare" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should be used when constructing AlphaShare layers from Sluice Networks</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>output_layers</strong> (<em>list of Layers or tensors with same size as in_layers</em>) &#8211;
Distance matrix.</li>
<li><em>References</em></li>
<li><strong>Sluice networks</strong> (<em>Learning what to share between loosely related tasks</em>)</li>
<li><strong>https</strong> (<em>//arxiv.org/abs/1705.08142</em>)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AlphaShareLayer</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Part of a sluice network. Adds alpha parameters to control
sharing between the main and auxillary tasks</p>
<p>Factory method AlphaShare should be used for construction</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>out_tensor</strong> (<em>a tensor with shape [len(in_layers), x, y] where x, y were the original layer dimensions</em>)</li>
<li><em>Distance matrix.</em></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AlphaShareLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AlphaShareLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AlphaShareLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AtomicConvolution</code><span class="sig-paren">(</span><em>atom_types=None</em>, <em>radial_params=[]</em>, <em>boxsize=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features.</li>
<li><strong>Nbrs</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Neighbor list.</li>
<li><strong>Nbrs_Z</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Atomic numbers of neighbor atoms.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>layer</strong> &#8211;
A new tensor representing the output of the atomic conv layer</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, l)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.distance_matrix">
<code class="descname">distance_matrix</code><span class="sig-paren">(</span><em>D</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calcuates the distance matrix from the distance tensor</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>D</strong> (<em>tf.Tensor of shape (B, N, M, d)</em>) &#8211; Distance tensor.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>R</strong> &#8211;
Distance matrix.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor of shape (B, N, M)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.distance_tensor">
<code class="descname">distance_tensor</code><span class="sig-paren">(</span><em>X</em>, <em>Nbrs</em>, <em>boxsize</em>, <em>B</em>, <em>N</em>, <em>M</em>, <em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.distance_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.distance_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates distance tensor for batch of molecules.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features tensor.</li>
<li><strong>Nbrs</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Neighbor list tensor.</li>
<li><strong>boxsize</strong> (<em>float or None</em>) &#8211; Simulation box length [Angstrom].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>D</strong> &#8211;
Coordinates/features distance tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, M, d)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.gather_neighbors">
<code class="descname">gather_neighbors</code><span class="sig-paren">(</span><em>X</em>, <em>nbr_indices</em>, <em>B</em>, <em>N</em>, <em>M</em>, <em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.gather_neighbors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.gather_neighbors" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers the neighbor subsets of the atoms in X.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_features</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (B, N, d)</em>) &#8211; Coordinates/features tensor.</li>
<li><strong>atom_indices</strong> (<em>tf.Tensor of shape (B, M)</em>) &#8211; Neighbor list for single atom.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>neighbors</strong> &#8211;
Neighbor coordinates/features tensor for single atom.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, M, d)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.gaussian_distance_matrix">
<code class="descname">gaussian_distance_matrix</code><span class="sig-paren">(</span><em>R</em>, <em>rs</em>, <em>e</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.gaussian_distance_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.gaussian_distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates gaussian distance matrix.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>[B, N, M]</strong> (<em>R</em>) &#8211; Distance matrix.</li>
<li><strong>rs</strong> (<em>tf.Variable</em>) &#8211; Gaussian distance matrix mean.</li>
<li><strong>e</strong> (<em>tf.Variable</em>) &#8211; Gaussian distance matrix width (e = .5/std**2).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>retval [B, N, M]</strong> &#8211;
Gaussian distance matrix.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.radial_cutoff">
<code class="descname">radial_cutoff</code><span class="sig-paren">(</span><em>R</em>, <em>rc</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.radial_cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.radial_cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates radial cutoff matrix.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>[B, N, M]</strong> (<em>R</em>) &#8211; Distance matrix.</li>
<li><strong>rc</strong> (<em>tf.Variable</em>) &#8211; Interaction cutoff [Angstrom].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>FC [B, N, M]</strong> &#8211;
Radial cutoff matrix.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.radial_symmetry_function">
<code class="descname">radial_symmetry_function</code><span class="sig-paren">(</span><em>R</em>, <em>rc</em>, <em>rs</em>, <em>e</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AtomicConvolution.radial_symmetry_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.radial_symmetry_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates radial symmetry function.</p>
<p>B = batch_size, N = max_num_atoms, M = max_num_neighbors, d = num_filters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>R</strong> (<em>tf.Tensor of shape (B, N, M)</em>) &#8211; Distance matrix.</li>
<li><strong>rc</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Interaction cutoff [Angstrom].</li>
<li><strong>rs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Gaussian distance matrix mean.</li>
<li><strong>e</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Gaussian distance matrix width.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>retval</strong> &#8211;
Radial symmetry function (before summation)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B, N, M)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AtomicConvolution.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AtomicConvolution.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">AttnLSTMEmbedding</code><span class="sig-paren">(</span><em>n_test</em>, <em>n_support</em>, <em>n_feat</em>, <em>max_depth</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Implements AttnLSTM as in matching networks paper.</p>
<p>The AttnLSTM embedding adjusts two sets of vectors, the &#8220;test&#8221; and
&#8220;support&#8221; sets. The &#8220;support&#8221; consists of a set of evidence vectors.
Think of these as the small training set for low-data machine
learning.  The &#8220;test&#8221; consists of the queries we wish to answer with
the small amounts ofavailable data. The AttnLSTMEmbdding allows us to
modify the embedding of the &#8220;test&#8221; set depending on the contents of
the &#8220;support&#8221;.  The AttnLSTMEmbedding is thus a type of learnable
metric that allows a network to modify its internal notion of
distance.</p>
<p>References:
Matching Networks for One Shot Learning
<a class="reference external" href="https://arxiv.org/pdf/1606.04080v1.pdf">https://arxiv.org/pdf/1606.04080v1.pdf</a></p>
<p>Order Matters: Sequence to sequence for sets
<a class="reference external" href="https://arxiv.org/abs/1511.06391">https://arxiv.org/abs/1511.06391</a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list</em>) &#8211; List of two tensors (X, Xp). X should be of shape (n_test,
n_feat) and Xp should be of shape (n_support, n_feat) where
n_test is the size of the test set, n_support that of the support
set, and n_feat is the number of per-atom features.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns two tensors of same shape as input. Namely the output
shape will be [(n_test, n_feat), (n_support, n_feat)]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#AttnLSTMEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.AttnLSTMEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BatchNorm</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNorm.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNorm.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNorm.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BatchNormalization</code><span class="sig-paren">(</span><em>epsilon=1e-05</em>, <em>axis=-1</em>, <em>momentum=0.99</em>, <em>beta_init='zero'</em>, <em>gamma_init='one'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.add_weight">
<code class="descname">add_weight</code><span class="sig-paren">(</span><em>shape</em>, <em>initializer</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.add_weight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.add_weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BatchNormalization.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BatchNormalization.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BatchNormalization.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.BetaShare">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">BetaShare</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Part of a sluice network. Adds beta params to control which layer
outputs are used for prediction</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; tensors in list must be the same size and list must include two or more tensors</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>output_layers</strong> &#8211;
Distance matrix.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of Layers or tensors with same size as in_layers</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Size of input layers must all be the same</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#BetaShare.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.BetaShare.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.BetaShare.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Cast">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Cast</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Cast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Wrapper around tf.cast.  Changes the dtype of a single layer</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Cast.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Cast.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Cast.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Cast.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Cast.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">CombineMeanStd</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>training_only=False</em>, <em>noise_epsilon=0.01</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#CombineMeanStd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Generate Gaussian nose.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#CombineMeanStd.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.CombineMeanStd.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.CombineMeanStd.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Concat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Concat</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Concat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Concat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Concat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Concat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Concat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Constant">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Constant</code><span class="sig-paren">(</span><em>value</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Output a constant value.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Constant.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Constant.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Constant.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Constant.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Constant.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv1D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv1D</code><span class="sig-paren">(</span><em>filters</em>, <em>kernel_size</em>, <em>strides=1</em>, <em>padding='valid'</em>, <em>dilation_rate=1</em>, <em>activation=None</em>, <em>use_bias=True</em>, <em>kernel_initializer='glorot_uniform'</em>, <em>bias_initializer='zeros'</em>, <em>kernel_regularizer=None</em>, <em>bias_regularizer=None</em>, <em>activity_regularizer=None</em>, <em>kernel_constraint=None</em>, <em>bias_constraint=None</em>, <em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 1D convolution on the input.</p>
<p>This layer expects its input to be a three dimensional tensor of shape (batch size, width, # channels).
If there is only one channel, the third dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv1D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv1D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv1D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv2D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv2D</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A 2D convolution on the input.</p>
<p>This layer expects its input to be a four dimensional tensor of shape (batch size, height, width, # channels).
If there is only one channel, the fourth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2D.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv2DTranspose</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2DTranspose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A transposed 2D convolution on the input.</p>
<p>This layer is typically used for upsampling in a deconvolutional network.  It
expects its input to be a four dimensional tensor of shape (batch size, height, width, # channels).
If there is only one channel, the fourth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv2DTranspose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv2DTranspose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv2DTranspose.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv3D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv3D</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A 3D convolution on the input.</p>
<p>This layer expects its input to be a five dimensional tensor of shape
(batch size, height, width, depth, # channels).
If there is only one channel, the fifth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3D.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Conv3DTranspose</code><span class="sig-paren">(</span><em>num_outputs</em>, <em>kernel_size=5</em>, <em>stride=1</em>, <em>padding='SAME'</em>, <em>activation_fn=&lt;function relu&gt;</em>, <em>normalizer_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function xavier_initializer&gt;</em>, <em>scope_name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3DTranspose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<p>A transposed 3D convolution on the input.</p>
<p>This layer is typically used for upsampling in a deconvolutional network.  It
expects its input to be a five dimensional tensor of shape (batch size, height, width, depth, # channels).
If there is only one channel, the fifth dimension may optionally be omitted.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Conv3DTranspose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Conv3DTranspose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Conv3DTranspose.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Dense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Dense</code><span class="sig-paren">(</span><em>out_channels</em>, <em>activation_fn=None</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function variance_scaling_initializer&gt;</em>, <em>time_series=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="deepchem.models.tensorgraph.layers.SharedVariableScope"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.SharedVariableScope</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dense.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Divide">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Divide</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Divide"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the ratio of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Divide.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Divide.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Divide.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Divide.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Divide.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Dropout">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>dropout_prob</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Dropout.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dropout.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Dropout.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Dropout.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Dropout.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Exp">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Exp</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Exp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the exponential of the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Exp.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Exp.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Exp.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Exp.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Exp.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Feature">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Feature</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Feature"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Feature.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Feature.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Feature.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Feature.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Flatten">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Flatten</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Flatten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Flatten every dimension except the first</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Flatten.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Flatten.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Flatten.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Flatten.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Flatten.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GRU">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>n_hidden</em>, <em>batch_size</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A Gated Recurrent Unit.</p>
<p>This layer expects its input to be of shape (batch_size, sequence_length, ...).
It consists of a set of independent sequences (one for each element in the batch),
that are each propagated independently through the GRU.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GRU.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GRU.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GRU.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GRU.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GRU.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Gather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Gather</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>indices=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Gather elements or slices from the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Gather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Gather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Gather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Gather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Gather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphCNN</code><span class="sig-paren">(</span><em>num_filters</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>GraphCNN Layer from Robust Spatial Filtering with Graph Convolutional Neural Networks
<a class="reference external" href="https://arxiv.org/abs/1703.00792">https://arxiv.org/abs/1703.00792</a></p>
<p>Spatial-domain convolutions can be defined as
H = h_0I + h_1A + h_2A^2 + ... + hkAk, H ∈ R**(N×N)</p>
<p>We approximate it by
H ≈ h_0I + h_1A</p>
<p>We can define a convolution as applying multiple these linear filters
over edges of different types (think up, down, left, right, diagonal in images)
Where each edge type has its own adjacency matrix
H ≈ h_0I + h_1A_1 + h_2A_2 + . . . h_(L−1)A_(L−1)</p>
<p>V_out = sum_{c=1}^{C} H^{c} V^{c} + b</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.batch_mat_mult">
<code class="descname">batch_mat_mult</code><span class="sig-paren">(</span><em>A</em>, <em>B</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.batch_mat_mult"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.batch_mat_mult" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.graphConvolution">
<code class="descname">graphConvolution</code><span class="sig-paren">(</span><em>V</em>, <em>A</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNN.graphConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.graphConvolution" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphCNN.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNN.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.GraphCNNPool">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphCNNPool</code><span class="sig-paren">(</span><em>num_vertices</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphCNNPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphCNNPool" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphConv">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphConv</code><span class="sig-paren">(</span><em>out_channel</em>, <em>min_deg=0</em>, <em>max_deg=10</em>, <em>activation_fn=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphConv.sum_neigh">
<code class="descname">sum_neigh</code><span class="sig-paren">(</span><em>atoms</em>, <em>deg_adj_lists</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphConv.sum_neigh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphConv.sum_neigh" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the summed atoms by degree</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphEmbedPoolLayer</code><span class="sig-paren">(</span><em>num_vertices</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>GraphCNNPool Layer from Robust Spatial Filtering with Graph Convolutional Neural Networks
<a class="reference external" href="https://arxiv.org/abs/1703.00792">https://arxiv.org/abs/1703.00792</a></p>
<p>This is a learnable pool operation
It constructs a new adjacency matrix for a graph of specified number of nodes.</p>
<p>This differs from our other pool opertions which set vertices to a function value
without altering the adjacency matrix.</p>
<p>$V_{emb} = SpatialGraphCNN({V_{in}})$$V_{out} = sigma(V_{emb})^{T} * V_{in}$
$A_{out} = V_{emb}^{T} * A_{in} * V_{emb}$</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_filters</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of filters to have in the output</li>
<li><strong>in_layers</strong> (<em>list of Layers or tensors</em>) &#8211; <p>[V, A, mask]
V are the vertex features must be of shape (batch, vertex, channel)</p>
<dl class="docutils">
<dt>A are the adjacency matrixes for each graph</dt>
<dd>Shape (batch, from_vertex, adj_matrix, to_vertex)</dd>
</dl>
<p>mask is optional, to be used when not every graph has the
same number of vertices</p>
</li>
<li><strong>Returns</strong> (<em>tf.tensor</em>) &#8211; </li>
<li><strong>a tf.tensor with a graph convolution applied</strong> (<em>Returns</em>) &#8211; </li>
<li><strong>shape will be (batch, vertex, self.num_filters)</strong> (<em>The</em>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.embedding_factors">
<code class="descname">embedding_factors</code><span class="sig-paren">(</span><em>V</em>, <em>no_filters</em>, <em>name='default'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.embedding_factors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.embedding_factors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.softmax_factors">
<code class="descname">softmax_factors</code><span class="sig-paren">(</span><em>V</em>, <em>axis=1</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphEmbedPoolLayer.softmax_factors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphEmbedPoolLayer.softmax_factors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphGather</code><span class="sig-paren">(</span><em>batch_size</em>, <em>activation_fn=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.GraphPool">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">GraphPool</code><span class="sig-paren">(</span><em>min_degree=0</em>, <em>max_degree=10</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#GraphPool.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.GraphPool.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.GraphPool.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Highway">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Highway</code><span class="sig-paren">(</span><em>activation_fn=&lt;function relu&gt;</em>, <em>biases_initializer=&lt;class 'tensorflow.python.ops.init_ops.Zeros'&gt;</em>, <em>weights_initializer=&lt;function variance_scaling_initializer&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Highway"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Create a highway layer. y = H(x) * T(x) + x * (1 - T(x))
H(x) = activation_fn(matmul(W_H, x) + b_H) is the non-linear transformed output
T(x) = sigmoid(matmul(W_T, x) + b_T) is the transform gate</p>
<p>reference: <a class="reference external" href="https://arxiv.org/pdf/1505.00387.pdf">https://arxiv.org/pdf/1505.00387.pdf</a></p>
<p>This layer expects its input to be a two dimensional tensor of shape (batch size, # input features).
Outputs will be in the same shape.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Highway.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Highway.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Highway.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Highway.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Highway.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Input">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Input</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.create_pre_q"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Input.get_pre_q_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Input.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Input.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Input.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Input.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">InputFifoQueue</code><span class="sig-paren">(</span><em>shapes</em>, <em>names</em>, <em>capacity=5</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>This Queue Is used to allow asynchronous batching of inputs
During the fitting process</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InputFifoQueue.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InputFifoQueue.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InputFifoQueue.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">InteratomicL2Distances</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InteratomicL2Distances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute (squared) L2 Distances between atoms given neighbors.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#InteratomicL2Distances.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.InteratomicL2Distances.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.InteratomicL2Distances.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">IterRefLSTMEmbedding</code><span class="sig-paren">(</span><em>n_test</em>, <em>n_support</em>, <em>n_feat</em>, <em>max_depth</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Implements the Iterative Refinement LSTM.</p>
<p>Much like AttnLSTMEmbedding, the IterRefLSTMEmbedding is another type
of learnable metric which adjusts &#8220;test&#8221; and &#8220;support.&#8221; Recall that
&#8220;support&#8221; is the small amount of data available in a low data machine
learning problem, and that &#8220;test&#8221; is the query. The AttnLSTMEmbedding
only modifies the &#8220;test&#8221; based on the contents of the support.
However, the IterRefLSTM modifies both the &#8220;support&#8221; and &#8220;test&#8221; based
on each other. This allows the learnable metric to be more malleable
than that from AttnLSTMEmbeding.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list</em>) &#8211; List of two tensors (X, Xp). X should be of shape (n_test, n_feat) and
Xp should be of shape (n_support, n_feat) where n_test is the size of
the test set, n_support that of the support set, and n_feat is the number
of per-atom features.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns two tensors of same shape as input. Namely the output shape will
be [(n_test, n_feat), (n_support, n_feat)]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#IterRefLSTMEmbedding.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.IterRefLSTMEmbedding.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.L1Loss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the mean absolute difference between the elements of the inputs.</p>
<p>This layer should have two or three inputs.  If there is a third input, the
difference between the first two inputs is multiplied by the third one to
produce a weighted error.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L1Loss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L1Loss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L1Loss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.L2Loss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">L2Loss</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L2Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the mean squared difference between the elements of the inputs.</p>
<p>This layer should have two or three inputs.  If there is a third input, the
squared difference between the first two inputs is multiplied by the third one to
produce a weighted error.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#L2Loss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.L2Loss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.L2Loss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.LSTM">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>n_hidden</em>, <em>batch_size</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A Long Short Term Memory.</p>
<p>This layer expects its input to be of shape (batch_size, sequence_length, ...).
It consists of a set of independent sequences (one for each element in the batch),
that are each propagated independently through the LSTM.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTM.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTM.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTM.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTM.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTM.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTM.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTM.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">LSTMStep</code><span class="sig-paren">(</span><em>output_dim</em>, <em>input_dim</em>, <em>init_fn=&lt;function glorot_uniform&gt;</em>, <em>inner_init_fn=&lt;function orthogonal&gt;</em>, <em>activation_fn=&lt;function tanh&gt;</em>, <em>inner_activation_fn=&lt;function hard_sigmoid&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Layer that performs a single step LSTM update.</p>
<p>This layer performs a single step LSTM update. Note that it is <em>not</em>
a full LSTM recurrent network. The LSTMStep layer is useful as a
primitive for designing layers such as the AttnLSTMEmbedding or the
IterRefLSTMEmbedding below.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs learnable weights for this layer.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute this layer on input tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>in_layers</strong> (<em>list</em>) &#8211; List of three tensors (x, h_tm1, c_tm1). h_tm1 means &#8220;h, t-1&#8221;.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Returns h, [h + c]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.get_initial_states">
<code class="descname">get_initial_states</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.get_initial_states"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.get_initial_states" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Zeros out stored tensors for pickling.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LSTMStep.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets all stored tensors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LSTMStep.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LSTMStep.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Label">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Label</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Label"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Label.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Label.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Label.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Label.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Layer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Layer</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.add_summary_to_tg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.clone"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.copy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Layer.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.set_variable_initial_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Layer.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Layer.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Layer.shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Layer.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">LayerSplitter</code><span class="sig-paren">(</span><em>output_num</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LayerSplitter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Layer which takes a tensor from in_tensor[0].out_tensors at an index
Only layers which need to output multiple layers set and use the variable
self.out_tensors.
This is a utility for those special layers which set self.out_tensors
to return a layer wrapping a specific tensor in in_layers[0].out_tensors</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#LayerSplitter.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.LayerSplitter.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.LayerSplitter.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Log">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Log</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the natural log of the input.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Log.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Log.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Log.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Log.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Log.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool1D</code><span class="sig-paren">(</span><em>window_shape=2</em>, <em>strides=1</em>, <em>padding='SAME'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 1D max pooling on the input.</p>
<p>This layer expects its input to be a three dimensional tensor of shape
(batch size, width, # channels).</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool1D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool1D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool1D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool2D</code><span class="sig-paren">(</span><em>ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool2D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool2D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool2D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">MaxPool3D</code><span class="sig-paren">(</span><em>ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A 3D max pooling on the input.</p>
<p>This layer expects its input to be a five dimensional tensor of shape
(batch size, height, width, depth, # channels).</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#MaxPool3D.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.MaxPool3D.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.MaxPool3D.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Multiply">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Multiply</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Multiply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the product of the input layers.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Multiply.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Multiply.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Multiply.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Multiply.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Multiply.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.NeighborList">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">NeighborList</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>nbr_cutoff</em>, <em>start</em>, <em>stop</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes a neighbor-list in Tensorflow.</p>
<p>Neighbor-lists (also called Verlet Lists) are a tool for grouping atoms which
are close to each other spatially</p>
<p>TODO(rbharath): Make this layer support batching.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.compute_nbr_list">
<code class="descname">compute_nbr_list</code><span class="sig-paren">(</span><em>coords</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.compute_nbr_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.compute_nbr_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get closest neighbors for atoms.</p>
<p>Needs to handle padding for atoms with no neighbors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; Shape (N_atoms, ndim)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>nbr_list</strong> &#8211;
Shape (N_atoms, M_nbrs) of atom indices</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates tensors associated with neighbor-listing.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_atoms_in_nbrs">
<code class="descname">get_atoms_in_nbrs</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_atoms_in_nbrs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_atoms_in_nbrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the atoms in neighboring cells for each cells.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">atoms_in_nbrs = (N_atoms, n_nbr_cells, M_nbrs)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_cells">
<code class="descname">get_cells</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_cells"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_cells" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the locations of all grid points in box.</p>
<p>Suppose start is -10 Angstrom, stop is 10 Angstrom, nbr_cutoff is 1.
Then would return a list of length 20^3 whose entries would be
[(-10, -10, -10), (-10, -10, -9), ..., (9, 9, 9)]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>cells</strong> &#8211;
(n_cells, ndim) shape.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_cells_for_atoms">
<code class="descname">get_cells_for_atoms</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_cells_for_atoms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_cells_for_atoms" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cells each atom belongs to.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; Shape (N_atoms, ndim)</li>
<li><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>cells_for_atoms</strong> &#8211;
Shape (N_atoms, 1)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_closest_atoms">
<code class="descname">get_closest_atoms</code><span class="sig-paren">(</span><em>coords</em>, <em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_closest_atoms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_closest_atoms" title="Permalink to this definition">¶</a></dt>
<dd><p>For each cell, find M_nbrs closest atoms.</p>
<p>Let N_atoms be the number of atoms.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>coords</strong> (<em>tf.Tensor</em>) &#8211; (N_atoms, ndim) shape.</li>
<li><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>closest_inds</strong> &#8211;
Of shape (n_cells, M_nbrs)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.get_neighbor_cells">
<code class="descname">get_neighbor_cells</code><span class="sig-paren">(</span><em>cells</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#NeighborList.get_neighbor_cells"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.get_neighbor_cells" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute neighbors of cells in grid.</p>
<p># TODO(rbharath): Do we need to handle periodic boundary conditions
properly here?
# TODO(rbharath): This doesn&#8217;t handle boundaries well. We hard-code
# looking for n_nbr_cells neighbors, which isn&#8217;t right for boundary cells in
# the cube.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cells</strong> (<em>tf.Tensor</em>) &#8211; (n_cells, ndim) shape.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>nbr_cells</strong> &#8211;
(n_cells, n_nbr_cells)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.NeighborList.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.NeighborList.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReLU">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the relu activation of input: f(x) = relu(x)
Only one input is allowed, output will have the same shape as input</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReLU.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReLU.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReLU.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReLU.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReLU.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceMax</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMax.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMax.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMax.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceMean</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceMean.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceMean.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceMean.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceSquareDifference</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSquareDifference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSquareDifference.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSquareDifference.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSquareDifference.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ReduceSum</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ReduceSum.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ReduceSum.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ReduceSum.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Repeat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Repeat</code><span class="sig-paren">(</span><em>n_times</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Repeat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Repeat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Repeat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Repeat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Repeat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Repeat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Reshape">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Reshape</code><span class="sig-paren">(</span><em>shape</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Reshape.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Reshape.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Reshape.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Reshape.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Reshape.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SharedVariableScope</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SharedVariableScope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>A Layer that can share variables with another layer via name scope.</p>
<p>This abstract class can be used as a parent for any layer that implements
shared() by means of the variable name scope.  It exists to avoid duplicated
code.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SharedVariableScope.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SharedVariableScope.shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SharedVariableScope.shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Sigmoid</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the sigmoid of input: f(x) = sigmoid(x)
Only one input is allowed, output will have the same shape as input</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Sigmoid.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Sigmoid.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Sigmoid.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SigmoidCrossEntropy</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SigmoidCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Compute the sigmoid cross entropy of inputs: [labels, logits]
<cite>labels</cite> hold the binary labels(with no axis of n_classes),
<cite>logits</cite> hold the log probabilities for positive class(label=1),
<cite>labels</cite> and <cite>logits</cite> should have same shape and type.
Output will have the same shape as <cite>logits</cite></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SigmoidCrossEntropy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SigmoidCrossEntropy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SluiceLoss</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SluiceLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Calculates the loss in a Sluice Network
Every input into an AlphaShare should be used in SluiceLoss</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SluiceLoss.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SluiceLoss.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SluiceLoss.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SoftMax">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SoftMax</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMax.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMax.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMax.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SoftMaxCrossEntropy</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMaxCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SoftMaxCrossEntropy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SoftMaxCrossEntropy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">SparseSoftMaxCrossEntropy</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SparseSoftMaxCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#SparseSoftMaxCrossEntropy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.SparseSoftMaxCrossEntropy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Squeeze">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Squeeze</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>squeeze_dims=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Squeeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Squeeze.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Squeeze.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Squeeze.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Stack">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Stack</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>axis=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Stack.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Stack.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Stack.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Stack.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Stack.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.StopGradient">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">StopGradient</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#StopGradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Block the flow of gradients.</p>
<p>This layer copies its input directly to its output, but reports that all
gradients of its output are zero.  This means, for example, that optimizers
will not try to optimize anything &#8220;upstream&#8221; of this layer.</p>
<p>For example, suppose you have pre-trained a stack of layers to perform a
calculation.  You want to use the result of that calculation as the input to
another layer, but because they are already pre-trained, you do not want the
optimizer to modify them.  You can wrap the output in a StopGradient layer,
then use that as the input to the next layer.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#StopGradient.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.StopGradient.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.StopGradient.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">TensorWrapper</code><span class="sig-paren">(</span><em>out_tensor</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TensorWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Used to wrap a tensorflow tensor.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TensorWrapper.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Take no actions.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TensorWrapper.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TensorWrapper.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">TimeSeriesDense</code><span class="sig-paren">(</span><em>out_channels</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TimeSeriesDense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#TimeSeriesDense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.TimeSeriesDense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.TimeSeriesDense.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.ToFloat">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">ToFloat</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ToFloat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#ToFloat.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.ToFloat.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.ToFloat.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Transpose">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Transpose</code><span class="sig-paren">(</span><em>perm</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Transpose.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Transpose.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Transpose.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Transpose.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Transpose.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Variable">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Variable</code><span class="sig-paren">(</span><em>initial_value</em>, <em>dtype=tf.float32</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Output a trainable value.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Variable.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Variable.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Variable.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Variable.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Variable.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">VinaFreeEnergy</code><span class="sig-paren">(</span><em>N_atoms</em>, <em>M_nbrs</em>, <em>ndim</em>, <em>nbr_cutoff</em>, <em>start</em>, <em>stop</em>, <em>stddev=0.3</em>, <em>Nrot=1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes free-energy as defined by Autodock Vina.</p>
<p>TODO(rbharath): Make this layer support batching.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>tf.Tensor of shape (N, d)</em>) &#8211; Coordinates/features.</li>
<li><strong>Z</strong> (<em>tf.Tensor of shape (N)</em>) &#8211; Atomic numbers of neighbor atoms.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>layer</strong> &#8211;
The free energy of each complex in batch</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor of shape (B)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.cutoff">
<code class="descname">cutoff</code><span class="sig-paren">(</span><em>d</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.cutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.cutoff" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_first">
<code class="descname">gaussian_first</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.gaussian_first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s first Gaussian interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_second">
<code class="descname">gaussian_second</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.gaussian_second"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.gaussian_second" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s second Gaussian interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrogen_bond">
<code class="descname">hydrogen_bond</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.hydrogen_bond"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrogen_bond" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s hydrogen bond interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrophobic">
<code class="descname">hydrophobic</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.hydrophobic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.hydrophobic" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s hydrophobic interaction term.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.nonlinearity">
<code class="descname">nonlinearity</code><span class="sig-paren">(</span><em>c</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.nonlinearity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.nonlinearity" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes non-linearity used in Vina.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.repulsion">
<code class="descname">repulsion</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#VinaFreeEnergy.repulsion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.repulsion" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Autodock Vina&#8217;s repulsion interaction term.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.VinaFreeEnergy.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.VinaFreeEnergy.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightDecay</code><span class="sig-paren">(</span><em>penalty</em>, <em>penalty_type</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Apply a weight decay penalty.</p>
<p>The input should be the loss value.  This layer adds a weight decay penalty to it
and outputs the sum.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightDecay.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightDecay.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightDecay.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightedError">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightedError</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedError"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedError.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedError.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedError.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">WeightedLinearCombo</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>std=0.3</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedLinearCombo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Computes a weighted linear combination of input layers, with the weights defined by trainable variables.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#WeightedLinearCombo.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.WeightedLinearCombo.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.WeightedLinearCombo.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.layers.Weights">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">Weights</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#Weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Input" title="deepchem.models.tensorgraph.layers.Input"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Input</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.create_pre_q">
<code class="descname">create_pre_q</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.create_pre_q" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.get_pre_q_name">
<code class="descname">get_pre_q_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.get_pre_q_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Weights.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.layers.Weights.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.layers.Weights.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.layers.Weights.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.layers.convert_to_layers">
<code class="descclassname">deepchem.models.tensorgraph.layers.</code><code class="descname">convert_to_layers</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/layers.html#convert_to_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.layers.convert_to_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrap all inputs into tensors if necessary.</p>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.model_ops">
<span id="deepchem-models-tensorgraph-model-ops-module"></span><h2>deepchem.models.tensorgraph.model_ops module<a class="headerlink" href="#module-deepchem.models.tensorgraph.model_ops" title="Permalink to this headline">¶</a></h2>
<p>Ops for graph construction.</p>
<p>Large amounts of code borrowed from Keras. Will try to incorporate into
DeepChem properly.</p>
<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.add_bias">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">add_bias</code><span class="sig-paren">(</span><em>tensor</em>, <em>init=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#add_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.add_bias" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a bias term to a tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<em>tf.Tensor</em>) &#8211; Variable tensor.</li>
<li><strong>init</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Bias initializer. Defaults to zero.</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; Name for this op. Defaults to tensor.op.name.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A biased tensor with the same shape as the input tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.binary_crossentropy">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">binary_crossentropy</code><span class="sig-paren">(</span><em>output</em>, <em>target</em>, <em>from_logits=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#binary_crossentropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.binary_crossentropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary crossentropy between an output tensor and a target tensor.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><p class="first">output: A tensor.
target: A tensor with the same shape as <cite>output</cite>.
from_logits: Whether <cite>output</cite> is expected to be a logits tensor.</p>
<blockquote class="last">
<div>By default, we consider that <cite>output</cite>
encodes a probability distribution.</div></blockquote>
</dd>
<dt># Returns</dt>
<dd>A tensor.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.cast_to_floatx">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">cast_to_floatx</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#cast_to_floatx"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.cast_to_floatx" title="Permalink to this definition">¶</a></dt>
<dd><p>Cast a Numpy array to the default Keras float type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>Numpy array.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">The same Numpy array, cast to its new type.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.categorical_crossentropy">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">categorical_crossentropy</code><span class="sig-paren">(</span><em>output</em>, <em>target</em>, <em>from_logits=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#categorical_crossentropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.categorical_crossentropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Categorical crossentropy between an output tensor
and a target tensor, where the target is a tensor of the same
shape as the output.</p>
<p># TODO(rbharath): Should probably swap this over to tf mode.</p>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.clip">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">clip</code><span class="sig-paren">(</span><em>x</em>, <em>min_value</em>, <em>max_value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#clip"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.clip" title="Permalink to this definition">¶</a></dt>
<dd><p>Element-wise value clipping.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">A tensor.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.concatenate">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">concatenate</code><span class="sig-paren">(</span><em>tensors</em>, <em>axis=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#concatenate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.concatenate" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates a list of tensors alongside the specified axis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">A tensor.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.cosine_distances">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">cosine_distances</code><span class="sig-paren">(</span><em>test</em>, <em>support</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#cosine_distances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.cosine_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes pairwise cosine distances between provided tensors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>test</strong> (<em>tf.Tensor</em>) &#8211; Of shape (n_test, n_feat)</li>
<li><strong>support</strong> (<em>tf.Tensor</em>) &#8211; Of shape (n_support, n_feat)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Of shape (n_test, n_support)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.dot">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">dot</code><span class="sig-paren">(</span><em>x</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#dot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiplies 2 tensors (and/or variables) and returns a <em>tensor</em>.
When attempting to multiply a ND tensor
with a ND tensor, it reproduces the Theano behavior.
(e.g. (2, 3).(4, 3, 5) = (2, 4, 5))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>Tensor or variable.</em>) &#8211; </li>
<li><strong>y</strong> (<em>Tensor or variable.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor, dot product of x and y.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.dropout">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>tensor</em>, <em>dropout_prob</em>, <em>training=True</em>, <em>training_only=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Random dropout.</p>
<p>This implementation supports &#8220;always-on&#8221; dropout (training_only=False), which
can be used to calculate model uncertainty. See Gal and Ghahramani,
<a class="reference external" href="http://arxiv.org/abs/1506.02142">http://arxiv.org/abs/1506.02142</a>.</p>
<dl class="docutils">
<dt>NOTE(user): To simplify the implementation, I have chosen not to reverse</dt>
<dd>the scaling that occurs in tf.nn.dropout when using dropout during
inference. This shouldn&#8217;t be an issue since the activations will be scaled
by the same constant in both training and inference. This means that there
are no training-time differences between networks that use dropout during
inference and those that do not.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<em>tf.Tensor</em>) &#8211; Input tensor.</li>
<li><strong>dropout_prob</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Float giving dropout probability for weights (NOT keep probability).</li>
<li><strong>training_only</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; Boolean. If True (standard dropout), apply dropout only
during training. If False, apply dropout during inference as well.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A tensor with the same shape as the input tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.elu">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>x</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#elu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Exponential linear unit.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable to compute the activation function for.</em>) &#8211; </li>
<li><strong>alpha</strong> (<em>A scalar, slope of positive section.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.epsilon">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">epsilon</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#epsilon"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.epsilon" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the fuzz
factor used in numeric expressions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">A float.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.euclidean_distance">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">euclidean_distance</code><span class="sig-paren">(</span><em>test</em>, <em>support</em>, <em>max_dist_sq=20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#euclidean_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.euclidean_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes pairwise euclidean distances between provided tensors</p>
<p>TODO(rbharath): BROKEN! THIS DOESN&#8217;T WORK!</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>test</strong> (<em>tf.Tensor</em>) &#8211; Of shape (n_test, n_feat)</li>
<li><strong>support</strong> (<em>tf.Tensor</em>) &#8211; Of shape (n_support, n_feat)</li>
<li><strong>max_dist_sq</strong> (<em>float, optional</em>) &#8211; Maximum pairwise distance allowed.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Of shape (n_test, n_support)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.fully_connected_layer">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">fully_connected_layer</code><span class="sig-paren">(</span><em>tensor</em>, <em>size=None</em>, <em>weight_init=None</em>, <em>bias_init=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#fully_connected_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.fully_connected_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Fully connected layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<em>tf.Tensor</em>) &#8211; Input tensor.</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of output nodes for this layer.</li>
<li><strong>weight_init</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Weight initializer.</li>
<li><strong>bias_init</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Bias initializer.</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; Name for this op. Defaults to &#8216;fully_connected&#8217;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A new tensor representing the output of the fully connected layer.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">tf.Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211;
If input tensor is not 2D.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.get_dtype">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">get_dtype</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#get_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.get_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the dtype of a Keras tensor or variable, as a string.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>Tensor or variable.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">String, dtype of <cite>x</cite>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.get_ndim">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">get_ndim</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#get_ndim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.get_ndim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of axes in a tensor, as an integer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>Tensor or variable.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">Integer (scalar), number of axes.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.hard_sigmoid">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">hard_sigmoid</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#hard_sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.hard_sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Segment-wise linear approximation of sigmoid.</p>
<p>Faster than sigmoid. Returns 0. if x &lt; -2.5, 1. if x &gt; 2.5.
In -2.5 &lt;= x &lt;= 2.5, returns 0.2 * x + 0.5.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">A tensor.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.int_shape">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">int_shape</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#int_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.int_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape of a Keras tensor or a Keras variable as a tuple of
integers or None entries.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>Tensor or variable.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">A tuple of integers (or None entries).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.l2_normalize">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">l2_normalize</code><span class="sig-paren">(</span><em>x</em>, <em>axis</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#l2_normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.l2_normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalizes a tensor wrt the L2 norm alongside the specified axis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>input tensor.</em>) &#8211; </li>
<li><strong>axis</strong> (<em>axis along which to perform normalization.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.logits">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">logits</code><span class="sig-paren">(</span><em>features</em>, <em>num_classes=2</em>, <em>weight_init=None</em>, <em>bias_init=None</em>, <em>dropout_prob=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a logits tensor for a single classification task.</p>
<p>You almost certainly don&#8217;t want dropout on there &#8211; it&#8217;s like randomly setting
the (unscaled) probability of a target class to 0.5.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>features</strong> &#8211; A 2D tensor with dimensions batch_size x num_features.</li>
<li><strong>num_classes</strong> &#8211; Number of classes for each task.</li>
<li><strong>weight_init</strong> &#8211; Weight initializer.</li>
<li><strong>bias_init</strong> &#8211; Bias initializer.</li>
<li><strong>dropout_prob</strong> &#8211; Float giving dropout probability for weights (NOT keep
probability).</li>
<li><strong>name</strong> &#8211; Name for this op.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A logits tensor with shape batch_size x num_classes.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.lrelu">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">lrelu</code><span class="sig-paren">(</span><em>alpha=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#lrelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.lrelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a leaky rectified linear unit function.</p>
<p>This function returns a new function that implements the LReLU with a
specified alpha.  The returned value can be used as an activation function in
network layers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; the slope of the function when x&lt;0</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">a function f(x) that returns alpha*x when x&lt;0, and x when x&gt;0.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.max">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">max</code><span class="sig-paren">(</span><em>x</em>, <em>axis=None</em>, <em>keepdims=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximum value in a tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </li>
<li><strong>axis</strong> (<em>An integer, the axis to find maximum values.</em>) &#8211; </li>
<li><strong>keepdims</strong> (<em>A boolean, whether to keep the dimensions or not.</em>) &#8211; If <cite>keepdims</cite> is <cite>False</cite>, the rank of the tensor is reduced
by 1. If <cite>keepdims</cite> is <cite>True</cite>,
the reduced dimension is retained with length 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor with maximum values of <cite>x</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.mean">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>x</em>, <em>axis=None</em>, <em>keepdims=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean of a tensor, alongside the specified axis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </li>
<li><strong>axis</strong> (<em>A list of integer. Axes to compute the mean.</em>) &#8211; </li>
<li><strong>keepdims</strong> (<em>A boolean, whether to keep the dimensions or not.</em>) &#8211; If keepdims is False, the rank of the tensor is reduced
by 1 for each entry in axis. If keep_dims is True,
the reduced dimensions are retained with length 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor with the mean of elements of x.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.moving_average_update">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">moving_average_update</code><span class="sig-paren">(</span><em>variable</em>, <em>value</em>, <em>momentum</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#moving_average_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.moving_average_update" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.multitask_logits">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">multitask_logits</code><span class="sig-paren">(</span><em>features</em>, <em>num_tasks</em>, <em>num_classes=2</em>, <em>weight_init=None</em>, <em>bias_init=None</em>, <em>dropout_prob=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#multitask_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.multitask_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a logit tensor for each classification task.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>features</strong> &#8211; A 2D tensor with dimensions batch_size x num_features.</li>
<li><strong>num_tasks</strong> &#8211; Number of classification tasks.</li>
<li><strong>num_classes</strong> &#8211; Number of classes for each task.</li>
<li><strong>weight_init</strong> &#8211; Weight initializer.</li>
<li><strong>bias_init</strong> &#8211; Bias initializer.</li>
<li><strong>dropout_prob</strong> &#8211; Float giving dropout probability for weights (NOT keep
probability).</li>
<li><strong>name</strong> &#8211; Name for this op. Defaults to &#8216;multitask_logits&#8217;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A list of logit tensors; one for each classification task.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.ones">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#ones"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates an all-ones tensor variable and returns it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>shape</strong> (<em>Tuple of integers, shape of returned Keras variable.</em>) &#8211; </li>
<li><strong>dtype</strong> (<em>Tensorflow dtype</em>) &#8211; </li>
<li><strong>name</strong> (<em>String, name of returned Keras variable.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Keras variable, filled with <cite>1.0</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.optimizer">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">optimizer</code><span class="sig-paren">(</span><em>optimizer='adam'</em>, <em>learning_rate=0.001</em>, <em>momentum=0.9</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Create model optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>optimizer</strong> (<em>str, optional</em>) &#8211; Name of optimizer</li>
<li><strong>learning_rate</strong> (<em>float, optional</em>) &#8211; Learning rate for algorithm</li>
<li><strong>momentum</strong> (<em>float, optional</em>) &#8211; Momentum rate</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A training Optimizer.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">NotImplementedError</span></code> &#8211;
If an unsupported optimizer is requested.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.random_normal_variable">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">random_normal_variable</code><span class="sig-paren">(</span><em>shape</em>, <em>mean</em>, <em>scale</em>, <em>dtype=tf.float32</em>, <em>name=None</em>, <em>seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#random_normal_variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.random_normal_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates an Keras variable filled with
samples drawn from a normal distribution and returns it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>shape</strong> (<em>Tuple of integers, shape of returned Keras variable.</em>) &#8211; </li>
<li><strong>mean</strong> (<em>Float, mean of the normal distribution.</em>) &#8211; </li>
<li><strong>scale</strong> (<em>Float, standard deviation of the normal distribution.</em>) &#8211; </li>
<li><strong>dtype</strong> (<em>Tensorflow dtype</em>) &#8211; </li>
<li><strong>name</strong> (<em>String, name of returned Keras variable.</em>) &#8211; </li>
<li><strong>seed</strong> (<em>Integer, random seed.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tf.Variable, filled with drawn samples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.random_uniform_variable">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">random_uniform_variable</code><span class="sig-paren">(</span><em>shape</em>, <em>low</em>, <em>high</em>, <em>dtype=tf.float32</em>, <em>name=None</em>, <em>seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#random_uniform_variable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.random_uniform_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates an variable filled with
samples drawn from a uniform distribution and returns it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>shape</strong> (<em>Tuple of integers, shape of returned variable.</em>) &#8211; </li>
<li><strong>low</strong> (<em>Float, lower boundary of the output inteval.</em>) &#8211; </li>
<li><strong>high</strong> (<em>Float, upper boundary of the output interval.</em>) &#8211; </li>
<li><strong>dtype</strong> (<em>Tensorflow dtype</em>) &#8211; </li>
<li><strong>name</strong> (<em>String, name of returned variable.</em>) &#8211; </li>
<li><strong>seed</strong> (<em>Integer, random seed.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tf.Variable, filled with drawn samples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.relu">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>x</em>, <em>alpha=0.0</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Rectified linear unit.
With default values, it returns element-wise <cite>max(x, 0)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </li>
<li><strong>alpha</strong> (<em>A scalar, slope of negative section (default=`0.`).</em>) &#8211; </li>
<li><strong>max_value</strong> (<em>Saturation threshold.</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.selu">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Scaled Exponential Linear unit.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">A tensor.</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>[Self-Normalizing Neural Networks](<a class="reference external" href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</a>)</li>
</ul>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.softmax_N">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">softmax_N</code><span class="sig-paren">(</span><em>tensor</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#softmax_N"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.softmax_N" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply softmax across last dimension of a tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> &#8211; Input tensor.</li>
<li><strong>name</strong> &#8211; Name for this op. If None, defaults to &#8216;softmax_N&#8217;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor with softmax-normalized values on the last dimension.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.sparse_categorical_crossentropy">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">sparse_categorical_crossentropy</code><span class="sig-paren">(</span><em>output</em>, <em>target</em>, <em>from_logits=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#sparse_categorical_crossentropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.sparse_categorical_crossentropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Categorical crossentropy between an output tensor
and a target tensor, where the target is an integer tensor.</p>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.sqrt">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">sqrt</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#sqrt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Element-wise square root.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>input tensor.</em>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">A tensor.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.sum">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>x</em>, <em>axis=None</em>, <em>keepdims=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Sum of the values in a tensor, alongside the specified axis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </li>
<li><strong>axis</strong> (<em>An integer, the axis to sum over.</em>) &#8211; </li>
<li><strong>keepdims</strong> (<em>A boolean, whether to keep the dimensions or not.</em>) &#8211; If keepdims is False, the rank of the tensor is reduced
by 1. If keepdims is True,
the reduced dimension is retained with length 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor with sum of x.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.var">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">var</code><span class="sig-paren">(</span><em>x</em>, <em>axis=None</em>, <em>keepdims=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#var"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.var" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance of a tensor, alongside the specified axis.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>A tensor or variable.</em>) &#8211; </li>
<li><strong>axis</strong> (<em>An integer, the axis to compute the variance.</em>) &#8211; </li>
<li><strong>keepdims</strong> (<em>A boolean, whether to keep the dimensions or not.</em>) &#8211; If keepdims is False, the rank of the tensor is reduced
by 1. If keepdims is True,
the reduced dimension is retained with length 1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tensor with the variance of elements of <cite>x</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.weight_decay">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">weight_decay</code><span class="sig-paren">(</span><em>penalty_type</em>, <em>penalty</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#weight_decay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.weight_decay" title="Permalink to this definition">¶</a></dt>
<dd><p>Add weight decay.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>model</strong> &#8211; TensorflowGraph.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A scalar tensor containing the weight decay cost.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><code class="xref py py-exc docutils literal"><span class="pre">NotImplementedError</span></code> &#8211;
If an unsupported penalty type is requested.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.model_ops.zeros">
<code class="descclassname">deepchem.models.tensorgraph.model_ops.</code><code class="descname">zeros</code><span class="sig-paren">(</span><em>shape</em>, <em>dtype=tf.float32</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/model_ops.html#zeros"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.model_ops.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates an all-zeros variable and returns it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>shape</strong> (<em>Tuple of integers, shape of returned Keras variable</em>) &#8211; </li>
<li><strong>dtype</strong> (<em>Tensorflow dtype</em>) &#8211; </li>
<li><strong>name</strong> (<em>String, name of returned Keras variable</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A variable (including Keras metadata), filled with <cite>0.0</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.optimizers">
<span id="deepchem-models-tensorgraph-optimizers-module"></span><h2>deepchem.models.tensorgraph.optimizers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.optimizers" title="Permalink to this headline">¶</a></h2>
<p>Optimizers and related classes for use with TensorGraph.</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.Adam">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">Adam</code><span class="sig-paren">(</span><em>learning_rate=0.001</em>, <em>beta1=0.9</em>, <em>beta2=0.999</em>, <em>epsilon=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#Adam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.Adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.Optimizer</span></code></a></p>
<p>The Adam optimization algorithm.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.ExponentialDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">ExponentialDecay</code><span class="sig-paren">(</span><em>initial_rate</em>, <em>decay_rate</em>, <em>decay_steps</em>, <em>staircase=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#ExponentialDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.ExponentialDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="deepchem.models.tensorgraph.optimizers.LearningRateSchedule"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.LearningRateSchedule</span></code></a></p>
<p>A learning rate that decreases exponentially with the number of training steps.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.GradientDescent">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">GradientDescent</code><span class="sig-paren">(</span><em>learning_rate=0.001</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#GradientDescent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.GradientDescent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.Optimizer</span></code></a></p>
<p>The gradient descent optimization algorithm.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.LearningRateSchedule">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">LearningRateSchedule</code><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#LearningRateSchedule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>A schedule for changing the learning rate over the course of optimization.</p>
<p>This is an abstract class.  Subclasses represent specific schedules.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.Optimizer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">Optimizer</code><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#Optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>An algorithm for optimizing a TensorGraph based model.</p>
<p>This is an abstract class.  Subclasses represent specific optimization algorithms.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.optimizers.PolynomialDecay">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.optimizers.</code><code class="descname">PolynomialDecay</code><span class="sig-paren">(</span><em>initial_rate</em>, <em>final_rate</em>, <em>decay_steps</em>, <em>power=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/optimizers.html#PolynomialDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.optimizers.PolynomialDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.LearningRateSchedule" title="deepchem.models.tensorgraph.optimizers.LearningRateSchedule"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.optimizers.LearningRateSchedule</span></code></a></p>
<p>A learning rate that decreases from an initial value to a final value over a fixed number of training steps.</p>
</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.progressive_multitask">
<span id="deepchem-models-tensorgraph-progressive-multitask-module"></span><h2>deepchem.models.tensorgraph.progressive_multitask module<a class="headerlink" href="#module-deepchem.models.tensorgraph.progressive_multitask" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.progressive_multitask.</code><code class="descname">ProgressiveMultitaskRegressor</code><span class="sig-paren">(</span><em>n_tasks, n_features, alpha_init_stddevs=0.02, layer_sizes=[1000], weight_init_stddevs=0.02, bias_init_consts=1.0, weight_decay_penalty=0.0, weight_decay_penalty_type='l2', dropouts=0.5, activation_fns=&lt;function relu&gt;, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<p>Implements a progressive multitask neural network.</p>
<p>Progressive Networks: <a class="reference external" href="https://arxiv.org/pdf/1606.04671v3.pdf">https://arxiv.org/pdf/1606.04671v3.pdf</a></p>
<p>Progressive networks allow for multitask learning where each task
gets a new column of weights. As a result, there is no exponential
forgetting where previous tasks are ignored.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.add_adapter">
<code class="descname">add_adapter</code><span class="sig-paren">(</span><em>all_layers</em>, <em>task</em>, <em>layer_num</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.add_adapter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.add_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Add an adapter connection for given task/layer combo</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_task">
<code class="descname">fit_task</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.fit_task"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.fit_task" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit one task.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/progressive_multitask.html#ProgressiveMultitaskRegressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.progressive_multitask.ProgressiveMultitaskRegressor.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.regularizers">
<span id="deepchem-models-tensorgraph-regularizers-module"></span><h2>deepchem.models.tensorgraph.regularizers module<a class="headerlink" href="#module-deepchem.models.tensorgraph.regularizers" title="Permalink to this headline">¶</a></h2>
<p>Ops for regularizers</p>
<p>Code borrowed from Keras.</p>
<dl class="attribute">
<dt id="deepchem.models.tensorgraph.regularizers.ActivityRegularizer">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">ActivityRegularizer</code><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.ActivityRegularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#deepchem.models.tensorgraph.regularizers.L1L2Regularizer" title="deepchem.models.tensorgraph.regularizers.L1L2Regularizer"><code class="xref py py-class docutils literal"><span class="pre">L1L2Regularizer</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.regularizers.L1L2Regularizer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">L1L2Regularizer</code><span class="sig-paren">(</span><em>l1=0.0</em>, <em>l2=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#L1L2Regularizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.L1L2Regularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.regularizers.Regularizer" title="deepchem.models.tensorgraph.regularizers.Regularizer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.regularizers.Regularizer</span></code></a></p>
<p>Regularizer for L1 and L2 regularization.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>l1: Float; L1 regularization factor.
l2: Float; L2 regularization factor.</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.regularizers.Regularizer">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">Regularizer</code><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#Regularizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.Regularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.regularizers.WeightRegularizer">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">WeightRegularizer</code><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.WeightRegularizer" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#deepchem.models.tensorgraph.regularizers.L1L2Regularizer" title="deepchem.models.tensorgraph.regularizers.L1L2Regularizer"><code class="xref py py-class docutils literal"><span class="pre">L1L2Regularizer</span></code></a></p>
</dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.activity_l1">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">activity_l1</code><span class="sig-paren">(</span><em>l=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#activity_l1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.activity_l1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.activity_l1l2">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">activity_l1l2</code><span class="sig-paren">(</span><em>l1=0.01</em>, <em>l2=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#activity_l1l2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.activity_l1l2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.activity_l2">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">activity_l2</code><span class="sig-paren">(</span><em>l=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#activity_l2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.activity_l2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.l1">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">l1</code><span class="sig-paren">(</span><em>l=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#l1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.l1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.l1l2">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">l1l2</code><span class="sig-paren">(</span><em>l1=0.01</em>, <em>l2=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#l1l2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.l1l2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepchem.models.tensorgraph.regularizers.l2">
<code class="descclassname">deepchem.models.tensorgraph.regularizers.</code><code class="descname">l2</code><span class="sig-paren">(</span><em>l=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/regularizers.html#l2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.regularizers.l2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.robust_multitask">
<span id="deepchem-models-tensorgraph-robust-multitask-module"></span><h2>deepchem.models.tensorgraph.robust_multitask module<a class="headerlink" href="#module-deepchem.models.tensorgraph.robust_multitask" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.robust_multitask.</code><code class="descname">RobustMultitaskClassifier</code><span class="sig-paren">(</span><em>n_tasks, n_features, layer_sizes=[1000], weight_init_stddevs=0.02, bias_init_consts=1.0, weight_decay_penalty=0.0, weight_decay_penalty_type='l2', dropouts=0.5, activation_fns=&lt;function relu&gt;, n_classes=2, bypass_layer_sizes=[100], bypass_weight_init_stddevs=[0.02], bypass_bias_init_consts=[1.0], bypass_dropouts=[0.5], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<p>Implements a neural network for robust multitasking.</p>
<p>Key idea is to have bypass layers that feed directly from features to task
output. Hopefully will allow tasks to route around bad multitasking.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskClassifier.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskClassifier.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.robust_multitask.</code><code class="descname">RobustMultitaskRegressor</code><span class="sig-paren">(</span><em>n_tasks, n_features, layer_sizes=[1000], weight_init_stddevs=0.02, bias_init_consts=1.0, weight_decay_penalty=0.0, weight_decay_penalty_type='l2', dropouts=0.5, activation_fns=&lt;function relu&gt;, bypass_layer_sizes=[100], bypass_weight_init_stddevs=[0.02], bypass_bias_init_consts=[1.0], bypass_dropouts=[0.5], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<p>Implements a neural network for robust multitasking.</p>
<p>Key idea is to have bypass layers that feed directly from features to task
output. Hopefully will allow tasks to route around bad multitasking.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/robust_multitask.html#RobustMultitaskRegressor.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.robust_multitask.RobustMultitaskRegressor.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.sequential">
<span id="deepchem-models-tensorgraph-sequential-module"></span><h2>deepchem.models.tensorgraph.sequential module<a class="headerlink" href="#module-deepchem.models.tensorgraph.sequential" title="Permalink to this headline">¶</a></h2>
<p>Convenience class for building sequential deep networks.</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.sequential.Sequential">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.sequential.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/sequential.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.tensor_graph.TensorGraph</span></code></a></p>
<p>Sequential models are linear stacks of layers.</p>
<p>Analogous to the Sequential model from Keras and allows for less
verbose construction of simple deep learning model.</p>
<p class="rubric">Example</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">deepchem</span> <span class="kn">as</span> <span class="nn">dc</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define Data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">SoftMax</span><span class="p">())</span>
</pre></div>
</div>
<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/sequential.html#Sequential.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a new layer to model.</p>
<dl class="docutils">
<dt>layer: Layer</dt>
<dd>Adds layer to this graph.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>loss</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/sequential.html#Sequential.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits on the specified dataset.</p>
<p>If called for the first time, constructs the TensorFlow graph for this
model. Fits this graph on the specified dataset according to the specified
loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset with data</li>
<li><strong>loss</strong> (<a class="reference external" href="https://docs.python.org/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; Only &#8220;binary_crossentropy&#8221; or &#8220;mse&#8221; for now.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.load_from_dir">
<code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/sequential.html#Sequential.restore"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Not currently supported.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.sequential.Sequential.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.sequential.Sequential.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.symmetry_functions">
<span id="deepchem-models-tensorgraph-symmetry-functions-module"></span><h2>deepchem.models.tensorgraph.symmetry_functions module<a class="headerlink" href="#module-deepchem.models.tensorgraph.symmetry_functions" title="Permalink to this headline">¶</a></h2>
<p>Created on Thu Jul  6 20:43:23 2017</p>
<p>&#64;author: zqwu</p>
<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AngularSymmetry</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>lambd_init=None</em>, <em>ita_init=None</em>, <em>zeta_init=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Angular Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetry.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Angular Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetry.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AngularSymmetryMod</code><span class="sig-paren">(</span><em>max_atoms, lambd_init=None, ita_init=None, zeta_init=None, Rs_init=None, thetas_init=None, atomic_number_differentiated=False, atom_numbers=[1, 6, 7, 8], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Angular Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AngularSymmetryMod.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Angular Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AngularSymmetryMod.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">AtomicDifferentiatedDense</code><span class="sig-paren">(</span><em>max_atoms, out_channels, atom_number_cases=[1, 6, 7, 8], init='glorot_uniform', activation='relu', **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Separate Dense module for different atoms</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Radial Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.none_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#AtomicDifferentiatedDense.set_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.AtomicDifferentiatedDense.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">BPFeatureMerge</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPFeatureMerge"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPFeatureMerge.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge features together</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPFeatureMerge.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">BPGather</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPGather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#BPGather.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge features together</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.BPGather.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.BPGather.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">DistanceCutoff</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>cutoff=11.338356747390371</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceCutoff.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix for BPSymmetryFunction with trainable cutoff</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceCutoff.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">DistanceMatrix</code><span class="sig-paren">(</span><em>max_atoms</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceMatrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#DistanceMatrix.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate distance matrix for BPSymmetryFunction with trainable cutoff</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.DistanceMatrix.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.symmetry_functions.</code><code class="descname">RadialSymmetry</code><span class="sig-paren">(</span><em>max_atoms, Rs_init=None, ita_init=None, atomic_number_differentiated=False, atom_numbers=[1, 6, 7, 8], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.tensorgraph.layers.Layer</span></code></a></p>
<p>Radial Symmetry Function</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.add_summary_to_tg">
<code class="descname">add_summary_to_tg</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.add_summary_to_tg" title="Permalink to this definition">¶</a></dt>
<dd><p>Can only be called after self.create_layer to gaurentee that name is not none</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameters for the Gaussian</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer with different inputs.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><em>replacements={}</em>, <em>variables_graph=None</em>, <em>shared=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Duplicate this Layer and all its inputs.</p>
<p>This is similar to clone(), but instead of only cloning one layer, it also
recursively calls copy() on all of this layer&#8217;s inputs to clone the entire
hierarchy of layers.  In the process, you can optionally tell it to replace
particular layers with specific existing ones.  For example, you can clone a
stack of layers, while connecting the topmost ones to different inputs.</p>
<p>For example, consider a stack of dense layers that depend on an input:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
<p>The following will clone all three dense layers, but not the input layer.
Instead, the input to the first dense layer will be a different layer
specified in the replacements map.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_input</span> <span class="o">=</span> <span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">replacements</span> <span class="o">=</span> <span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">new_input</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense3_copy</span> <span class="o">=</span> <span class="n">dense3</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>replacements</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#map" title="(in Python v2.7)"><em>map</em></a>) &#8211; specifies existing layers, and the layers to replace them with (instead of
cloning them).  This argument serves two purposes.  First, you can pass in
a list of replacements to control which layers get cloned.  In addition,
as each layer is cloned, it is added to this map.  On exit, it therefore
contains a complete record of all layers that were copied, and a reference
to the copy of each one.</li>
<li><strong>variables_graph</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="deepchem.models.tensorgraph.tensor_graph.TensorGraph"><em>TensorGraph</em></a>) &#8211; an optional TensorGraph from which to take variables.  If this is specified,
the current value of each variable in each layer is recorded, and the copy
has that value specified as its initial value.  This allows a piece of a
pre-trained model to be copied to another model.</li>
<li><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, create new layers by calling shared() on the input layers.
This means the newly created layers will share variables with the original
ones.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.create_tensor">
<code class="descname">create_tensor</code><span class="sig-paren">(</span><em>in_layers=None</em>, <em>set_tensors=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/symmetry_functions.html#RadialSymmetry.create_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.create_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Radial Symmetry Function</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.layer_number_dict">
<code class="descname">layer_number_dict</code><em class="property"> = {}</em><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.layer_number_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.none_tensors">
<code class="descname">none_tensors</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.none_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_summary">
<code class="descname">set_summary</code><span class="sig-paren">(</span><em>summary_op</em>, <em>summary_description=None</em>, <em>collections=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates a tensor with a tf.summary operation
Collects data from self.out_tensor by default but can be changed by setting
self.tb_input to another tensor in create_tensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>summary_op</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; summary operation to annotate node</li>
<li><strong>summary_description</strong> (<em>object, optional</em>) &#8211; Optional summary_pb2.SummaryDescription()</li>
<li><strong>collections</strong> (<em>list of graph collections keys, optional</em>) &#8211; New summary op is added to these collections. Defaults to [GraphKeys.SUMMARIES]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_tensors">
<code class="descname">set_tensors</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_tensors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_variable_initial_values">
<code class="descname">set_variable_initial_values</code><span class="sig-paren">(</span><em>values</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.set_variable_initial_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the initial values of all variables.</p>
<p>This takes a list, which contains the initial values to use for all of
this layer&#8217;s values (in the same order retured by
TensorGraph.get_layer_variables()).  When this layer is used in a
TensorGraph, it will automatically initialize each variable to the value
specified in the list.  Note that some layers also have separate mechanisms
for specifying variable initializers; this method overrides them. The
purpose of this method is to let a Layer object represent a pre-trained
layer, complete with trained values for its variables.</p>
</dd></dl>

<dl class="attribute">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shape">
<code class="descname">shape</code><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the shape of this Layer&#8217;s output.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shared">
<code class="descname">shared</code><span class="sig-paren">(</span><em>in_layers</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.symmetry_functions.RadialSymmetry.shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of this layer that shares variables with it.</p>
<p>This is similar to clone(), but where clone() creates two independent layers,
this causes the layers to share variables with each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_layers</strong> (<em>list tensor</em>) &#8211; </li>
<li><strong>in tensors for the shared layer</strong> (<em>List</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer">Layer</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph.tensor_graph">
<span id="deepchem-models-tensorgraph-tensor-graph-module"></span><h2>deepchem.models.tensorgraph.tensor_graph module<a class="headerlink" href="#module-deepchem.models.tensorgraph.tensor_graph" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.Submodel">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">Submodel</code><span class="sig-paren">(</span><em>graph</em>, <em>layers</em>, <em>loss</em>, <em>optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#Submodel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>An alternate objective for training one piece of a TensorGraph.</p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.Submodel.get_train_op">
<code class="descname">get_train_op</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#Submodel.get_train_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.Submodel.get_train_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Tensorflow operator to use for training.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.TFWrapper">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">TFWrapper</code><span class="sig-paren">(</span><em>tf_class</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TFWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TFWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>This class exists as a workaround for Tensorflow objects not being picklable.</p>
<p>The job of a TFWrapper is to create Tensorflow objects by passing defined arguments
to a constructor.  There are cases where we really want to store Tensorflow objects
of various sorts (optimizers, initializers, etc.), but we can&#8217;t because they cannot
be pickled.  So instead we store a TFWrapper that creates the object when needed.</p>
</dd></dl>

<dl class="class">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph">
<em class="property">class </em><code class="descclassname">deepchem.models.tensorgraph.tensor_graph.</code><code class="descname">TensorGraph</code><span class="sig-paren">(</span><em>tensorboard=False</em>, <em>tensorboard_log_frequency=100</em>, <em>batch_size=100</em>, <em>random_seed=None</em>, <em>use_queue=True</em>, <em>graph=None</em>, <em>learning_rate=0.001</em>, <em>configproto=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="deepchem.models.html#deepchem.models.models.Model" title="deepchem.models.models.Model"><code class="xref py py-class docutils literal"><span class="pre">deepchem.models.models.Model</span></code></a></p>
<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.add_output">
<code class="descname">add_output</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.add_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.add_output" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.build">
<code class="descname">build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.build"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.build" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.create_submodel">
<code class="descname">create_submodel</code><span class="sig-paren">(</span><em>layers=None</em>, <em>loss=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.create_submodel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.create_submodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an alternate objective for training one piece of a TensorGraph.</p>
<p>A TensorGraph consists of a set of layers, and specifies a loss function and
optimizer to use for training those layers.  Usually this is sufficient, but
there are cases where you want to train different parts of a model separately.
For example, a GAN consists of a generator and a discriminator.  They are
trained separately, and they use different loss functions.</p>
<p>A submodel defines an alternate objective to use in cases like this.  It may
optionally specify any of the following: a subset of layers in the model to
train; a different loss function; and a different optimizer to use.  This
method creates a submodel, which you can then pass to fit() to use it for
training.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layers</strong> (<em>list</em>) &#8211; the list of layers to train.  If None, all layers in the model will be
trained.</li>
<li><strong>loss</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.layers.Layer" title="deepchem.models.tensorgraph.layers.Layer"><em>Layer</em></a>) &#8211; the loss function to optimize.  If None, the model&#8217;s main loss function
will be used.</li>
<li><strong>optimizer</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.optimizers.Optimizer" title="deepchem.models.tensorgraph.optimizers.Optimizer"><em>Optimizer</em></a>) &#8211; the optimizer to use for training.  If None, the model&#8217;s main optimizer
will be used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>the newly created submodel, which can be passed to any of the fitting</em></li>
<li><em>methods.</em></li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.default_generator">
<code class="descname">default_generator</code><span class="sig-paren">(</span><em>dataset</em>, <em>epochs=1</em>, <em>predict=False</em>, <em>deterministic=True</em>, <em>pad_batches=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.default_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>dataset</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the performance of this model on specified dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset object.</li>
<li><strong>metric</strong> (<a class="reference internal" href="deepchem.metrics.html#deepchem.metrics.Metric" title="deepchem.metrics.Metric"><em>deepchem.metrics.Metric</em></a>) &#8211; Evaluation metric</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of deepchem.transformers.Transformer</li>
<li><strong>per_task_metrics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, return per-task scores.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Maps tasks to scores under metric.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate_generator">
<code class="descname">evaluate_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>metrics</em>, <em>transformers=[]</em>, <em>labels=None</em>, <em>outputs=None</em>, <em>weights=[]</em>, <em>per_task_metrics=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.evaluate_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.evaluate_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>dataset</em>, <em>nb_epoch=10</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>deterministic=False</em>, <em>restore=False</em>, <em>submodel=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="deepchem.data.html#deepchem.data.datasets.Dataset" title="deepchem.data.datasets.Dataset"><em>Dataset</em></a>) &#8211; the Dataset to train on</li>
<li><strong>nb_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the number of epochs to train for</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, the samples are processed in order.  If False, a different random
order is used for each epoch.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_generator">
<code class="descname">fit_generator</code><span class="sig-paren">(</span><em>feed_dict_generator</em>, <em>max_checkpoints_to_keep=5</em>, <em>checkpoint_interval=1000</em>, <em>restore=False</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Train this model on data from a generator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>feed_dict_generator</strong> (<em>generator</em>) &#8211; this should generate batches, each represented as a dict that maps
Layers to values.</li>
<li><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</li>
<li><strong>checkpoint_interval</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the frequency at which to write checkpoints, measured in training steps.
Set this to 0 to disable automatic checkpointing.</li>
<li><strong>restore</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if True, restore the model from the most recent checkpoint and continue training
from there.  If False, retrain the model from scratch.</li>
<li><strong>submodel</strong> (<a class="reference internal" href="#deepchem.models.tensorgraph.tensor_graph.Submodel" title="deepchem.models.tensorgraph.tensor_graph.Submodel"><em>Submodel</em></a>) &#8211; an alternate training objective to use.  This should have been created by
calling create_submodel().</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">the average loss over the most recent checkpoint interval</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_on_batch">
<code class="descname">fit_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>w</em>, <em>submodel=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.fit_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.fit_on_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_checkpoints">
<code class="descname">get_checkpoints</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_checkpoints"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_checkpoints" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all available checkpoint files.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_global_step">
<code class="descname">get_global_step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_global_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_global_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_layer_variables">
<code class="descname">get_layer_variables</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_layer_variables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_layer_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of trainable variables in a layer of the graph.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_model_filename">
<code class="descname">get_model_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_model_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_num_tasks">
<code class="descname">get_num_tasks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_num_tasks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_num_tasks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><em>deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>deep</strong> (<em>boolean, optional</em>) &#8211; If True, will return the parameters for this estimator and
contained subobjects that are estimators.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>params</strong> &#8211;
Parameter names mapped to their values.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">mapping of string to any</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params_filename">
<code class="descname">get_params_filename</code><span class="sig-paren">(</span><em>model_dir</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_params_filename" title="Permalink to this definition">¶</a></dt>
<dd><p>Given model directory, obtain filename for the model itself.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pickling_errors">
<code class="descname">get_pickling_errors</code><span class="sig-paren">(</span><em>obj</em>, <em>seen=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_pickling_errors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pickling_errors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pre_q_input">
<code class="descname">get_pre_q_input</code><span class="sig-paren">(</span><em>input_layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.get_pre_q_input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_pre_q_input" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_task_type">
<code class="descname">get_task_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.get_task_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Currently models can only be classifiers or regressors.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.load_from_dir">
<em class="property">static </em><code class="descname">load_from_dir</code><span class="sig-paren">(</span><em>model_dir</em>, <em>restore=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.load_from_dir"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.load_from_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses self to make predictions on provided Dataset object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_batch">
<code class="descname">predict_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_generator">
<code class="descname">predict_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_on_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>generator</strong> (<em>Generator</em>) &#8211; Generator that constructs feed dictionaries for TensorGraph.</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs.
If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
<li><strong>Returns</strong> &#8211; y_pred: numpy ndarray of shape (n_samples, n_classes*n_tasks)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>dataset</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dataset</strong> (<em>dc.data.Dataset</em>) &#8211; Dataset to make prediction on</li>
<li><strong>transformers</strong> (<em>list</em>) &#8211; List of dc.trans.Transformers.</li>
<li><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><em>object</em></a>) &#8211; If outputs is None, then will assume outputs = self.outputs[0] (single
output). If outputs is a Layer/Tensor, then will evaluate and return as a
single ndarray. If outputs is a list of Layers/Tensors, will return a list
of ndarrays.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>y_pred</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy ndarray or list of numpy ndarrays</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_batch">
<code class="descname">predict_proba_on_batch</code><span class="sig-paren">(</span><em>X</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba_on_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates predictions for input samples, processing samples in a batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>ndarray</em>) &#8211; the input data, as a Numpy array.</li>
<li><strong>transformers</strong> (<em>List</em>) &#8211; List of dc.trans.Transformers</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A Numpy array of predictions.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_generator">
<code class="descname">predict_proba_on_generator</code><span class="sig-paren">(</span><em>generator</em>, <em>transformers=[]</em>, <em>outputs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.predict_proba_on_generator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.predict_proba_on_generator" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">numpy ndarray of shape (n_samples, n_classes*n_tasks)</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">y_pred</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.reload">
<code class="descname">reload</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.reload" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload trained model from disk.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>checkpoint=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.restore"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Reload the values of all variables from a checkpoint file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; the path to the checkpoint file to load.  If this is None, the most recent
checkpoint will be chosen automatically.  Call get_checkpoints() to get a
list of all available checkpoints.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.save">
<code class="descname">save</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.save_checkpoint">
<code class="descname">save_checkpoint</code><span class="sig-paren">(</span><em>max_checkpoints_to_keep=5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.save_checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a checkpoint to disk.</p>
<p>Usually you do not need to call this method, since fit() saves checkpoints
automatically.  If you have disabled automatic checkpointing during fitting,
this can be called to manually write checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>max_checkpoints_to_keep</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the maximum number of checkpoints to keep.  Older checkpoints are discarded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_loss">
<code class="descname">set_loss</code><span class="sig-paren">(</span><em>layer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.set_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_optimizer">
<code class="descname">set_optimizer</code><span class="sig-paren">(</span><em>optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.set_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the optimizer to use for fitting.</p>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>**params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it&#8217;s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">self</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="deepchem.models.tensorgraph.tensor_graph.TensorGraph.topsort">
<code class="descname">topsort</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/deepchem/models/tensorgraph/tensor_graph.html#TensorGraph.topsort"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#deepchem.models.tensorgraph.tensor_graph.TensorGraph.topsort" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepchem.models.tensorgraph">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-deepchem.models.tensorgraph" title="Permalink to this headline">¶</a></h2>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="_sources/deepchem.models.tensorgraph.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>