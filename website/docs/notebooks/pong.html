<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>deepchem package &mdash; deepchem master documentation</title>

    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     'master',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem master documentation" href="../index.html" />
    <link rel="next" title="deepchem.data package" href="../deepchem.data.html" />
    <link rel="prev" title="DeepChem" href="../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>master</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">

                <li><a href="/docs/notebooks/index.html">Tutorials</a></li>


              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/deepchem.html">master</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/2.0.0/deepchem.html">2.0.0</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/1.3.1/deepchem.html">1.3.1</a></li>
</ul>
</ul>
</li>
</ul>
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>

        </div>
    </div>
  </div><div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="pong-in-deepchem-with-a3c">
<h1>Pong in DeepChem with A3C<a class="headerlink" href="#pong-in-deepchem-with-a3c" title="Permalink to this headline">¶</a></h1>
<p>This notebook demonstrates using reinforcement learning to train an
agent to play Pong.</p>
<p>The first step is to create an <code class="docutils literal"><span class="pre">Environment</span></code> that implements this
task. Fortunately, OpenAI Gym already provides an implementation of Pong
(and many other tasks appropriate for reinforcement learning).
DeepChem’s <code class="docutils literal"><span class="pre">GymEnvironment</span></code> class provides an easy way to use
environments from OpenAI Gym. We could just use it directly, but in this
case we subclass it and preprocess the screen image a little bit to make
learning easier.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">deepchem</span> <span class="kn">as</span> <span class="nn">dc</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">PongEnv</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">rl</span><span class="o">.</span><span class="n">GymEnvironment</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PongEnv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s1">&#39;Pong-v0&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Crop everything outside the play area, reduce the image size,</span>
    <span class="c1"># and convert it to black and white.</span>
    <span class="n">cropped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">)[</span><span class="mi">34</span><span class="p">:</span><span class="mi">194</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">reduced</span> <span class="o">=</span> <span class="n">cropped</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">grayscale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">bw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grayscale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">bw</span><span class="p">[</span><span class="n">grayscale</span> <span class="o">!=</span> <span class="mi">233</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">bw</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">PongEnv</span><span class="p">()</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">PongEnv</span><span class="p">()</span>
</pre></div>
</div>
<p>Next we create a network to implement the policy. We begin with two
convolutional layers to process the image. That is followed by a dense
(fully connected) layer to provide plenty of capacity for game logic. We
also add a small Gated Recurrent Unit. That gives the network a little
bit of memory, so it can keep track of which way the ball is moving.</p>
<p>We concatenate the dense and GRU outputs together, and use them as
inputs to two final layers that serve as the network’s outputs. One
computes the action probabilities, and the other computes an estimate of
the state value function.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">deepchem.models.tensorgraph.layers</span> <span class="kn">as</span> <span class="nn">layers</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">PongPolicy</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">rl</span><span class="o">.</span><span class="n">Policy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">create_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">conv1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">num_outputs</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">conv2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">num_outputs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">conv1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">in_layers</span><span class="o">=</span><span class="n">conv2</span><span class="p">),</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">gru</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">dense</span><span class="p">))</span>
        <span class="n">concat</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">in_layers</span><span class="o">=</span><span class="p">[</span><span class="n">dense</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">gru</span><span class="p">)])</span>
        <span class="n">action_prob</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">concat</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">concat</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;action_prob&#39;</span><span class="p">:</span><span class="n">action_prob</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="n">value</span><span class="p">}</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">PongPolicy</span><span class="p">()</span>
</pre></div>
</div>
<p>We will optimize the policy using the Asynchronous Advantage Actor
Critic (A3C) algorithm. There are lots of hyperparameters we could
specify at this point, but the default values for most of them work well
on this problem. The only one we need to customize is the learning rate.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="n">a3c</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">rl</span><span class="o">.</span><span class="n">A3C</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">))</span>
</pre></div>
</div>
<p>Optimize for as long as you have patience to. By 1 million steps you
should see clear signs of learning. Around 3 million steps it should
start to occasionally beat the game’s built in AI. By 7 million steps it
should be winning almost every time. Running on my laptop, training
takes about 20 minutes for every million steps.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">a3c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="c1"># Change this for how long you have the patience</span>
<span class="n">million</span> <span class="o">=</span> <span class="mf">10e6</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">round</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rounds</span><span class="p">):</span>
    <span class="n">a3c</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">million</span><span class="p">,</span> <span class="n">restore</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s watch it play and see how it does!</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="k">def</span> <span class="nf">render_env</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="n">a3c</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">while</span> <span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="o">.</span><span class="n">total_seconds</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">120</span><span class="p">:</span>
    <span class="n">render_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a3c</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">state</span><span class="p">))</span>
</pre></div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/notebooks/pong.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>