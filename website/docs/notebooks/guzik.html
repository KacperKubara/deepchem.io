<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>deepchem package &mdash; deepchem master documentation</title>

    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     'master',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem master documentation" href="../index.html" />
    <link rel="next" title="deepchem.data package" href="../deepchem.data.html" />
    <link rel="prev" title="DeepChem" href="../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html"><span><img src="../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>master</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">

                <li><a href="/docs/notebooks/index.html">Tutorials</a></li>


              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/deepchem.html">master</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/2.0.0/deepchem.html">2.0.0</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="/docs/1.3.1/deepchem.html">1.3.1</a></li>
</ul>
</ul>
</li>
</ul>
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>

        </div>
    </div>
  </div><div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="seqtoseq-fingerprint">
<h1>SeqToSeq Fingerprint<a class="headerlink" href="#seqtoseq-fingerprint" title="Permalink to this headline">¶</a></h1>
<p>In this example, we will use a <code class="docutils literal"><span class="pre">SeqToSeq</span></code> model to generate
fingerprints for classifying molecules. This is based on the following
paper, although some of the implementation details are different: Xu et
al., “Seq2seq Fingerprint: An Unsupervised Deep Molecular Embedding for
Drug Discovery” (<a class="reference external" href="https://doi.org/10.1145/3107411.3107424">https://doi.org/10.1145/3107411.3107424</a>).</p>
<p>Many types of models require their inputs to have a fixed shape. Since
molecules can vary widely in the numbers of atoms and bonds they
contain, this makes it hard to apply those models to them. We need a way
of generating a fixed length “fingerprint” for each molecule. Various
ways of doing this have been designed, such as Extended-Connectivity
Fingerprints (ECFPs). But in this example, instead of designing a
fingerprint by hand, we will let a <code class="docutils literal"><span class="pre">SeqToSeq</span></code> model learn its own
method of creating fingerprints.</p>
<p>A <code class="docutils literal"><span class="pre">SeqToSeq</span></code> model performs sequence to sequence translation. For
example, they are often used to translate text from one language to
another. It consists of two parts called the “encoder” and “decoder”.
The encoder is a stack of recurrent layers. The input sequence is fed
into it, one token at a time, and it generates a fixed length vector
called the “embedding vector”. The decoder is another stack of recurrent
layers that performs the inverse operation: it takes the embedding
vector as input, and generates the output sequence. By training it on
appropriately chosen input/output pairs, you can create a model that
performs many sorts of transformations.</p>
<p>In this case, we will use SMILES strings describing molecules as the
input sequences. We will train the model as an autoencoder, so it tries
to make the output sequences identical to the input sequences. For that
to work, the encoder must create embedding vectors that contain all
information from the original sequence. That’s exactly what we want in a
fingerprint, so perhaps those embedding vectors will then be useful as a
way to represent molecules in other models!</p>
<p>Let’s start by loading the data. We will use the MUV dataset. It
includes 74,501 molecules in the training set, and 9313 molecules in the
validation set, so it gives us plenty of SMILES strings to work with.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">deepchem</span> <span class="kn">as</span> <span class="nn">dc</span>
<span class="n">tasks</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transformers</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">molnet</span><span class="o">.</span><span class="n">load_muv</span><span class="p">()</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span>
<span class="n">train_smiles</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">ids</span>
<span class="n">valid_smiles</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="o">.</span><span class="n">ids</span>
</pre></div>
</div>
<p>We need to define the “alphabet” for our <code class="docutils literal"><span class="pre">SeqToSeq</span></code> model, the list of
all tokens that can appear in sequences. (It’s also possible for input
and output sequences to have different alphabets, but since we’re
training it as an autoencoder, they’re identical in this case.) Make a
list of every character that appears in any training sequence.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">to_canon</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Chem</span><span class="o">.</span><span class="n">MolToSmiles</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
<span class="n">canon_train_smiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_canon</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train_smiles</span><span class="p">]</span>
<span class="n">canon_train_smiles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span> <span class="n">canon_train_smiles</span><span class="p">))</span>
</pre></div>
</div>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_smiles</span><span class="p">:</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">))</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>
</div>
<p>Create the model and define the optimization method to use. In this
case, learning works much better if we gradually decrease the learning
rate. We use an <code class="docutils literal"><span class="pre">ExponentialDecay</span></code> to multiply the learning rate by
0.9 after each epoch.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">ExponentialDecay</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_smiles</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">tensorgraph</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">seqtoseq</span><span class="o">.</span><span class="n">AspuruGuzikAutoEncoder</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span>
                           <span class="n">max_length</span><span class="p">,</span>
                           <span class="n">tensorboard</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                           <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;aspuru_guzik_vae&#39;</span><span class="p">,</span>
                           <span class="n">anneal_start_step</span><span class="o">=</span><span class="mi">40000</span><span class="p">,</span>
                           <span class="n">anneal_stop_step</span><span class="o">=</span><span class="mi">60000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="n">batches_per_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_smiles</span><span class="p">)</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">batch_size</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_optimizer</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">ExponentialDecay</span><span class="p">(</span><span class="mf">0.004</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">batches_per_epoch</span><span class="p">)))</span>
</pre></div>
</div>
<p>Let’s train it! The input to <code class="docutils literal"><span class="pre">fit_sequences()</span></code> is a generator that
produces input/output pairs. On a good GPU, this should take a few hours
or less.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">batches_per_epoch</span>
</pre></div>
</div>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_sequences</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">canon_train_smiles</span><span class="p">:</span>
      <span class="k">yield</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit_sequences</span><span class="p">(</span><span class="n">generate_sequences</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>
</pre></div>
</div>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="c1">#model.set_optimizer(Adam())</span>
<span class="c1">#model.tensor_objects[&#39;Optimizer&#39;] = model.optimizer._create_optimizer(model._get_tf(&#39;GlobalStep&#39;))</span>
<span class="k">def</span> <span class="nf">generate_sequences</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">canon_train_smiles</span><span class="p">:</span>
      <span class="k">yield</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_sequences</span><span class="p">(</span><span class="n">generate_sequences</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>
</pre></div>
</div>
<p>Let’s see how well it works as an autoencoder. We’ll run the first 500
molecules from the validation set through it, and see how many of them
are exactly reproduced.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_from_sequences</span><span class="p">(</span><span class="n">train_smiles</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">train_smiles</span><span class="p">[:</span><span class="mi">500</span><span class="p">],</span> <span class="n">predicted</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">)))</span>
  <span class="k">if</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">==</span> <span class="n">s</span><span class="p">:</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;reproduced&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="s1">&#39;of 500 validation SMILES strings&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we’ll trying using the encoder as a way to generate molecular
fingerprints. We compute the embedding vectors for all molecules in the
training and validation datasets, and create new datasets that have
those as their feature vectors. The amount of data is small enough that
we can just store everything in memory.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rdkit</span> <span class="kn">import</span> <span class="n">Chem</span>
<span class="k">def</span> <span class="nf">is_canon</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">==</span> <span class="n">Chem</span><span class="o">.</span><span class="n">MolToSmiles</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">is_canon</span><span class="p">(</span><span class="s1">&#39;NC(=O)c1ccccc1NC(=O)/C=C/c1cccc(Cl)c1&#39;</span><span class="p">)</span>
<span class="s2">&quot;NC(=O)c1ccccc1NC(=O)/C=C/c1cccc(Cl)c1&quot;</span><span class="o">==</span><span class="s2">&quot;NC(=O)c1ccccc1NC(=O)/C=C/c1cccc(Cl)c1&quot;</span>
</pre></div>
</div>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">train_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_embeddings</span><span class="p">(</span><span class="n">train_smiles</span><span class="p">)</span>
<span class="n">train_embeddings_dataset</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NumpyDataset</span><span class="p">(</span><span class="n">train_embeddings</span><span class="p">,</span>
                                                <span class="n">train_dataset</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                                                <span class="n">train_dataset</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
                                                <span class="n">train_dataset</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>

<span class="n">valid_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_embeddings</span><span class="p">(</span><span class="n">valid_smiles</span><span class="p">)</span>
<span class="n">valid_embeddings_dataset</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">NumpyDataset</span><span class="p">(</span><span class="n">valid_embeddings</span><span class="p">,</span>
                                                <span class="n">valid_dataset</span><span class="o">.</span><span class="n">y</span><span class="p">,</span>
                                                <span class="n">valid_dataset</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
                                                <span class="n">valid_dataset</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
</pre></div>
</div>
<p>For classification, we’ll use a simple fully connected network with one
hidden layer.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">MultiTaskClassifier</span><span class="p">(</span><span class="n">n_tasks</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tasks</span><span class="p">),</span>
                                                      <span class="n">n_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                                                      <span class="n">layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">])</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_embeddings_dataset</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Find out how well it worked. Compute the ROC AUC for the training and
validation datasets.</p>
<div class="code ipython3 highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">dc</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">)</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">train_embeddings_dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">metric</span><span class="p">],</span> <span class="n">transformers</span><span class="p">)</span>
<span class="n">valid_score</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid_embeddings_dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">metric</span><span class="p">],</span> <span class="n">transformers</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Training set ROC AUC:&#39;</span><span class="p">,</span> <span class="n">train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Validation set ROC AUC:&#39;</span><span class="p">,</span> <span class="n">valid_score</span><span class="p">)</span>
</pre></div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/notebooks/guzik.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>