<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>deepchem.models.tensorgraph.models.gan &mdash; deepchem 1.3.1 documentation</title>
    
    <link rel="stylesheet" href="../../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../../',
        VERSION:     '1.3.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="deepchem 1.3.1 documentation" href="../../../../../index.html" />
    <link rel="up" title="Module code" href="../../../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../../index.html"><span><img src="../../../../../_static/logo.png"></span>
          deepchem</a>
        <span class="navbar-text navbar-version pull-left"><b>1.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../../../notebooks/index.html">Notebooks</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deepchem.html">deepchem package</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for deepchem.models.tensorgraph.models.gan</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Generative Adversarial Networks.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">deepchem.models</span> <span class="kn">import</span> <span class="n">TensorGraph</span>
<span class="kn">from</span> <span class="nn">deepchem.models.tensorgraph</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Sequence</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">time</span>


<div class="viewcode-block" id="GAN"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN">[docs]</a><span class="k">class</span> <span class="nc">GAN</span><span class="p">(</span><span class="n">TensorGraph</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements Generative Adversarial Networks.</span>

<span class="sd">  A Generative Adversarial Network (GAN) is a type of generative model.  It</span>
<span class="sd">  consists of two parts called the &quot;generator&quot; and the &quot;discriminator&quot;.  The</span>
<span class="sd">  generator takes random noise as input and transforms it into an output that</span>
<span class="sd">  (hopefully) resembles the training data.  The discriminator takes a set of</span>
<span class="sd">  samples as input and tries to distinguish the real training samples from the</span>
<span class="sd">  ones created by the generator.  Both of them are trained together.  The</span>
<span class="sd">  discriminator tries to get better and better at telling real from false data,</span>
<span class="sd">  while the generator tries to get better and better at fooling the discriminator.</span>

<span class="sd">  In many cases there also are additional inputs to the generator and</span>
<span class="sd">  discriminator.  In that case it is known as a Conditional GAN (CGAN), since it</span>
<span class="sd">  learns a distribution that is conditional on the values of those inputs.  They</span>
<span class="sd">  are referred to as &quot;conditional inputs&quot;.</span>

<span class="sd">  Many variations on this idea have been proposed, and new varieties of GANs are</span>
<span class="sd">  constantly being proposed.  This class tries to make it very easy to implement</span>
<span class="sd">  straightforward GANs of the most conventional types.  At the same time, it</span>
<span class="sd">  tries to be flexible enough that it can be used to implement many (but</span>
<span class="sd">  certainly not all) variations on the concept.</span>

<span class="sd">  To define a GAN, you must create a subclass that provides implementations of</span>
<span class="sd">  the following methods:</span>

<span class="sd">  get_noise_input_shape()</span>
<span class="sd">  get_data_input_shapes()</span>
<span class="sd">  create_generator()</span>
<span class="sd">  create_discriminator()</span>

<span class="sd">  If you want your GAN to have any conditional inputs you must also implement:</span>

<span class="sd">  get_conditional_input_shapes()</span>

<span class="sd">  The following methods have default implementations that are suitable for most</span>
<span class="sd">  conventional GANs.  You can override them if you want to customize their</span>
<span class="sd">  behavior:</span>

<span class="sd">  create_generator_loss()</span>
<span class="sd">  create_discriminator_loss()</span>
<span class="sd">  get_noise_batch()</span>

<span class="sd">  This class allows a GAN to have multiple generators and discriminators, a model</span>
<span class="sd">  known as MIX+GAN.  It is described in Arora et al., &quot;Generalization and</span>
<span class="sd">  Equilibrium in Generative Adversarial Nets (GANs)&quot; (https://arxiv.org/abs/1703.00573).</span>
<span class="sd">  This can lead to better models, and is especially useful for reducing mode</span>
<span class="sd">  collapse, since different generators can learn different parts of the</span>
<span class="sd">  distribution.  To use this technique, simply specify the number of generators</span>
<span class="sd">  and discriminators when calling the constructor.  You can then tell</span>
<span class="sd">  predict_gan_generator() which generator to use for predicting samples.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_generators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_discriminators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a GAN.</span>

<span class="sd">    In addition to the parameters listed below, this class accepts all the</span>
<span class="sd">    keyword arguments from TensorGraph.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_generators: int</span>
<span class="sd">      the number of generators to include</span>
<span class="sd">    n_discriminators: int</span>
<span class="sd">      the number of discriminators to include</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GAN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">use_queue</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_generators</span> <span class="o">=</span> <span class="n">n_generators</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_discriminators</span> <span class="o">=</span> <span class="n">n_discriminators</span>

    <span class="c1"># Create the inputs.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">noise_input</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_noise_input_shape</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_data_input_shapes</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_conditional_input_shapes</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">))</span>

    <span class="c1"># Create the generators.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">generators</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_generators</span><span class="p">):</span>
      <span class="n">generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_input</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;create_generator() must return a list of Layers&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;The number of generator outputs must match the number of data inputs&#39;</span>
        <span class="p">)</span>
      <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s1">&#39;The shapes of the generator outputs must match the shapes of the data inputs&#39;</span>
          <span class="p">)</span>
      <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">generator</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_output</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">generators</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>

    <span class="c1"># Create the discriminators.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">discrim_train</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discrim_gen</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_discriminators</span><span class="p">):</span>
      <span class="n">discrim_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span><span class="p">,</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">discrim_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">discrim_train</span><span class="p">)</span>

      <span class="c1"># Make a copy of the discriminator that takes each generator&#39;s output as</span>
      <span class="c1"># its input.</span>

      <span class="k">for</span> <span class="n">generator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generators</span><span class="p">:</span>
        <span class="n">replacements</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_inputs</span><span class="p">):</span>
          <span class="n">replacements</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span><span class="p">:</span>
          <span class="n">replacements</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span>
        <span class="n">discrim_gen</span> <span class="o">=</span> <span class="n">discrim_train</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">replacements</span><span class="p">,</span> <span class="n">shared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrim_gen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">discrim_gen</span><span class="p">)</span>

    <span class="c1"># Make a list of all layers in the generators and discriminators.</span>

    <span class="k">def</span> <span class="nf">add_layers_to_set</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_layers</span><span class="p">:</span>
          <span class="n">add_layers_to_set</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>

    <span class="n">gen_layers</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">generator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generators</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">generator</span><span class="p">:</span>
        <span class="n">add_layers_to_set</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">gen_layers</span><span class="p">)</span>
    <span class="n">discrim_layers</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">discriminator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrim_train</span><span class="p">:</span>
      <span class="n">add_layers_to_set</span><span class="p">(</span><span class="n">discriminator</span><span class="p">,</span> <span class="n">discrim_layers</span><span class="p">)</span>
    <span class="n">discrim_layers</span> <span class="o">-=</span> <span class="n">gen_layers</span>

    <span class="c1"># Compute the loss functions.</span>

    <span class="n">gen_losses</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">create_generator_loss</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrim_gen</span><span class="p">]</span>
    <span class="n">discrim_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_discriminators</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_generators</span><span class="p">):</span>
        <span class="n">discrim_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_discriminator_loss</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">discrim_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrim_gen</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n_generators</span> <span class="o">+</span> <span class="n">j</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">n_generators</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n_discriminators</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">total_gen_loss</span> <span class="o">=</span> <span class="n">gen_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">total_discrim_loss</span> <span class="o">=</span> <span class="n">discrim_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Create learnable weights for the generators and discriminators.</span>

      <span class="n">gen_alpha</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_generators</span><span class="p">)))</span>
      <span class="n">gen_weights</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftMax</span><span class="p">(</span><span class="n">gen_alpha</span><span class="p">)</span>
      <span class="n">discrim_alpha</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_discriminators</span><span class="p">)))</span>
      <span class="n">discrim_weights</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SoftMax</span><span class="p">(</span><span class="n">discrim_alpha</span><span class="p">)</span>

      <span class="c1"># Compute the weighted errors</span>

      <span class="n">weight_products</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
          <span class="p">(</span><span class="n">n_generators</span> <span class="o">*</span> <span class="n">n_discriminators</span><span class="p">,),</span>
          <span class="n">in_layers</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
              <span class="p">(</span><span class="n">n_discriminators</span><span class="p">,</span>
               <span class="mi">1</span><span class="p">),</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">discrim_weights</span><span class="p">)</span> <span class="o">*</span> <span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span>
                   <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_generators</span><span class="p">),</span> <span class="n">in_layers</span><span class="o">=</span><span class="n">gen_weights</span><span class="p">))</span>
      <span class="n">total_gen_loss</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">WeightedError</span><span class="p">((</span><span class="n">layers</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">gen_losses</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                             <span class="n">weight_products</span><span class="p">))</span>
      <span class="n">total_discrim_loss</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">WeightedError</span><span class="p">((</span><span class="n">layers</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span>
          <span class="n">discrim_losses</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">weight_products</span><span class="p">))</span>
      <span class="n">gen_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">gen_alpha</span><span class="p">)</span>
      <span class="n">discrim_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">gen_alpha</span><span class="p">)</span>
      <span class="n">discrim_layers</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">discrim_alpha</span><span class="p">)</span>

      <span class="c1"># Add an entropy term to the loss.</span>

      <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span>
          <span class="n">layers</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">gen_weights</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_generators</span> <span class="o">+</span>
          <span class="n">layers</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">discrim_weights</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_discriminators</span><span class="p">)</span>
      <span class="n">total_discrim_loss</span> <span class="o">+=</span> <span class="n">entropy</span>

    <span class="c1"># Create submodels for training the generators and discriminators.</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">generator_submodel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_submodel</span><span class="p">(</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">gen_layers</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">total_gen_loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_submodel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_submodel</span><span class="p">(</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">discrim_layers</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">total_discrim_loss</span><span class="p">)</span>

<div class="viewcode-block" id="GAN.get_noise_input_shape"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.get_noise_input_shape">[docs]</a>  <span class="k">def</span> <span class="nf">get_noise_input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the shape of the generator&#39;s noise input layer.</span>

<span class="sd">    Subclasses must override this to return a tuple giving the shape of the</span>
<span class="sd">    noise input.  The actual Input layer will be created automatically.  The</span>
<span class="sd">    first dimension must be None, since it will correspond to the batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclasses must implement this.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.get_data_input_shapes"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.get_data_input_shapes">[docs]</a>  <span class="k">def</span> <span class="nf">get_data_input_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the shapes of the inputs for training data.</span>

<span class="sd">    Subclasses must override this to return a list of tuples, each giving the</span>
<span class="sd">    shape of one of the inputs.  The actual Input layers will be created</span>
<span class="sd">    automatically.  This list of shapes must also match the shapes of the</span>
<span class="sd">    generator&#39;s outputs.  The first dimension of each shape must be None, since</span>
<span class="sd">    it will correspond to the batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclasses must implement this.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.get_conditional_input_shapes"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.get_conditional_input_shapes">[docs]</a>  <span class="k">def</span> <span class="nf">get_conditional_input_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the shapes of any conditional inputs.</span>

<span class="sd">    Subclasses may override this to return a list of tuples, each giving the</span>
<span class="sd">    shape of one of the conditional inputs.  The actual Input layers will be</span>
<span class="sd">    created automatically.  The first dimension of each shape must be None,</span>
<span class="sd">    since it will correspond to the batch size.</span>

<span class="sd">    The default implementation returns an empty list, meaning there are no</span>
<span class="sd">    conditional inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="GAN.get_noise_batch"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.get_noise_batch">[docs]</a>  <span class="k">def</span> <span class="nf">get_noise_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a batch of random noise to pass to the generator.</span>

<span class="sd">    This should return a NumPy array whose shape matches the one returned by</span>
<span class="sd">    get_noise_input_shape().  The default implementation returns normally</span>
<span class="sd">    distributed values.  Subclasses can override this to implement a different</span>
<span class="sd">    distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_noise_input_shape</span><span class="p">())</span>
    <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.create_generator"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.create_generator">[docs]</a>  <span class="k">def</span> <span class="nf">create_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise_input</span><span class="p">,</span> <span class="n">conditional_inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the generator.</span>

<span class="sd">    Subclasses must override this to construct the generator and return its</span>
<span class="sd">    output layers.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    noise_input: Input</span>
<span class="sd">      the Input layer from which the generator can read random noise.  The shape</span>
<span class="sd">      will match the return value from get_noise_input_shape().</span>
<span class="sd">    conditional_inputs: list</span>
<span class="sd">      the Input layers for any conditional inputs to the network.  The number</span>
<span class="sd">      and shapes of these inputs will match the return value from</span>
<span class="sd">      get_conditional_input_shapes().</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A list of Layer objects that produce the generator&#39;s outputs.  The number and</span>
<span class="sd">    shapes of these layers must match the return value from get_data_input_shapes(),</span>
<span class="sd">    since generated data must have the same form as training data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclasses must implement this.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.create_discriminator"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.create_discriminator">[docs]</a>  <span class="k">def</span> <span class="nf">create_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_inputs</span><span class="p">,</span> <span class="n">conditional_inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the discriminator.</span>

<span class="sd">    Subclasses must override this to construct the discriminator and return its</span>
<span class="sd">    output layer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data_inputs: list</span>
<span class="sd">      the Input layers from which the discriminator can read the input data.</span>
<span class="sd">      The number and shapes of these inputs will match the return value from</span>
<span class="sd">      get_data_input_shapes().  The samples read from these layers may be either</span>
<span class="sd">      training data or generated data.</span>
<span class="sd">    conditional_inputs: list</span>
<span class="sd">      the Input layers for any conditional inputs to the network.  The number</span>
<span class="sd">      and shapes of these inputs will match the return value from</span>
<span class="sd">      get_conditional_input_shapes().</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A Layer object that outputs the probability of each sample being a training</span>
<span class="sd">    sample.  The shape of this layer must be [None].  That is, it must output a</span>
<span class="sd">    one dimensional tensor whose length equals the batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclasses must implement this.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.create_generator_loss"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.create_generator_loss">[docs]</a>  <span class="k">def</span> <span class="nf">create_generator_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discrim_output</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the loss function for the generator.</span>

<span class="sd">    The default implementation is appropriate for most cases.  Subclasses can</span>
<span class="sd">    override this if the need to customize it.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    discrim_output: Layer</span>
<span class="sd">      the output from the discriminator on a batch of generated data.  This is</span>
<span class="sd">      its estimate of the probability that each sample is training data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A Layer object that outputs the loss function to use for optimizing the</span>
<span class="sd">    generator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">layers</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">discrim_output</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">))</span></div>

<div class="viewcode-block" id="GAN.create_discriminator_loss"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.create_discriminator_loss">[docs]</a>  <span class="k">def</span> <span class="nf">create_discriminator_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discrim_output_train</span><span class="p">,</span> <span class="n">discrim_output_gen</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the loss function for the discriminator.</span>

<span class="sd">    The default implementation is appropriate for most cases.  Subclasses can</span>
<span class="sd">    override this if the need to customize it.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    discrim_output_train: Layer</span>
<span class="sd">      the output from the discriminator on a batch of generated data.  This is</span>
<span class="sd">      its estimate of the probability that each sample is training data.</span>
<span class="sd">    discrim_output_gen: Layer</span>
<span class="sd">      the output from the discriminator on a batch of training data.  This is</span>
<span class="sd">      its estimate of the probability that each sample is training data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A Layer object that outputs the loss function to use for optimizing the</span>
<span class="sd">    discriminator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">training_data_loss</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">discrim_output_train</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="n">gen_data_loss</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">discrim_output_gen</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">layers</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">training_data_loss</span> <span class="o">+</span> <span class="n">gen_data_loss</span><span class="p">)</span></div>

<div class="viewcode-block" id="GAN.fit_gan"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.fit_gan">[docs]</a>  <span class="k">def</span> <span class="nf">fit_gan</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">batches</span><span class="p">,</span>
              <span class="n">generator_steps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
              <span class="n">max_checkpoints_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
              <span class="n">checkpoint_interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
              <span class="n">restore</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train this model on data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batches: iterable</span>
<span class="sd">      batches of data to train the discriminator on, each represented as a dict</span>
<span class="sd">      that maps Layers to values.  It should specify values for all members of</span>
<span class="sd">      data_inputs and conditional_inputs.</span>
<span class="sd">    generator_steps: float</span>
<span class="sd">      the number of training steps to perform for the generator for each batch.</span>
<span class="sd">      This can be used to adjust the ratio of training steps for the generator</span>
<span class="sd">      and discriminator.  For example, 2.0 will perform two training steps for</span>
<span class="sd">      every batch, while 0.5 will only perform one training step for every two</span>
<span class="sd">      batches.</span>
<span class="sd">    max_checkpoints_to_keep: int</span>
<span class="sd">      the maximum number of checkpoints to keep.  Older checkpoints are discarded.</span>
<span class="sd">    checkpoint_interval: int</span>
<span class="sd">      the frequency at which to write checkpoints, measured in batches.  Set</span>
<span class="sd">      this to 0 to disable automatic checkpointing.</span>
<span class="sd">    restore: bool</span>
<span class="sd">      if True, restore the model from the most recent checkpoint before training</span>
<span class="sd">      it.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">restore</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>
    <span class="n">gen_train_fraction</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">discrim_error</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">gen_error</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">discrim_average_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">gen_average_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">time1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tf</span><span class="p">(</span><span class="s2">&quot;Graph&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">checkpoint_interval</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">max_to_keep</span><span class="o">=</span><span class="n">max_checkpoints_to_keep</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">feed_dict</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="c1"># Every call to fit_generator() will increment global_step, but we only</span>
        <span class="c1"># want it to get incremented once for the entire batch, so record the</span>
        <span class="c1"># value and keep resetting it.</span>

        <span class="n">global_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span>

        <span class="c1"># Train the discriminator.</span>

        <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">feed_dict</span><span class="p">)</span>
        <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_input</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_noise_batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">discrim_error</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span>
            <span class="p">[</span><span class="n">feed_dict</span><span class="p">],</span>
            <span class="n">submodel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">discriminator_submodel</span><span class="p">,</span>
            <span class="n">checkpoint_interval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span> <span class="o">=</span> <span class="n">global_step</span>
        <span class="n">discrim_average_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Train the generator.</span>

        <span class="k">if</span> <span class="n">generator_steps</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
          <span class="n">gen_train_fraction</span> <span class="o">+=</span> <span class="n">generator_steps</span>
          <span class="k">while</span> <span class="n">gen_train_fraction</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_input</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_noise_batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">gen_error</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span>
                <span class="p">[</span><span class="n">feed_dict</span><span class="p">],</span>
                <span class="n">submodel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator_submodel</span><span class="p">,</span>
                <span class="n">checkpoint_interval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span> <span class="o">=</span> <span class="n">global_step</span>
            <span class="n">gen_average_steps</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">gen_train_fraction</span> <span class="o">-=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Write checkpoints and report progress.</span>

        <span class="k">if</span> <span class="n">discrim_average_steps</span> <span class="o">==</span> <span class="n">checkpoint_interval</span><span class="p">:</span>
          <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_file</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
          <span class="n">discrim_loss</span> <span class="o">=</span> <span class="n">discrim_error</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">discrim_average_steps</span><span class="p">)</span>
          <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">gen_error</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gen_average_steps</span><span class="p">)</span>
          <span class="k">print</span><span class="p">(</span>
              <span class="s1">&#39;Ending global_step </span><span class="si">%d</span><span class="s1">: generator average loss </span><span class="si">%g</span><span class="s1">, discriminator average loss </span><span class="si">%g</span><span class="s1">&#39;</span>
              <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">gen_loss</span><span class="p">,</span> <span class="n">discrim_loss</span><span class="p">))</span>
          <span class="n">discrim_error</span> <span class="o">=</span> <span class="mf">0.0</span>
          <span class="n">gen_error</span> <span class="o">=</span> <span class="mf">0.0</span>
          <span class="n">discrim_average_steps</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="n">gen_average_steps</span> <span class="o">=</span> <span class="mi">0</span>

      <span class="c1"># Write out final results.</span>

      <span class="k">if</span> <span class="n">checkpoint_interval</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">discrim_average_steps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">gen_average_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">discrim_loss</span> <span class="o">=</span> <span class="n">discrim_error</span> <span class="o">/</span> <span class="n">discrim_average_steps</span>
          <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">gen_error</span> <span class="o">/</span> <span class="n">gen_average_steps</span>
          <span class="k">print</span><span class="p">(</span>
              <span class="s1">&#39;Ending global_step </span><span class="si">%d</span><span class="s1">: generator average loss </span><span class="si">%g</span><span class="s1">, discriminator average loss </span><span class="si">%g</span><span class="s1">&#39;</span>
              <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">gen_loss</span><span class="p">,</span> <span class="n">discrim_loss</span><span class="p">))</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_file</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
        <span class="n">time2</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;TIMING: model fitting took </span><span class="si">%0.3f</span><span class="s2"> s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time2</span> <span class="o">-</span> <span class="n">time1</span><span class="p">))</span></div>

<div class="viewcode-block" id="GAN.predict_gan_generator"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GAN.predict_gan_generator">[docs]</a>  <span class="k">def</span> <span class="nf">predict_gan_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                            <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">noise_input</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                            <span class="n">conditional_inputs</span><span class="o">=</span><span class="p">[],</span>
                            <span class="n">generator_index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Use the GAN to generate a batch of samples.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batch_size: int</span>
<span class="sd">      the number of samples to generate.  If either noise_input or</span>
<span class="sd">      conditional_inputs is specified, this argument is ignored since the batch</span>
<span class="sd">      size is then determined by the size of that argument.</span>
<span class="sd">    noise_input: array</span>
<span class="sd">      the value to use for the generator&#39;s noise input.  If None (the default),</span>
<span class="sd">      get_noise_batch() is called to generate a random input, so each call will</span>
<span class="sd">      produce a new set of samples.</span>
<span class="sd">    conditional_inputs: list of arrays</span>
<span class="sd">      the values to use for all conditional inputs.  This must be specified if</span>
<span class="sd">      the GAN has any conditional inputs.</span>
<span class="sd">    generator_index: int</span>
<span class="sd">      the index of the generator (between 0 and n_generators-1) to use for</span>
<span class="sd">      generating the samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    An array (if the generator has only one output) or list of arrays (if it has</span>
<span class="sd">    multiple outputs) containing the generated samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">noise_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">noise_input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">conditional_inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">conditional_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">noise_input</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">noise_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_noise_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">batch</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_input</span><span class="p">]</span> <span class="o">=</span> <span class="n">noise_input</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conditional_inputs</span><span class="p">,</span> <span class="n">conditional_inputs</span><span class="p">):</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_on_generator</span><span class="p">(</span>
        <span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generators</span><span class="p">[</span><span class="n">generator_index</span><span class="p">])</span></div>

  <span class="k">def</span> <span class="nf">_set_empty_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set entries in a feed dict corresponding to a batch size of 0.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">feed_dict</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="WGAN"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.WGAN">[docs]</a><span class="k">class</span> <span class="nc">WGAN</span><span class="p">(</span><span class="n">GAN</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements Wasserstein Generative Adversarial Networks.</span>

<span class="sd">  This class implements Wasserstein Generative Adversarial Networks (WGANs) as</span>
<span class="sd">  described in Arjovsky et al., &quot;Wasserstein GAN&quot; (https://arxiv.org/abs/1701.07875).</span>
<span class="sd">  A WGAN is conceptually rather different from a conventional GAN, but in</span>
<span class="sd">  practical terms very similar.  It reinterprets the discriminator (often called</span>
<span class="sd">  the &quot;critic&quot; in this context) as learning an approximation to the Earth Mover</span>
<span class="sd">  distance between the training and generated distributions.  The generator is</span>
<span class="sd">  then trained to minimize that distance.  In practice, this just means using</span>
<span class="sd">  slightly different loss functions for training the generator and discriminator.</span>

<span class="sd">  WGANs have theoretical advantages over conventional GANs, and they often work</span>
<span class="sd">  better in practice.  In addition, the discriminator&#39;s loss function can be</span>
<span class="sd">  directly interpreted as a measure of the quality of the model.  That is an</span>
<span class="sd">  advantage over conventional GANs, where the loss does not directly convey</span>
<span class="sd">  information about the quality of the model.</span>

<span class="sd">  The theory WGANs are based on requires the discriminator&#39;s gradient to be</span>
<span class="sd">  bounded.  The original paper achieved this by clipping its weights.  This</span>
<span class="sd">  class instead does it by adding a penalty term to the discriminator&#39;s loss, as</span>
<span class="sd">  described in https://arxiv.org/abs/1704.00028.  This is sometimes found to</span>
<span class="sd">  produce better results.</span>

<span class="sd">  There are a few other practical differences between GANs and WGANs.  In a</span>
<span class="sd">  conventional GAN, the discriminator&#39;s output must be between 0 and 1 so it can</span>
<span class="sd">  be interpreted as a probability.  In a WGAN, it should produce an unbounded</span>
<span class="sd">  output that can be interpreted as a distance.</span>

<span class="sd">  When training a WGAN, you also should usually use a smaller value for</span>
<span class="sd">  generator_steps.  Conventional GANs rely on keeping the generator and</span>
<span class="sd">  discriminator &quot;in balance&quot; with each other.  If the discriminator ever gets</span>
<span class="sd">  too good, it becomes impossible for the generator to fool it and training</span>
<span class="sd">  stalls.  WGANs do not have this problem, and in fact the better the</span>
<span class="sd">  discriminator is, the easier it is for the generator to improve.  It therefore</span>
<span class="sd">  usually works best to perform several training steps on the discriminator for</span>
<span class="sd">  each training step on the generator.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient_penalty</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a WGAN.</span>

<span class="sd">    In addition to the following, this class accepts all the keyword arguments</span>
<span class="sd">    from TensorGraph.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    gradient_penalty: float</span>
<span class="sd">      the magnitude of the gradient penalty loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WGAN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gradient_penalty</span> <span class="o">=</span> <span class="n">gradient_penalty</span>

<div class="viewcode-block" id="WGAN.create_generator_loss"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.WGAN.create_generator_loss">[docs]</a>  <span class="k">def</span> <span class="nf">create_generator_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discrim_output</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">discrim_output</span><span class="p">)</span></div>

<div class="viewcode-block" id="WGAN.create_discriminator_loss"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.WGAN.create_discriminator_loss">[docs]</a>  <span class="k">def</span> <span class="nf">create_discriminator_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discrim_output_train</span><span class="p">,</span> <span class="n">discrim_output_gen</span><span class="p">):</span>
    <span class="n">gradient_penalty</span> <span class="o">=</span> <span class="n">GradientPenaltyLayer</span><span class="p">(</span><span class="n">discrim_output_train</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradient_penalty</span> <span class="o">+</span> <span class="n">layers</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">discrim_output_train</span> <span class="o">-</span>
                                                <span class="n">discrim_output_gen</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="GradientPenaltyLayer"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GradientPenaltyLayer">[docs]</a><span class="k">class</span> <span class="nc">GradientPenaltyLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implements the gradient penalty loss term for WGANs.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discrim_output_train</span><span class="p">,</span> <span class="n">gan</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GradientPenaltyLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">([</span><span class="n">discrim_output_train</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gan</span> <span class="o">=</span> <span class="n">gan</span>

<div class="viewcode-block" id="GradientPenaltyLayer.create_tensor"><a class="viewcode-back" href="../../../../../deepchem.models.tensorgraph.models.html#deepchem.models.tensorgraph.models.gan.GradientPenaltyLayer.create_tensor">[docs]</a>  <span class="k">def</span> <span class="nf">create_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_layers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">set_tensors</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">gan</span><span class="o">.</span><span class="n">data_inputs</span><span class="p">)</span>
    <span class="n">norm2</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">:</span>
      <span class="n">g2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
      <span class="n">dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">dims</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">g2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">g2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dims</span><span class="p">)))</span>
      <span class="n">norm2</span> <span class="o">+=</span> <span class="n">g2</span>
    <span class="n">penalty</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">norm2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gan</span><span class="o">.</span><span class="n">gradient_penalty</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">penalty</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_tensor</span></div></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2016, Stanford University and the Authors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>